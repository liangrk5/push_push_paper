[
    {
        "title": "RecGPT: A Foundation Model for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2506.06270v1",
        "pub_date": "2025-06-06",
        "summary": "This work addresses a fundamental barrier in recommender systems: the inability to generalize across domains without extensive retraining. Traditional ID-based approaches fail entirely in cold-start and cross-domain scenarios where new users or items lack sufficient interaction history. Inspired by foundation models' cross-domain success, we develop a foundation model for sequential recommendation that achieves genuine zero-shot generalization capabilities. Our approach fundamentally departs from existing ID-based methods by deriving item representations exclusively from textual features. This enables immediate embedding of any new item without model retraining. We introduce unified item tokenization with Finite Scalar Quantization that transforms heterogeneous textual descriptions into standardized discrete tokens. This eliminates domain barriers that plague existing systems. Additionally, the framework features hybrid bidirectional-causal attention that captures both intra-item token coherence and inter-item sequential dependencies. An efficient catalog-aware beam search decoder enables real-time token-to-item mapping. Unlike conventional approaches confined to their training domains, RecGPT naturally bridges diverse recommendation contexts through its domain-invariant tokenization mechanism. Comprehensive evaluations across six datasets and industrial scenarios demonstrate consistent performance advantages.",
        "translated": "本工作旨在解决推荐系统面临的一个根本性障碍：即在不进行大量再训练的情况下，难以实现跨领域泛化。传统的基于ID的方法在冷启动和跨域场景中完全失效，因为新用户或新物品缺乏足够的交互历史。受基础模型跨域成功的启发，我们开发了一个用于序列推荐的基础模型，它实现了真正的零样本泛化能力。\n\n我们的方法与现有基于ID的方法从根本上不同，因为它仅从文本特征中推导物品表征。这使得任何新物品都无需模型再训练即可立即嵌入。我们引入了采用有限标量量化（FSQ）的统一物品分词方法，将异构文本描述转换为标准化离散词元。这消除了困扰现有系统的领域障碍。此外，该框架还具有混合双向-因果注意力机制，能够同时捕捉物品内词元连贯性和物品间序列依赖性。一个高效的目录感知束搜索解码器能够实现实时词元到物品的映射。\n\n与局限于其训练领域的传统方法不同，RecGPT通过其领域不变的分词机制自然地弥合了多样化的推荐上下文。在六个数据集和工业场景中的全面评估表明了其持续的性能优势。"
    },
    {
        "title": "Optimizing Recall or Relevance? A Multi-Task Multi-Head Approach for\n  Item-to-Item Retrieval in Recommendation",
        "url": "http://arxiv.org/abs/2506.06239v1",
        "pub_date": "2025-06-06",
        "summary": "The task of item-to-item (I2I) retrieval is to identify a set of relevant and highly engaging items based on a given trigger item. It is a crucial component in modern recommendation systems, where users' previously engaged items serve as trigger items to retrieve relevant content for future engagement. However, existing I2I retrieval models in industry are primarily built on co-engagement data and optimized using the recall measure, which overly emphasizes co-engagement patterns while failing to capture semantic relevance. This often leads to overfitting short-term co-engagement trends at the expense of long-term benefits such as discovering novel interests and promoting content diversity. To address this challenge, we propose MTMH, a Multi-Task and Multi-Head I2I retrieval model that achieves both high recall and semantic relevance. Our model consists of two key components: 1) a multi-task learning loss for formally optimizing the trade-off between recall and semantic relevance, and 2) a multi-head I2I retrieval architecture for retrieving both highly co-engaged and semantically relevant items. We evaluate MTMH using proprietary data from a commercial platform serving billions of users and demonstrate that it can improve recall by up to 14.4% and semantic relevance by up to 56.6% compared with prior state-of-the-art models. We also conduct live experiments to verify that MTMH can enhance both short-term consumption metrics and long-term user-experience-related metrics. Our work provides a principled approach for jointly optimizing I2I recall and semantic relevance, which has significant implications for improving the overall performance of recommendation systems.",
        "translated": "物品-物品（I2I）召回的任务是基于给定的触发物品，识别出一组相关且高度吸引人的物品。它是现代推荐系统中的一个关键组成部分，其中用户的历史互动物品可作为触发物品，用于召回相关内容以供未来互动。然而，现有行业中的I2I召回模型主要基于协同互动数据构建，并使用召回率指标进行优化，这过度强调了协同互动模式，却未能捕捉到语义相关性。这通常会导致模型过拟合短期协同互动趋势，从而牺牲了发现新兴趣和促进内容多样性等长期效益。为应对这一挑战，我们提出了MTMH，一个多任务多头（Multi-Task and Multi-Head）I2I召回模型，它能够同时实现高召回率和语义相关性。我们的模型包含两个关键组成部分：1) 一个多任务学习损失函数，用于正式优化召回率和语义相关性之间的权衡；2) 一个多头I2I召回架构，用于召回高度协同互动和语义相关的物品。我们使用来自一个服务数十亿用户的商业平台的专有数据对MTMH进行了评估，结果表明，相比于现有最先进的模型，它能将召回率提升高达14.4%，将语义相关性提升高达56.6%。我们还进行了在线实验，验证MTMH能够提升短期消费指标和长期用户体验相关指标。我们的工作为联合优化I2I召回率和语义相关性提供了一种原则性的方法，这对提升推荐系统的整体性能具有重要意义。"
    },
    {
        "title": "Recommender systems, stigmergy, and the tyranny of popularity",
        "url": "http://arxiv.org/abs/2506.06162v1",
        "pub_date": "2025-06-06",
        "summary": "Scientific recommender systems, such as Google Scholar and Web of Science, are essential tools for discovery. Search algorithms that power work through stigmergy, a collective intelligence mechanism that surfaces useful paths through repeated engagement. While generally effective, this ``rich-get-richer'' dynamic results in a small number of high-profile papers that dominate visibility. This essay argues argue that these algorithm over-reliance on popularity fosters intellectual homogeneity and exacerbates structural inequities, stifling innovative and diverse perspectives critical for scientific progress. We propose an overhaul of search platforms to incorporate user-specific calibration, allowing researchers to manually adjust the weights of factors like popularity, recency, and relevance. We also advise platform developers on how word embeddings and LLMs could be implemented in ways that increase user autonomy. While our suggestions are particularly pertinent to aligning recommender systems with scientific values, these ideas are broadly applicable to information access systems in general. Designing platforms that increase user autonomy is an important step toward more robust and dynamic information",
        "translated": "诸如 Google 学术（Google Scholar）和 Web of Science 等科学推荐系统，是重要的科研发现工具。驱动这些系统运行的搜索算法通过“触发式协作”（stigmergy）机制发挥作用，这是一种通过重复互动来揭示有用路径的集体智能机制。尽管这种机制通常有效，但其“富者愈富”（rich-get-richer）的动态会导致少数备受关注的论文占据主导可见度。\n\n本文认为，这些算法对流行度的过度依赖助长了学术思想的同质化，加剧了结构性不平等，从而扼杀了对科学进步至关重要的创新和多样化视角。我们建议对搜索平台进行彻底改革，以纳入用户特定的校准功能，允许研究人员手动调整流行度、时新度、相关性等因素的权重。我们还就平台开发者如何实施词嵌入（word embeddings）和大型语言模型（LLMs）以提高用户自主性提出建议。\n\n尽管我们的建议在使推荐系统与科学价值观保持一致方面尤其适用，但这些理念也普遍适用于一般的信息获取系统。设计能够提高用户自主性的平台是迈向更强大、更动态信息获取的重要一步。"
    },
    {
        "title": "CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval",
        "url": "http://arxiv.org/abs/2506.06144v1",
        "pub_date": "2025-06-06",
        "summary": "Online video web content is richly multimodal: a single video blends vision, speech, ambient audio, and on-screen text. Retrieval systems typically treat these modalities as independent retrieval sources, which can lead to noisy and subpar retrieval. We explore multimodal video content retrieval, where relevance can be scored from one particular modality or jointly across multiple modalities simultaneously. Consequently, an effective retriever must dynamically choose which modality (or set of modalities) best addresses the query. We introduce CLaMR, a multimodal, late-interaction retriever that jointly indexes 4 modalities: video frames, transcribed speech, on-screen text, and metadata. CLaMR jointly encodes all modalities with a unified multimodal backbone for improved contextualization and is trained to enhance dynamic modality selection via two key innovations. First, given the lack of training data for multimodal retrieval, we introduce MultiVENT 2.0++, a large-scale synthetic training dataset built on MultiVENT 2.0 (event-centric videos in various languages paired with queries) with modality-targeted queries. Next, we propose a modality-aware loss that jointly trains according to a standard contrastive objective alongside an objective for learning correct modality usage. On the test sets of MultiVENT 2.0++ and MSRVTT, conventional aggregation strategies, such as averaging similarities for baseline retrievers, degrade performance by introducing noise from irrelevant modalities. In contrast, CLaMR consistently outperforms existing retrievers: on MultiVENT 2.0++, CLaMR improves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4 over the best multi-modality retriever. We illustrate CLaMR's downstream utility on long-video QA, retrieving relevant frames and obtaining a 3.50% boost over LanguageBind on Video-MME and 1.42% over dense sampling on LongVideoBench.",
        "translated": "在线视频网络内容模态丰富：单个视频融合了视觉、语音、环境音频和屏幕文本。传统的检索系统通常将这些模态视为独立的检索源，这可能导致检索结果嘈杂且不理想。我们探索多模态视频内容检索，其中相关性可以从某一特定模态或同时从多个模态联合评估。因此，一个有效的检索器必须动态地选择哪个模态（或模态组合）最能满足查询需求。我们引入了CLaMR，一个多模态的晚期交互检索器，它联合索引了四种模态：视频帧、转录语音、屏幕文本和元数据。CLaMR通过统一的多模态骨干网络对所有模态进行联合编码，以提升上下文理解能力，并通过两项关键创新来增强动态模态选择的训练。首先，鉴于多模态检索训练数据的缺乏，我们引入了MultiVENT 2.0++，这是一个基于MultiVENT 2.0（包含多种语言的事件中心视频与查询配对）构建的大规模合成训练数据集，并加入了模态导向的查询。其次，我们提出了一种模态感知损失，它根据标准对比目标与学习正确模态使用的目标进行联合训练。在MultiVENT 2.0++和MSRVTT的测试集上，传统的聚合策略（例如对基线检索器进行相似度平均）会因为引入不相关模态的噪声而导致性能下降。相比之下，CLaMR持续超越现有检索器：在MultiVENT 2.0++上，CLaMR相较于最佳单模态检索器，nDCG@10提升了25.6；相较于最佳多模态检索器，提升了35.4。我们展示了CLaMR在长视频问答中的下游应用价值，通过检索相关帧，在Video-MME上比LanguageBind提升了3.50%，在LongVideoBench上比密集采样提升了1.42%。"
    },
    {
        "title": "Phonetically-Augmented Discriminative Rescoring for Voice Search Error\n  Correction",
        "url": "http://arxiv.org/abs/2506.06117v1",
        "pub_date": "2025-06-06",
        "summary": "End-to-end (E2E) Automatic Speech Recognition (ASR) models are trained using paired audio-text samples that are expensive to obtain, since high-quality ground-truth data requires human annotators. Voice search applications, such as digital media players, leverage ASR to allow users to search by voice as opposed to an on-screen keyboard. However, recent or infrequent movie titles may not be sufficiently represented in the E2E ASR system's training data, and hence, may suffer poor recognition.   In this paper, we propose a phonetic correction system that consists of (a) a phonetic search based on the ASR model's output that generates phonetic alternatives that may not be considered by the E2E system, and (b) a rescorer component that combines the ASR model recognition and the phonetic alternatives, and select a final system output.   We find that our approach improves word error rate between 4.4 and 7.6% relative on benchmarks of popular movie titles over a series of competitive baselines.",
        "translated": "端到端（E2E）自动语音识别（ASR）模型使用配对的音频-文本样本进行训练，但这些样本获取成本高昂，因为高质量的真实标注数据需要人工标注者。数字媒体播放器等语音搜索应用利用ASR技术，使用户能够通过语音进行搜索，而非使用屏幕键盘。然而，最近上映或不常见的电影名称可能在E2E ASR系统的训练数据中没有得到充分体现，因此可能导致较差的识别效果。\n\n本文中，我们提出了一种语音校正系统，该系统包含：(a) 基于ASR模型输出的语音搜索，用于生成E2E系统可能未考虑的语音备选项；以及 (b) 一个重打分组件，用于结合ASR模型识别结果和语音备选项，并选择最终的系统输出。\n\n我们发现，与一系列具有竞争力的基线相比，我们的方法在流行电影名称的基准测试中，相对降低了词错误率（WER）4.4%至7.6%。"
    },
    {
        "title": "On the Merits of LLM-Based Corpus Enrichment",
        "url": "http://arxiv.org/abs/2506.06015v1",
        "pub_date": "2025-06-06",
        "summary": "Generative AI (genAI) technologies -- specifically, large language models (LLMs) -- and search have evolving relations. We argue for a novel perspective: using genAI to enrich a document corpus so as to improve query-based retrieval effectiveness. The enrichment is based on modifying existing documents or generating new ones. As an empirical proof of concept, we use LLMs to generate documents relevant to a topic which are more retrievable than existing ones. In addition, we demonstrate the potential merits of using corpus enrichment for retrieval augmented generation (RAG) and answer attribution in question answering.",
        "translated": "生成式人工智能（genAI）技术——特别是大型语言模型（LLMs）——与搜索的关系正在不断演变。我们提出一种新颖的视角：利用生成式人工智能（genAI）技术来丰富文档语料库，以提高基于查询的检索有效性。这种丰富方法基于修改现有文档或生成新文档。作为一项实证性概念验证，我们使用大型语言模型（LLMs）生成了针对特定主题的文档，这些文档比现有文档更易于检索。此外，我们展示了将语料库丰富应用于检索增强生成（RAG）以及问答系统中的答案归因的潜在优势。"
    },
    {
        "title": "Respecting Temporal-Causal Consistency: Entity-Event Knowledge Graphs\n  for Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2506.05939v1",
        "pub_date": "2025-06-06",
        "summary": "Retrieval-augmented generation (RAG) based on large language models often falters on narrative documents with inherent temporal structures. Standard unstructured RAG methods rely solely on embedding-similarity matching and lack any general mechanism to encode or exploit chronological information, while knowledge graph RAG (KG-RAG) frameworks collapse every mention of an entity into a single node, erasing the evolving context that drives many queries. To formalize this challenge and draw the community's attention, we construct ChronoQA, a robust and discriminative QA benchmark that measures temporal, causal, and character consistency understanding in narrative documents (e.g., novels) under the RAG setting. We then introduce Entity-Event RAG (E^2RAG), a dual-graph framework that keeps separate entity and event subgraphs linked by a bipartite mapping, thereby preserving the temporal and causal facets needed for fine-grained reasoning. Across ChronoQA, our approach outperforms state-of-the-art unstructured and KG-based RAG baselines, with notable gains on causal and character consistency queries. E^2RAG therefore offers a practical path to more context-aware retrieval for tasks that require precise answers grounded in chronological information.",
        "translated": "基于大语言模型的检索增强生成（RAG）在处理具有固有时间结构的叙事文档时常常表现不佳。标准的非结构化RAG方法仅依赖于嵌入相似度匹配，并缺乏编码或利用时间信息的通用机制；而知识图谱RAG（KG-RAG）框架则将实体的每一次提及都合并为一个单一节点，抹去了驱动许多查询的演变上下文。\n\n为了形式化这一挑战并引起社区关注，我们构建了ChronoQA，这是一个鲁棒且具有区分度的问答基准，用于衡量RAG设置下叙事文档（例如小说）中的时间、因果和人物一致性理解能力。随后，我们提出了实体-事件RAG（E^2RAG），这是一个双图框架，它通过二分图映射将独立的实体子图和事件子图连接起来，从而保留了细粒度推理所需的时间和因果方面。\n\n在ChronoQA基准测试中，我们的方法优于最先进的非结构化和基于KG的RAG基线，在因果和人物一致性查询上取得了显著提升。因此，E^2RAG为需要基于时间信息提供精确答案的任务，提供了一条更具上下文感知能力的实用检索路径。"
    },
    {
        "title": "Research on Personalized Financial Product Recommendation by Integrating\n  Large Language Models and Graph Neural Networks",
        "url": "http://arxiv.org/abs/2506.05873v1",
        "pub_date": "2025-06-06",
        "summary": "With the rapid growth of fintech, personalized financial product recommendations have become increasingly important. Traditional methods like collaborative filtering or content-based models often fail to capture users' latent preferences and complex relationships. We propose a hybrid framework integrating large language models (LLMs) and graph neural networks (GNNs). A pre-trained LLM encodes text data (e.g., user reviews) into rich feature vectors, while a heterogeneous user-product graph models interactions and social ties. Through a tailored message-passing mechanism, text and graph information are fused within the GNN to jointly optimize embeddings. Experiments on public and real-world financial datasets show our model outperforms standalone LLM or GNN in accuracy, recall, and NDCG, with strong interpretability. This work offers new insights for personalized financial recommendations and cross-modal fusion in broader recommendation tasks.",
        "translated": "随着金融科技的快速发展，个性化金融产品推荐变得愈发重要。传统的协同过滤或基于内容的模型通常难以捕获用户的潜在偏好和复杂的相互关系。为此，我们提出一种融合大语言模型（LLM）和图神经网络（GNN）的混合框架。预训练的LLM负责将文本数据（如用户评论）编码为丰富的特征向量，而异构用户-产品图则用于建模用户与产品之间的交互以及用户的社交关系。通过定制化的消息传递机制，文本和图信息在GNN内部得到融合，从而联合优化嵌入表示。在公开和真实世界的金融数据集上进行的实验表明，我们的模型在准确率、召回率和NDCG方面均优于单独的LLM或GNN，并展现出强大的可解释性。这项工作为个性化金融推荐以及更广泛推荐任务中的跨模态融合提供了新的见解。"
    },
    {
        "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
        "url": "http://arxiv.org/abs/2506.06266v1",
        "pub_date": "2025-06-06",
        "summary": "Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.",
        "translated": "大型语言模型常用于回答基于大型文本语料库（例如代码库、法律文档或聊天记录）的查询，其方法是将整个语料库放入上下文窗口中，并利用上下文学习（ICL）的能力。尽管当前模型支持10万到100万个词元的上下文，但这种设置的服务成本很高，因为KV缓存的内存消耗随输入长度线性增长。\n\n我们探索了一种替代方案：为每个语料库离线训练一个更小的KV缓存。在推理时，我们加载这个经过训练的KV缓存（我们称之为Cartridge），并解码生成响应。关键在于，Cartridge的训练成本可以分摊到所有引用相同语料库的查询中。\n\n然而，我们发现，使用朴素的下一个词元预测方法在语料库上训练Cartridge，其效果无法与ICL媲美。取而代之的是，我们提出了一种名为“自学习”（self-study）的训练方案，其中我们生成关于语料库的合成对话，并利用上下文蒸馏目标来训练Cartridge。我们发现，通过自学习训练的Cartridge能够复现ICL的功能，同时服务成本显著降低。\n\n在具有挑战性的长上下文基准测试中，通过自学习训练的Cartridge在性能上与ICL相当，同时内存使用量减少了38.6倍，吞吐量提高了26.4倍。自学习还扩展了模型的有效上下文长度（例如在MTOB上从12.8万个词元扩展到48.4万个词元），并且令人惊讶的是，它使得Cartridge无需重新训练即可在推理时进行组合。"
    },
    {
        "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at\n  Test Time",
        "url": "http://arxiv.org/abs/2506.06254v1",
        "pub_date": "2025-06-06",
        "summary": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.",
        "translated": "大语言模型（LLM）驱动的智能体近期作为先进范式涌现，在广泛的领域和任务中展现出令人印象深刻的能力。尽管其潜力巨大，当前的LLM智能体却普遍采用“一刀切”的方法，缺乏根据用户多样化需求和偏好进行响应的灵活性。这一局限性促使我们开发了PersonaAgent，这是首个旨在解决多样化个性化任务的LLM个性化智能体框架。具体而言，PersonaAgent整合了两个互补的组件：一个包含情景记忆和语义记忆机制的个性化记忆模块；以及一个使智能体能够执行为用户量身定制的工具行动的个性化行动模块。其核心在于，角色（定义为每个用户的独特系统提示）充当着中介：它利用个性化记忆中的洞察来控制智能体的行动，而这些行动的结果反过来又会优化记忆。基于该框架，我们提出了一种测试时用户偏好对齐策略，该策略通过模拟最近的n次交互来优化角色提示，并利用模拟响应与真实响应之间的文本损失反馈，确保实时用户偏好对齐。实验评估表明，PersonaAgent不仅有效实现了行动空间的个性化，而且在测试时的实际应用中具有良好的可扩展性，显著优于其他基线方法。这些结果凸显了我们方法在提供量身定制、动态用户体验方面的可行性和潜力。"
    },
    {
        "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval",
        "url": "http://arxiv.org/abs/2506.06220v1",
        "pub_date": "2025-06-06",
        "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image retrieval benchmarks. However, bridging this success to real-world applications remains a challenge. In practice, human search behavior is rarely a one-shot action. Instead, it is often a multi-round process guided by clues in mind, that is, a mental image ranging from vague recollections to vivid mental representations of the target image. Motivated by this gap, we study the task of Mental Image Retrieval (MIR), which targets the realistic yet underexplored setting where users refine their search for a mentally envisioned image through multi-round interactions with an image search engine. Central to successful interactive retrieval is the capability of machines to provide users with clear, actionable feedback; however, existing methods rely on indirect or abstract verbal feedback, which can be ambiguous, misleading, or ineffective for users to refine the query. To overcome this, we propose GenIR, a generative multi-round retrieval paradigm leveraging diffusion-based image generation to explicitly reify the AI system's understanding at each round. These synthetic visual representations provide clear, interpretable feedback, enabling users to refine their queries intuitively and effectively. We further introduce a fully automated pipeline to generate a high-quality multi-round MIR dataset. Experimental results demonstrate that GenIR significantly outperforms existing interactive methods in the MIR scenario. This work establishes a new task with a dataset and an effective generative retrieval method, providing a foundation for future research in this direction.",
        "translated": "视觉-语言模型（VLM）在文本到图像检索基准测试中表现出强大的性能。然而，将这一成功推广到现实世界应用仍然是一个挑战。在实践中，人类的搜索行为很少是单次操作。相反，它通常是一个多轮过程，由脑海中的线索所引导，即一种心理图像，从模糊的回忆到目标图像生动的心理表征。受此差距启发，我们研究了心理图像检索（MIR）任务，该任务旨在解决用户通过与图像搜索引擎进行多轮交互来细化其对脑海中构想的图像的搜索这一现实但未充分探索的场景。\n\n成功的交互式检索关键在于机器能够向用户提供清晰、可操作的反馈；然而，现有方法依赖于间接或抽象的语言反馈，这可能对用户细化查询而言是模糊、误导性或低效的。为了克服这一问题，我们提出了GenIR，这是一种生成式多轮检索范式，它利用基于扩散的图像生成技术来明确地具象化AI系统在每一轮中的理解。这些合成视觉表示提供了清晰、可解释的反馈，使用户能够直观且有效地细化其查询。我们进一步引入了一个全自动流程来生成高质量的多轮心理图像检索数据集。实验结果表明，GenIR在心理图像检索场景中显著优于现有的交互式方法。这项工作建立了一项新任务，并提供了数据集和一种有效的生成式检索方法，为未来该方向的研究奠定了基础。"
    },
    {
        "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at\n  Test Time",
        "url": "http://arxiv.org/abs/2506.06254v1",
        "pub_date": "2025-06-06",
        "summary": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.",
        "translated": "大型语言模型（LLM）驱动的智能体近期作为先进范式涌现，在广泛的领域和任务中展现出卓越的能力。尽管其潜力巨大，当前的LLM智能体通常采用“一刀切”的方式，缺乏灵活性以响应用户多样化的需求和偏好。这一局限性促使我们开发了PersonaAgent，这是首个旨在处理多样化个性化任务的个性化LLM智能体框架。具体而言，PersonaAgent集成了两个互补的组件：一个包含情景记忆和语义记忆机制的个性化记忆模块；以及一个使智能体能够执行为用户量身定制的工具操作的个性化行动模块。其核心在于，人格（定义为每个用户的独特系统提示）充当中间层：它利用个性化记忆中的洞察来控制智能体行动，而这些行动的结果反过来又会反哺记忆。基于该框架，我们提出了一种测试时用户偏好对齐策略，该策略通过模拟最近的n次交互来优化人格提示，并利用模拟响应与真实响应之间的文本损失反馈，确保实时用户偏好对齐。实验评估表明，PersonaAgent显著优于其他基线方法，不仅有效地个性化了行动空间，而且在测试时真实世界应用中展现出良好的扩展能力。这些结果凸显了我们方法在提供量身定制的、动态的用户体验方面的可行性和潜力。"
    }
]