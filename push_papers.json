[
    {
        "title": "RecGPT: A Foundation Model for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2506.06270v1",
        "pub_date": "2025-06-06",
        "summary": "This work addresses a fundamental barrier in recommender systems: the inability to generalize across domains without extensive retraining. Traditional ID-based approaches fail entirely in cold-start and cross-domain scenarios where new users or items lack sufficient interaction history. Inspired by foundation models' cross-domain success, we develop a foundation model for sequential recommendation that achieves genuine zero-shot generalization capabilities. Our approach fundamentally departs from existing ID-based methods by deriving item representations exclusively from textual features. This enables immediate embedding of any new item without model retraining. We introduce unified item tokenization with Finite Scalar Quantization that transforms heterogeneous textual descriptions into standardized discrete tokens. This eliminates domain barriers that plague existing systems. Additionally, the framework features hybrid bidirectional-causal attention that captures both intra-item token coherence and inter-item sequential dependencies. An efficient catalog-aware beam search decoder enables real-time token-to-item mapping. Unlike conventional approaches confined to their training domains, RecGPT naturally bridges diverse recommendation contexts through its domain-invariant tokenization mechanism. Comprehensive evaluations across six datasets and industrial scenarios demonstrate consistent performance advantages.",
        "translated": "本工作旨在解决推荐系统面临的一个根本性障碍：即在不进行大量再训练的情况下，难以实现跨领域泛化。传统的基于ID的方法在冷启动和跨域场景中完全失效，因为新用户或新物品缺乏足够的交互历史。受基础模型跨域成功的启发，我们开发了一个用于序列推荐的基础模型，它实现了真正的零样本泛化能力。\n\n我们的方法与现有基于ID的方法从根本上不同，因为它仅从文本特征中推导物品表征。这使得任何新物品都无需模型再训练即可立即嵌入。我们引入了采用有限标量量化（FSQ）的统一物品分词方法，将异构文本描述转换为标准化离散词元。这消除了困扰现有系统的领域障碍。此外，该框架还具有混合双向-因果注意力机制，能够同时捕捉物品内词元连贯性和物品间序列依赖性。一个高效的目录感知束搜索解码器能够实现实时词元到物品的映射。\n\n与局限于其训练领域的传统方法不同，RecGPT通过其领域不变的分词机制自然地弥合了多样化的推荐上下文。在六个数据集和工业场景中的全面评估表明了其持续的性能优势。"
    },
    {
        "title": "Optimizing Recall or Relevance? A Multi-Task Multi-Head Approach for\n  Item-to-Item Retrieval in Recommendation",
        "url": "http://arxiv.org/abs/2506.06239v1",
        "pub_date": "2025-06-06",
        "summary": "The task of item-to-item (I2I) retrieval is to identify a set of relevant and highly engaging items based on a given trigger item. It is a crucial component in modern recommendation systems, where users' previously engaged items serve as trigger items to retrieve relevant content for future engagement. However, existing I2I retrieval models in industry are primarily built on co-engagement data and optimized using the recall measure, which overly emphasizes co-engagement patterns while failing to capture semantic relevance. This often leads to overfitting short-term co-engagement trends at the expense of long-term benefits such as discovering novel interests and promoting content diversity. To address this challenge, we propose MTMH, a Multi-Task and Multi-Head I2I retrieval model that achieves both high recall and semantic relevance. Our model consists of two key components: 1) a multi-task learning loss for formally optimizing the trade-off between recall and semantic relevance, and 2) a multi-head I2I retrieval architecture for retrieving both highly co-engaged and semantically relevant items. We evaluate MTMH using proprietary data from a commercial platform serving billions of users and demonstrate that it can improve recall by up to 14.4% and semantic relevance by up to 56.6% compared with prior state-of-the-art models. We also conduct live experiments to verify that MTMH can enhance both short-term consumption metrics and long-term user-experience-related metrics. Our work provides a principled approach for jointly optimizing I2I recall and semantic relevance, which has significant implications for improving the overall performance of recommendation systems.",
        "translated": "物品-物品（I2I）召回的任务是基于给定的触发物品，识别出一组相关且高度吸引人的物品。它是现代推荐系统中的一个关键组成部分，其中用户的历史互动物品可作为触发物品，用于召回相关内容以供未来互动。然而，现有行业中的I2I召回模型主要基于协同互动数据构建，并使用召回率指标进行优化，这过度强调了协同互动模式，却未能捕捉到语义相关性。这通常会导致模型过拟合短期协同互动趋势，从而牺牲了发现新兴趣和促进内容多样性等长期效益。为应对这一挑战，我们提出了MTMH，一个多任务多头（Multi-Task and Multi-Head）I2I召回模型，它能够同时实现高召回率和语义相关性。我们的模型包含两个关键组成部分：1) 一个多任务学习损失函数，用于正式优化召回率和语义相关性之间的权衡；2) 一个多头I2I召回架构，用于召回高度协同互动和语义相关的物品。我们使用来自一个服务数十亿用户的商业平台的专有数据对MTMH进行了评估，结果表明，相比于现有最先进的模型，它能将召回率提升高达14.4%，将语义相关性提升高达56.6%。我们还进行了在线实验，验证MTMH能够提升短期消费指标和长期用户体验相关指标。我们的工作为联合优化I2I召回率和语义相关性提供了一种原则性的方法，这对提升推荐系统的整体性能具有重要意义。"
    },
    {
        "title": "Recommender systems, stigmergy, and the tyranny of popularity",
        "url": "http://arxiv.org/abs/2506.06162v1",
        "pub_date": "2025-06-06",
        "summary": "Scientific recommender systems, such as Google Scholar and Web of Science, are essential tools for discovery. Search algorithms that power work through stigmergy, a collective intelligence mechanism that surfaces useful paths through repeated engagement. While generally effective, this ``rich-get-richer'' dynamic results in a small number of high-profile papers that dominate visibility. This essay argues argue that these algorithm over-reliance on popularity fosters intellectual homogeneity and exacerbates structural inequities, stifling innovative and diverse perspectives critical for scientific progress. We propose an overhaul of search platforms to incorporate user-specific calibration, allowing researchers to manually adjust the weights of factors like popularity, recency, and relevance. We also advise platform developers on how word embeddings and LLMs could be implemented in ways that increase user autonomy. While our suggestions are particularly pertinent to aligning recommender systems with scientific values, these ideas are broadly applicable to information access systems in general. Designing platforms that increase user autonomy is an important step toward more robust and dynamic information",
        "translated": "诸如 Google 学术（Google Scholar）和 Web of Science 等科学推荐系统，是重要的科研发现工具。驱动这些系统运行的搜索算法通过“触发式协作”（stigmergy）机制发挥作用，这是一种通过重复互动来揭示有用路径的集体智能机制。尽管这种机制通常有效，但其“富者愈富”（rich-get-richer）的动态会导致少数备受关注的论文占据主导可见度。\n\n本文认为，这些算法对流行度的过度依赖助长了学术思想的同质化，加剧了结构性不平等，从而扼杀了对科学进步至关重要的创新和多样化视角。我们建议对搜索平台进行彻底改革，以纳入用户特定的校准功能，允许研究人员手动调整流行度、时新度、相关性等因素的权重。我们还就平台开发者如何实施词嵌入（word embeddings）和大型语言模型（LLMs）以提高用户自主性提出建议。\n\n尽管我们的建议在使推荐系统与科学价值观保持一致方面尤其适用，但这些理念也普遍适用于一般的信息获取系统。设计能够提高用户自主性的平台是迈向更强大、更动态信息获取的重要一步。"
    },
    {
        "title": "CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval",
        "url": "http://arxiv.org/abs/2506.06144v1",
        "pub_date": "2025-06-06",
        "summary": "Online video web content is richly multimodal: a single video blends vision, speech, ambient audio, and on-screen text. Retrieval systems typically treat these modalities as independent retrieval sources, which can lead to noisy and subpar retrieval. We explore multimodal video content retrieval, where relevance can be scored from one particular modality or jointly across multiple modalities simultaneously. Consequently, an effective retriever must dynamically choose which modality (or set of modalities) best addresses the query. We introduce CLaMR, a multimodal, late-interaction retriever that jointly indexes 4 modalities: video frames, transcribed speech, on-screen text, and metadata. CLaMR jointly encodes all modalities with a unified multimodal backbone for improved contextualization and is trained to enhance dynamic modality selection via two key innovations. First, given the lack of training data for multimodal retrieval, we introduce MultiVENT 2.0++, a large-scale synthetic training dataset built on MultiVENT 2.0 (event-centric videos in various languages paired with queries) with modality-targeted queries. Next, we propose a modality-aware loss that jointly trains according to a standard contrastive objective alongside an objective for learning correct modality usage. On the test sets of MultiVENT 2.0++ and MSRVTT, conventional aggregation strategies, such as averaging similarities for baseline retrievers, degrade performance by introducing noise from irrelevant modalities. In contrast, CLaMR consistently outperforms existing retrievers: on MultiVENT 2.0++, CLaMR improves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4 over the best multi-modality retriever. We illustrate CLaMR's downstream utility on long-video QA, retrieving relevant frames and obtaining a 3.50% boost over LanguageBind on Video-MME and 1.42% over dense sampling on LongVideoBench.",
        "translated": "在线视频网络内容模态丰富：单个视频融合了视觉、语音、环境音频和屏幕文本。传统的检索系统通常将这些模态视为独立的检索源，这可能导致检索结果嘈杂且不理想。我们探索多模态视频内容检索，其中相关性可以从某一特定模态或同时从多个模态联合评估。因此，一个有效的检索器必须动态地选择哪个模态（或模态组合）最能满足查询需求。我们引入了CLaMR，一个多模态的晚期交互检索器，它联合索引了四种模态：视频帧、转录语音、屏幕文本和元数据。CLaMR通过统一的多模态骨干网络对所有模态进行联合编码，以提升上下文理解能力，并通过两项关键创新来增强动态模态选择的训练。首先，鉴于多模态检索训练数据的缺乏，我们引入了MultiVENT 2.0++，这是一个基于MultiVENT 2.0（包含多种语言的事件中心视频与查询配对）构建的大规模合成训练数据集，并加入了模态导向的查询。其次，我们提出了一种模态感知损失，它根据标准对比目标与学习正确模态使用的目标进行联合训练。在MultiVENT 2.0++和MSRVTT的测试集上，传统的聚合策略（例如对基线检索器进行相似度平均）会因为引入不相关模态的噪声而导致性能下降。相比之下，CLaMR持续超越现有检索器：在MultiVENT 2.0++上，CLaMR相较于最佳单模态检索器，nDCG@10提升了25.6；相较于最佳多模态检索器，提升了35.4。我们展示了CLaMR在长视频问答中的下游应用价值，通过检索相关帧，在Video-MME上比LanguageBind提升了3.50%，在LongVideoBench上比密集采样提升了1.42%。"
    },
    {
        "title": "Phonetically-Augmented Discriminative Rescoring for Voice Search Error\n  Correction",
        "url": "http://arxiv.org/abs/2506.06117v1",
        "pub_date": "2025-06-06",
        "summary": "End-to-end (E2E) Automatic Speech Recognition (ASR) models are trained using paired audio-text samples that are expensive to obtain, since high-quality ground-truth data requires human annotators. Voice search applications, such as digital media players, leverage ASR to allow users to search by voice as opposed to an on-screen keyboard. However, recent or infrequent movie titles may not be sufficiently represented in the E2E ASR system's training data, and hence, may suffer poor recognition.   In this paper, we propose a phonetic correction system that consists of (a) a phonetic search based on the ASR model's output that generates phonetic alternatives that may not be considered by the E2E system, and (b) a rescorer component that combines the ASR model recognition and the phonetic alternatives, and select a final system output.   We find that our approach improves word error rate between 4.4 and 7.6% relative on benchmarks of popular movie titles over a series of competitive baselines.",
        "translated": "端到端（E2E）自动语音识别（ASR）模型使用配对的音频-文本样本进行训练，但这些样本获取成本高昂，因为高质量的真实标注数据需要人工标注者。数字媒体播放器等语音搜索应用利用ASR技术，使用户能够通过语音进行搜索，而非使用屏幕键盘。然而，最近上映或不常见的电影名称可能在E2E ASR系统的训练数据中没有得到充分体现，因此可能导致较差的识别效果。\n\n本文中，我们提出了一种语音校正系统，该系统包含：(a) 基于ASR模型输出的语音搜索，用于生成E2E系统可能未考虑的语音备选项；以及 (b) 一个重打分组件，用于结合ASR模型识别结果和语音备选项，并选择最终的系统输出。\n\n我们发现，与一系列具有竞争力的基线相比，我们的方法在流行电影名称的基准测试中，相对降低了词错误率（WER）4.4%至7.6%。"
    },
    {
        "title": "On the Merits of LLM-Based Corpus Enrichment",
        "url": "http://arxiv.org/abs/2506.06015v1",
        "pub_date": "2025-06-06",
        "summary": "Generative AI (genAI) technologies -- specifically, large language models (LLMs) -- and search have evolving relations. We argue for a novel perspective: using genAI to enrich a document corpus so as to improve query-based retrieval effectiveness. The enrichment is based on modifying existing documents or generating new ones. As an empirical proof of concept, we use LLMs to generate documents relevant to a topic which are more retrievable than existing ones. In addition, we demonstrate the potential merits of using corpus enrichment for retrieval augmented generation (RAG) and answer attribution in question answering.",
        "translated": "生成式人工智能（genAI）技术——特别是大型语言模型（LLMs）——与搜索的关系正在不断演变。我们提出一种新颖的视角：利用生成式人工智能（genAI）技术来丰富文档语料库，以提高基于查询的检索有效性。这种丰富方法基于修改现有文档或生成新文档。作为一项实证性概念验证，我们使用大型语言模型（LLMs）生成了针对特定主题的文档，这些文档比现有文档更易于检索。此外，我们展示了将语料库丰富应用于检索增强生成（RAG）以及问答系统中的答案归因的潜在优势。"
    },
    {
        "title": "Respecting Temporal-Causal Consistency: Entity-Event Knowledge Graphs\n  for Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2506.05939v1",
        "pub_date": "2025-06-06",
        "summary": "Retrieval-augmented generation (RAG) based on large language models often falters on narrative documents with inherent temporal structures. Standard unstructured RAG methods rely solely on embedding-similarity matching and lack any general mechanism to encode or exploit chronological information, while knowledge graph RAG (KG-RAG) frameworks collapse every mention of an entity into a single node, erasing the evolving context that drives many queries. To formalize this challenge and draw the community's attention, we construct ChronoQA, a robust and discriminative QA benchmark that measures temporal, causal, and character consistency understanding in narrative documents (e.g., novels) under the RAG setting. We then introduce Entity-Event RAG (E^2RAG), a dual-graph framework that keeps separate entity and event subgraphs linked by a bipartite mapping, thereby preserving the temporal and causal facets needed for fine-grained reasoning. Across ChronoQA, our approach outperforms state-of-the-art unstructured and KG-based RAG baselines, with notable gains on causal and character consistency queries. E^2RAG therefore offers a practical path to more context-aware retrieval for tasks that require precise answers grounded in chronological information.",
        "translated": "基于大语言模型的检索增强生成（RAG）在处理具有固有时间结构的叙事文档时常常表现不佳。标准的非结构化RAG方法仅依赖于嵌入相似度匹配，并缺乏编码或利用时间信息的通用机制；而知识图谱RAG（KG-RAG）框架则将实体的每一次提及都合并为一个单一节点，抹去了驱动许多查询的演变上下文。\n\n为了形式化这一挑战并引起社区关注，我们构建了ChronoQA，这是一个鲁棒且具有区分度的问答基准，用于衡量RAG设置下叙事文档（例如小说）中的时间、因果和人物一致性理解能力。随后，我们提出了实体-事件RAG（E^2RAG），这是一个双图框架，它通过二分图映射将独立的实体子图和事件子图连接起来，从而保留了细粒度推理所需的时间和因果方面。\n\n在ChronoQA基准测试中，我们的方法优于最先进的非结构化和基于KG的RAG基线，在因果和人物一致性查询上取得了显著提升。因此，E^2RAG为需要基于时间信息提供精确答案的任务，提供了一条更具上下文感知能力的实用检索路径。"
    },
    {
        "title": "Research on Personalized Financial Product Recommendation by Integrating\n  Large Language Models and Graph Neural Networks",
        "url": "http://arxiv.org/abs/2506.05873v1",
        "pub_date": "2025-06-06",
        "summary": "With the rapid growth of fintech, personalized financial product recommendations have become increasingly important. Traditional methods like collaborative filtering or content-based models often fail to capture users' latent preferences and complex relationships. We propose a hybrid framework integrating large language models (LLMs) and graph neural networks (GNNs). A pre-trained LLM encodes text data (e.g., user reviews) into rich feature vectors, while a heterogeneous user-product graph models interactions and social ties. Through a tailored message-passing mechanism, text and graph information are fused within the GNN to jointly optimize embeddings. Experiments on public and real-world financial datasets show our model outperforms standalone LLM or GNN in accuracy, recall, and NDCG, with strong interpretability. This work offers new insights for personalized financial recommendations and cross-modal fusion in broader recommendation tasks.",
        "translated": "随着金融科技的快速发展，个性化金融产品推荐变得愈发重要。传统的协同过滤或基于内容的模型通常难以捕获用户的潜在偏好和复杂的相互关系。为此，我们提出一种融合大语言模型（LLM）和图神经网络（GNN）的混合框架。预训练的LLM负责将文本数据（如用户评论）编码为丰富的特征向量，而异构用户-产品图则用于建模用户与产品之间的交互以及用户的社交关系。通过定制化的消息传递机制，文本和图信息在GNN内部得到融合，从而联合优化嵌入表示。在公开和真实世界的金融数据集上进行的实验表明，我们的模型在准确率、召回率和NDCG方面均优于单独的LLM或GNN，并展现出强大的可解释性。这项工作为个性化金融推荐以及更广泛推荐任务中的跨模态融合提供了新的见解。"
    },
    {
        "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
        "url": "http://arxiv.org/abs/2506.06266v1",
        "pub_date": "2025-06-06",
        "summary": "Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.",
        "translated": "大型语言模型常用于回答基于大型文本语料库（例如代码库、法律文档或聊天记录）的查询，其方法是将整个语料库放入上下文窗口中，并利用上下文学习（ICL）的能力。尽管当前模型支持10万到100万个词元的上下文，但这种设置的服务成本很高，因为KV缓存的内存消耗随输入长度线性增长。\n\n我们探索了一种替代方案：为每个语料库离线训练一个更小的KV缓存。在推理时，我们加载这个经过训练的KV缓存（我们称之为Cartridge），并解码生成响应。关键在于，Cartridge的训练成本可以分摊到所有引用相同语料库的查询中。\n\n然而，我们发现，使用朴素的下一个词元预测方法在语料库上训练Cartridge，其效果无法与ICL媲美。取而代之的是，我们提出了一种名为“自学习”（self-study）的训练方案，其中我们生成关于语料库的合成对话，并利用上下文蒸馏目标来训练Cartridge。我们发现，通过自学习训练的Cartridge能够复现ICL的功能，同时服务成本显著降低。\n\n在具有挑战性的长上下文基准测试中，通过自学习训练的Cartridge在性能上与ICL相当，同时内存使用量减少了38.6倍，吞吐量提高了26.4倍。自学习还扩展了模型的有效上下文长度（例如在MTOB上从12.8万个词元扩展到48.4万个词元），并且令人惊讶的是，它使得Cartridge无需重新训练即可在推理时进行组合。"
    },
    {
        "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at\n  Test Time",
        "url": "http://arxiv.org/abs/2506.06254v1",
        "pub_date": "2025-06-06",
        "summary": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.",
        "translated": "大语言模型（LLM）驱动的智能体近期作为先进范式涌现，在广泛的领域和任务中展现出令人印象深刻的能力。尽管其潜力巨大，当前的LLM智能体却普遍采用“一刀切”的方法，缺乏根据用户多样化需求和偏好进行响应的灵活性。这一局限性促使我们开发了PersonaAgent，这是首个旨在解决多样化个性化任务的LLM个性化智能体框架。具体而言，PersonaAgent整合了两个互补的组件：一个包含情景记忆和语义记忆机制的个性化记忆模块；以及一个使智能体能够执行为用户量身定制的工具行动的个性化行动模块。其核心在于，角色（定义为每个用户的独特系统提示）充当着中介：它利用个性化记忆中的洞察来控制智能体的行动，而这些行动的结果反过来又会优化记忆。基于该框架，我们提出了一种测试时用户偏好对齐策略，该策略通过模拟最近的n次交互来优化角色提示，并利用模拟响应与真实响应之间的文本损失反馈，确保实时用户偏好对齐。实验评估表明，PersonaAgent不仅有效实现了行动空间的个性化，而且在测试时的实际应用中具有良好的可扩展性，显著优于其他基线方法。这些结果凸显了我们方法在提供量身定制、动态用户体验方面的可行性和潜力。"
    },
    {
        "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval",
        "url": "http://arxiv.org/abs/2506.06220v1",
        "pub_date": "2025-06-06",
        "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image retrieval benchmarks. However, bridging this success to real-world applications remains a challenge. In practice, human search behavior is rarely a one-shot action. Instead, it is often a multi-round process guided by clues in mind, that is, a mental image ranging from vague recollections to vivid mental representations of the target image. Motivated by this gap, we study the task of Mental Image Retrieval (MIR), which targets the realistic yet underexplored setting where users refine their search for a mentally envisioned image through multi-round interactions with an image search engine. Central to successful interactive retrieval is the capability of machines to provide users with clear, actionable feedback; however, existing methods rely on indirect or abstract verbal feedback, which can be ambiguous, misleading, or ineffective for users to refine the query. To overcome this, we propose GenIR, a generative multi-round retrieval paradigm leveraging diffusion-based image generation to explicitly reify the AI system's understanding at each round. These synthetic visual representations provide clear, interpretable feedback, enabling users to refine their queries intuitively and effectively. We further introduce a fully automated pipeline to generate a high-quality multi-round MIR dataset. Experimental results demonstrate that GenIR significantly outperforms existing interactive methods in the MIR scenario. This work establishes a new task with a dataset and an effective generative retrieval method, providing a foundation for future research in this direction.",
        "translated": "视觉-语言模型（VLM）在文本到图像检索基准测试中表现出强大的性能。然而，将这一成功推广到现实世界应用仍然是一个挑战。在实践中，人类的搜索行为很少是单次操作。相反，它通常是一个多轮过程，由脑海中的线索所引导，即一种心理图像，从模糊的回忆到目标图像生动的心理表征。受此差距启发，我们研究了心理图像检索（MIR）任务，该任务旨在解决用户通过与图像搜索引擎进行多轮交互来细化其对脑海中构想的图像的搜索这一现实但未充分探索的场景。\n\n成功的交互式检索关键在于机器能够向用户提供清晰、可操作的反馈；然而，现有方法依赖于间接或抽象的语言反馈，这可能对用户细化查询而言是模糊、误导性或低效的。为了克服这一问题，我们提出了GenIR，这是一种生成式多轮检索范式，它利用基于扩散的图像生成技术来明确地具象化AI系统在每一轮中的理解。这些合成视觉表示提供了清晰、可解释的反馈，使用户能够直观且有效地细化其查询。我们进一步引入了一个全自动流程来生成高质量的多轮心理图像检索数据集。实验结果表明，GenIR在心理图像检索场景中显著优于现有的交互式方法。这项工作建立了一项新任务，并提供了数据集和一种有效的生成式检索方法，为未来该方向的研究奠定了基础。"
    },
    {
        "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at\n  Test Time",
        "url": "http://arxiv.org/abs/2506.06254v1",
        "pub_date": "2025-06-06",
        "summary": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.",
        "translated": "大型语言模型（LLM）驱动的智能体近期作为先进范式涌现，在广泛的领域和任务中展现出卓越的能力。尽管其潜力巨大，当前的LLM智能体通常采用“一刀切”的方式，缺乏灵活性以响应用户多样化的需求和偏好。这一局限性促使我们开发了PersonaAgent，这是首个旨在处理多样化个性化任务的个性化LLM智能体框架。具体而言，PersonaAgent集成了两个互补的组件：一个包含情景记忆和语义记忆机制的个性化记忆模块；以及一个使智能体能够执行为用户量身定制的工具操作的个性化行动模块。其核心在于，人格（定义为每个用户的独特系统提示）充当中间层：它利用个性化记忆中的洞察来控制智能体行动，而这些行动的结果反过来又会反哺记忆。基于该框架，我们提出了一种测试时用户偏好对齐策略，该策略通过模拟最近的n次交互来优化人格提示，并利用模拟响应与真实响应之间的文本损失反馈，确保实时用户偏好对齐。实验评估表明，PersonaAgent显著优于其他基线方法，不仅有效地个性化了行动空间，而且在测试时真实世界应用中展现出良好的扩展能力。这些结果凸显了我们方法在提供量身定制的、动态的用户体验方面的可行性和潜力。"
    },
    {
        "title": "Reflect-then-Plan: Offline Model-Based Planning through a Doubly\n  Bayesian Lens",
        "url": "http://arxiv.org/abs/2506.06261v1",
        "pub_date": "2025-06-06",
        "summary": "Offline reinforcement learning (RL) is crucial when online exploration is costly or unsafe but often struggles with high epistemic uncertainty due to limited data. Existing methods rely on fixed conservative policies, restricting adaptivity and generalization. To address this, we propose Reflect-then-Plan (RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach. RefPlan unifies uncertainty modeling and MB planning by recasting planning as Bayesian posterior estimation. At deployment, it updates a belief over environment dynamics using real-time observations, incorporating uncertainty into MB planning via marginalization. Empirical results on standard benchmarks show that RefPlan significantly improves the performance of conservative offline RL policies. In particular, RefPlan maintains robust performance under high epistemic uncertainty and limited data, while demonstrating resilience to changing environment dynamics, improving the flexibility, generalizability, and robustness of offline-learned policies.",
        "translated": "离线强化学习 (RL) 在在线探索成本高昂或不安全时至关重要，但由于数据有限，它往往难以应对高认知不确定性。现有方法依赖固定的保守策略，这限制了其适应性和泛化能力。为解决此问题，我们提出了一种新颖的双重贝叶斯离线基于模型 (MB) 的规划方法：Reflect-then-Plan (RefPlan)。RefPlan 通过将规划重构为贝叶斯后验估计，统一了不确定性建模和MB规划。在部署时，它利用实时观测更新对环境动态的信念，并通过边缘化将不确定性融入MB规划。在标准基准上的实验结果表明，RefPlan 显著提升了保守离线强化学习策略的性能。具体而言，RefPlan 在高认知不确定性和数据有限的情况下仍能保持鲁棒性能，同时展现出对环境动态变化的韧性，从而提升了离线学习策略的灵活性、泛化能力和鲁棒性。"
    },
    {
        "title": "Bridging External and Parametric Knowledge: Mitigating Hallucination of\n  LLMs with Shared-Private Semantic Synergy in Dual-Stream Knowledge",
        "url": "http://arxiv.org/abs/2506.06240v1",
        "pub_date": "2025-06-06",
        "summary": "Retrieval-augmented generation (RAG) is a cost-effective approach to mitigate the hallucination of Large Language Models (LLMs) by incorporating the retrieved external knowledge into the generation process. However, external knowledge may conflict with the parametric knowledge of LLMs. Furthermore, current LLMs lack inherent mechanisms for resolving such knowledge conflicts, making traditional RAG methods suffer from degraded performance and stability. Thus, we propose a Dual-Stream Knowledge-Augmented Framework for Shared-Private Semantic Synergy (DSSP-RAG). Central to the framework is a novel approach that refines self-attention into a mixed-attention, distinguishing shared and private semantics for a controlled internal-external knowledge integration. To effectively facilitate DSSP in RAG, we further introduce an unsupervised hallucination detection method based on cognitive uncertainty, ensuring the necessity of introducing knowledge, and an Energy Quotient (EQ) based on attention difference matrices to reduce noise in the retrieved external knowledge. Extensive experiments on benchmark datasets show that DSSP-RAG can effectively resolve conflicts and enhance the complementarity of dual-stream knowledge, leading to superior performance over strong baselines.",
        "translated": "检索增强生成（RAG）是一种经济高效的方法，通过将检索到的外部知识融入生成过程，以缓解大型语言模型（LLM）的幻觉问题。然而，外部知识可能与LLM的参数知识发生冲突。此外，当前的LLM缺乏解决此类知识冲突的内在机制，导致传统RAG方法在性能和稳定性方面均有所下降。\n\n因此，我们提出了一种用于共享-私有语义协同的双流知识增强框架（DSSP-RAG）。该框架的核心是一种新颖的方法，它将自注意力机制细化为混合注意力机制，以区分共享和私有语义，从而实现对内外部知识的受控整合。为了在RAG中有效促进DSSP，我们进一步引入了一种基于认知不确定性的无监督幻觉检测方法，以确保引入知识的必要性；以及一种基于注意力差异矩阵的能量商（EQ），以减少检索到的外部知识中的噪声。在基准数据集上进行的大量实验表明，DSSP-RAG能够有效解决冲突并增强双流知识的互补性，从而实现超越强大基线的优异性能。"
    },
    {
        "title": "Building Models of Neurological Language",
        "url": "http://arxiv.org/abs/2506.06208v1",
        "pub_date": "2025-06-06",
        "summary": "This report documents the development and evaluation of domain-specific language models for neurology. Initially focused on building a bespoke model, the project adapted to rapid advances in open-source and commercial medical LLMs, shifting toward leveraging retrieval-augmented generation (RAG) and representational models for secure, local deployment. Key contributions include the creation of neurology-specific datasets (case reports, QA sets, textbook-derived data), tools for multi-word expression extraction, and graph-based analyses of medical terminology. The project also produced scripts and Docker containers for local hosting. Performance metrics and graph community results are reported, with future possible work open for multimodal models using open-source architectures like phi-4.",
        "translated": "本报告记录了神经病学领域专用语言模型的开发和评估。项目最初专注于构建定制模型，但随着开源和商业医疗大型语言模型（LLMs）的快速发展，项目策略也随之调整，转而侧重于利用检索增强生成（RAG）和表征模型，以实现安全、本地化部署。主要贡献包括：创建了神经病学专用数据集（涵盖病例报告、问答集和教材衍生数据），开发了多词表达提取工具，并进行了医学术语的基于图的分析。该项目还提供了用于本地部署的脚本和 Docker 容器。报告中提供了性能指标和图社区结果。未来的工作方向可能包括使用 phi-4 等开源架构的多模态模型。"
    },
    {
        "title": "PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels",
        "url": "http://arxiv.org/abs/2506.07606v1",
        "pub_date": "2025-06-09",
        "summary": "Stance detection identifies the viewpoint expressed in text toward a specific target, such as a political figure. While previous datasets have focused primarily on tweet-level stances from established platforms, user-level stance resources, especially on emerging platforms like Bluesky remain scarce. User-level stance detection provides a more holistic view by considering a user's complete posting history rather than isolated posts. We present the first stance detection dataset for the 2024 U.S. presidential election, collected from Bluesky and centered on Kamala Harris and Donald Trump. The dataset comprises 16,044 user-target stance pairs enriched with engagement metadata, interaction graphs, and user posting histories. PolitiSky24 was created using a carefully evaluated pipeline combining advanced information retrieval and large language models, which generates stance labels with supporting rationales and text spans for transparency. The labeling approach achieves 81\\% accuracy with scalable LLMs. This resource addresses gaps in political stance analysis through its timeliness, open-data nature, and user-level perspective. The dataset is available at https://doi.org/10.5281/zenodo.15616911",
        "translated": "立场检测旨在识别文本中针对特定目标（如政治人物）所表达的观点。尽管以往的数据集主要关注来自现有平台的推文级立场，但用户级立场资源，特别是在Bluesky等新兴平台上的此类资源，仍然稀缺。用户级立场检测通过考虑用户完整的发帖历史而非孤立的帖子，提供了一种更全面的视角。我们提出了首个针对2024年美国总统大选的立场检测数据集，该数据集从Bluesky平台收集，并以卡马拉·哈里斯和唐纳德·特朗普为中心。该数据集包含16,044个用户-目标立场对，并辅以互动元数据、互动图谱和用户发帖历史。PolitiSky24是通过结合先进信息检索（IR）技术和大型语言模型（LLM）的精心评估流程创建的，它能生成立场标签，并提供支持性理由和文本片段，以增强透明度。该标注方法在使用可扩展大型语言模型时，能达到81%的准确率。该资源通过其及时性、开放数据特性和用户级视角，弥补了政治立场分析中的不足。该数据集可在 https://doi.org/10.5281/zenodo.15616911 获取。"
    },
    {
        "title": "MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with\n  Expert Specialization",
        "url": "http://arxiv.org/abs/2506.07563v2",
        "pub_date": "2025-06-09",
        "summary": "Personalized recommendation systems must adapt to user interactions across different domains. Traditional approaches like MLoRA apply a single adaptation per domain but lack flexibility in handling diverse user behaviors. To address this, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is first trained independently to specialize in its domain before a gating network is trained to weight their contributions dynamically. We evaluate MoE-MLoRA across eight CTR models on Movielens and Taobao, showing that it improves performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20) but offers limited benefits in structured datasets with low domain diversity and sparsity. Further analysis of the number of experts per domain reveals that larger ensembles do not always improve performance, indicating the need for model-aware tuning. Our findings highlight the potential of expert-based architectures for multi-domain recommendation systems, demonstrating that task-aware specialization and adaptive gating can enhance predictive accuracy in complex environments. The implementation and code are available in our GitHub repository.",
        "translated": "个性化推荐系统必须适应跨域的用户交互。传统的MLoRA等方法在每个域采用单一的适配策略，但在处理多样化的用户行为时缺乏灵活性。为解决此问题，我们提出了MoE-MLoRA，这是一个混合专家（MoE）框架。在该框架中，每个专家首先被独立训练，以专注于其特定领域，然后训练一个门控网络来动态加权它们的贡献。\n\n我们在Movielens和淘宝数据集上的八种点击率（CTR）模型上评估了MoE-MLoRA，结果表明，它在大规模、动态数据集（在Taobao-20上加权AUC提升了1.45）中显著提升了性能，但在领域多样性低和稀疏的结构化数据集中收益有限。对每个域的专家数量的进一步分析表明，更大的集成模型并非总能提升性能，这预示着需要进行模型感知（model-aware）调优。\n\n我们的研究结果凸显了基于专家的架构在多域推荐系统中的潜力，证明了任务感知专业化和自适应门控能够在复杂环境中提高预测准确性。本研究的实现代码已在我们的GitHub仓库中开源。"
    },
    {
        "title": "Addressing Correlated Latent Exogenous Variables in Debiased Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2506.07517v1",
        "pub_date": "2025-06-09",
        "summary": "Recommendation systems (RS) aim to provide personalized content, but they face a challenge in unbiased learning due to selection bias, where users only interact with items they prefer. This bias leads to a distorted representation of user preferences, which hinders the accuracy and fairness of recommendations. To address the issue, various methods such as error imputation based, inverse propensity scoring, and doubly robust techniques have been developed. Despite the progress, from the structural causal model perspective, previous debiasing methods in RS assume the independence of the exogenous variables. In this paper, we release this assumption and propose a learning algorithm based on likelihood maximization to learn a prediction model. We first discuss the correlation and difference between unmeasured confounding and our scenario, then we propose a unified method that effectively handles latent exogenous variables. Specifically, our method models the data generation process with latent exogenous variables under mild normality assumptions. We then develop a Monte Carlo algorithm to numerically estimate the likelihood function. Extensive experiments on synthetic datasets and three real-world datasets demonstrate the effectiveness of our proposed method. The code is at https://github.com/WallaceSUI/kdd25-background-variable.",
        "translated": "推荐系统（RS）旨在提供个性化内容，但由于选择偏差（即用户只与他们偏好的项目进行交互），它们在无偏学习方面面临挑战。这种偏差导致用户偏好表示失真，进而阻碍了推荐的准确性和公平性。为了解决这个问题，研究人员已经开发了各种方法，例如基于误差插补、逆倾向加权和双重鲁棒技术。\n\n尽管取得了进展，但从结构因果模型的角度来看，以往推荐系统中的去偏方法都假设外生变量是独立的。在本文中，我们放宽了这一假设，并提出了一种基于似然最大化的学习算法来学习一个预测模型。我们首先讨论了未测量混杂变量与我们场景之间的关联与区别，然后提出了一种统一的方法，能够有效处理潜在外生变量。具体来说，我们的方法在温和的正态性假设下，对带有潜在外生变量的数据生成过程进行了建模。随后，我们开发了一种蒙特卡洛算法来数值估计似然函数。在合成数据集和三个真实世界数据集上进行的大量实验证明了我们所提出方法的有效性。\n\n代码位于：https://github.com/WallaceSUI/kdd25-background-variable。"
    },
    {
        "title": "Leveraging Historical and Current Interests for Continual Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.07466v1",
        "pub_date": "2025-06-09",
        "summary": "Sequential recommendation models based on the Transformer architecture show superior performance in harnessing long-range dependencies within user behavior via self-attention. However, naively updating them on continuously arriving non-stationary data streams incurs prohibitive computation costs or leads to catastrophic forgetting. To address this, we propose Continual Sequential Transformer for Recommendation (CSTRec) that effectively leverages well-preserved historical user interests while capturing current interests. At its core is Continual Sequential Attention (CSA), a linear attention mechanism that retains past knowledge without direct access to old data. CSA integrates two key components: (1) Cauchy-Schwarz Normalization that stabilizes training under uneven interaction frequencies, and (2) Collaborative Interest Enrichment that mitigates forgetting through shared, learnable interest pools. We further introduce a technique that facilitates learning for cold-start users by transferring historical knowledge from behaviorally similar existing users. Extensive experiments on three real-world datasets indicate that CSTRec outperforms state-of-the-art baselines in both knowledge retention and acquisition.",
        "translated": "基于Transformer架构的序列推荐模型通过自注意力机制，在捕获用户行为序列中的长程依赖方面表现出卓越的性能。然而，若对其在持续到达的非平稳数据流上进行朴素更新，则会产生高昂的计算成本或导致灾难性遗忘。为解决此问题，我们提出了持续序列Transformer推荐模型（CSTRec），该模型能够有效利用保存完好的历史用户兴趣，同时捕获当前兴趣。\n\n其核心是持续序列注意力机制（CSA），这是一种线性注意力机制，能够在无需直接访问旧数据的情况下保留过往知识。CSA集成了两个关键组件：(1) 柯西-施瓦茨归一化（Cauchy-Schwarz Normalization），旨在稳定不均匀交互频率下的训练；以及(2) 协作兴趣丰富（Collaborative Interest Enrichment），通过共享的可学习兴趣池缓解遗忘。我们进一步引入了一种技术，通过从行为相似的现有用户中迁移历史知识，从而促进冷启动用户的学习。在三个真实世界数据集上进行的大量实验表明，CSTRec在知识保留和获取两方面均超越了最先进的基线模型。"
    },
    {
        "title": "LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework\n  for LLM-Based Ranking",
        "url": "http://arxiv.org/abs/2506.07449v1",
        "pub_date": "2025-06-09",
        "summary": "Recent advances in Large Language Models (LLMs) have driven their adoption in recommender systems through Retrieval-Augmented Generation (RAG) frameworks. However, existing RAG approaches predominantly rely on flat, similarity-based retrieval that fails to leverage the rich relational structure inherent in user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass, end-to-end trainable framework that integrates personalized knowledge graph context into LLM-based recommendation ranking. Our approach extends the LlamaRec architecture by incorporating a lightweight user preference module that dynamically identifies salient relation paths within a heterogeneous knowledge graph constructed from user behavior and item metadata. These personalized subgraphs are seamlessly integrated into prompts for a fine-tuned Llama-2 model, enabling efficient and interpretable recommendations through a unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty datasets demonstrate consistent and significant improvements over LlamaRec across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates the critical value of structured reasoning in LLM-based recommendations and establishes a foundation for scalable, knowledge-aware personalization in next-generation recommender systems. Code is available at~\\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.",
        "translated": "大型语言模型（LLM）的最新进展，推动了它们通过检索增强生成（RAG）框架在推荐系统中的应用。然而，现有的RAG方法主要依赖于扁平的、基于相似度的检索，未能充分利用用户-物品交互中固有的丰富关系结构。\n\n我们引入了LlamaRec-LKG-RAG，这是一种新颖的单次、端到端可训练的框架，它将个性化知识图谱上下文集成到基于LLM的推荐排名中。我们的方法扩展了LlamaRec架构，通过引入一个轻量级的用户偏好模块，该模块能够动态识别从用户行为和物品元数据构建的异构知识图谱中的显著关系路径。这些个性化子图被无缝集成到微调后的Llama-2模型的提示中，通过统一的推理步骤实现高效且可解释的推荐。\n\n在ML-100K和Amazon Beauty数据集上进行的全面实验表明，相对于LlamaRec，该方法在关键排名指标（MRR、NDCG、Recall）上取得了持续且显著的改进。LlamaRec-LKG-RAG证明了结构化推理在基于LLM的推荐中的关键价值，并为下一代推荐系统中可扩展、知识感知的个性化奠定了基础。代码已开源于[仓库](https://github.com/VahidAz/LlamaRec-LKG-RAG)。"
    },
    {
        "title": "HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language\n  Models for Efficient Multimodal Hotel Retrieval",
        "url": "http://arxiv.org/abs/2506.07296v1",
        "pub_date": "2025-06-08",
        "summary": "We present HotelMatch-LLM, a multimodal dense retrieval model for the travel domain that enables natural language property search, addressing the limitations of traditional travel search engines which require users to start with a destination and editing search parameters. HotelMatch-LLM features three key innovations: (1) Domain-specific multi-task optimization with three novel retrieval, visual, and language modeling objectives; (2) Asymmetrical dense retrieval architecture combining a small language model (SLM) for efficient online query processing and a large language model (LLM) for embedding hotel data; and (3) Extensive image processing to handle all property image galleries. Experiments on four diverse test sets show HotelMatch-LLM significantly outperforms state-of-the-art models, including VISTA and MARVEL. Specifically, on the test set -- main query type -- we achieve 0.681 for HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our analysis highlights the impact of our multi-task optimization, the generalizability of HotelMatch-LLM across LLM architectures, and its scalability for processing large image galleries.",
        "translated": "我们提出了 HotelMatch-LLM，这是一种专为旅游领域设计的多模态稠密检索模型，它支持自然语言酒店信息搜索，解决了传统旅游搜索引擎要求用户必须先选择目的地并手动调整搜索参数的局限性。HotelMatch-LLM 具有三项关键创新：(1) 领域特定多任务优化，包含三个新颖的检索、视觉和语言建模目标；(2) 非对称稠密检索架构，结合了用于高效在线查询处理的小型语言模型（SLM）和用于嵌入酒店数据的大型语言模型（LLM）；以及 (3) 大量的图像处理能力，以处理所有酒店的图片库。在四个多样化的测试集上进行的实验表明，HotelMatch-LLM 显著优于包括 VISTA 和 MARVEL 在内的最先进模型。具体而言，在测试集（主要查询类型）上，HotelMatch-LLM 的性能指标达到了 0.681，而最有效的基线模型 MARVEL 仅为 0.603。我们的分析强调了多任务优化的影响、HotelMatch-LLM 在不同 LLM 架构下的泛化能力，以及其处理大型图片库的可扩展性。"
    },
    {
        "title": "RADAR: Recall Augmentation through Deferred Asynchronous Retrieval",
        "url": "http://arxiv.org/abs/2506.07261v1",
        "pub_date": "2025-06-08",
        "summary": "Modern large-scale recommender systems employ multi-stage ranking funnel (Retrieval, Pre-ranking, Ranking) to balance engagement and computational constraints (latency, CPU). However, the initial retrieval stage, often relying on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles to effectively surface the most engaging items from billion-scale catalogs, particularly distinguishing highly relevant and engaging candidates from merely relevant ones. We introduce Recall Augmentation through Deferred Asynchronous Retrieval (RADAR), a novel framework that leverages asynchronous, offline computation to pre-rank a significantly larger candidate set for users using the full complexity ranking model. These top-ranked items are stored and utilized as a high-quality retrieval source during online inference, bypassing online retrieval and pre-ranking stages for these candidates. We demonstrate through offline experiments that RADAR significantly boosts recall (2X Recall@200 vs DNN retrieval baseline) by effectively combining a larger retrieved candidate set with a more powerful ranking model. Online A/B tests confirm a +0.8% lift in topline engagement metrics, validating RADAR as a practical and effective method to improve recommendation quality under strict online serving constraints.",
        "translated": "现代大规模推荐系统采用多阶段排序漏斗（召回、初排、精排）以平衡用户参与度与计算资源限制（如延迟、CPU）。然而，初始召回阶段通常依赖于K近邻（KNN）等高效但不那么精确的方法，这使得它难以在十亿级别的商品目录中有效发现最具吸引力的物品，尤其难以区分高度相关且能吸引用户参与的候选物品与仅仅相关的物品。\n\n我们提出了通过延迟异步召回进行召回增强（RADAR）这一新颖框架，它利用异步离线计算，使用全复杂度排序模型为用户预先排序一个显著更大的候选集。这些排名靠前的物品被存储起来，并在在线推理期间用作高质量的召回源，从而使这些候选物品绕过在线召回和初排阶段。我们通过离线实验证明，RADAR通过有效结合更大的召回候选集和更强大的排序模型，显著提升了召回率（相较于DNN召回基线，Recall@200提升2倍）。在线A/B测试证实，核心用户参与度指标提升了0.8%，验证了RADAR是在严格在线服务约束下提升推荐质量的实用且有效方法。"
    },
    {
        "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
        "url": "http://arxiv.org/abs/2506.07976v2",
        "pub_date": "2025-06-09",
        "summary": "The current paradigm of test-time scaling relies on generating long reasoning traces (\"thinking\" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.",
        "translated": "当前测试时规模化的范式依赖于在生成响应之前生成长的推理轨迹（即“思考”更多）。在需要交互的智能体问题中，这可以通过在环境中行动之前生成思考轨迹来实现。然而，这种方法不允许智能体从环境中获取新信息，也无法随时间推移调整其行为。\n\n在这项工作中，我们提出对测试时交互进行规模化，这是测试时规模化一个尚未开发的维度，它增加了智能体的交互视野，从而能够在单次推演（rollout）中运行丰富的行为，例如探索、回溯和动态重新规划。为了证明这一规模化维度的潜力，我们研究了网络智能体领域。我们首先展示，即使是基于提示的交互规模化，在没有任何训练的情况下，也能显著提高网络基准上的任务成功率。在此基础上，我们引入了TTI（Test-Time Interaction，测试时交互），这是一种基于课程的在线强化学习（RL）方法，通过自适应调整智能体的推演长度来训练它们。\n\n使用Gemma 3 12B模型，TTI在WebVoyager和WebArena基准上生成了最先进的开源、开放数据网络智能体。我们进一步表明，TTI使智能体能够自适应地平衡探索与利用。我们的结果确立了交互规模化作为扩展每步计算量（per-step compute）的强大且互补的维度，为训练自适应智能体提供了新途径。"
    },
    {
        "title": "Discrete Scale-invariant Metric Learning for Efficient Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2506.09898v1",
        "pub_date": "2025-06-11",
        "summary": "Metric learning has attracted extensive interest for its ability to provide personalized recommendations based on the importance of observed user-item interactions. Current metric learning methods aim to push negative items away from the corresponding users and positive items by an absolute geometrical distance margin. However, items may come from imbalanced categories with different intra-class variations. Thus, the absolute distance margin may not be ideal for estimating the difference between user preferences over imbalanced items. To this end, we propose a new method, named discrete scale-invariant metric learning (DSIML), by adding binary constraints to users and items, which maps users and items into binary codes of a shared Hamming subspace to speed up the online recommendation. Specifically, we firstly propose a scale-invariant margin based on angles at the negative item points in the shared Hamming subspace. Then, we derive a scale-invariant triple hinge loss based on the margin. To capture more preference difference information, we integrate a pairwise ranking loss into the scale-invariant loss in the proposed model. Due to the difficulty of directly optimizing the mixed integer optimization problem formulated with \\textit{log-sum-exp} functions, we seek to optimize its variational quadratic upper bound and learn hash codes with an alternating optimization strategy. Experiments on benchmark datasets clearly show that our proposed method is superior to competitive metric learning and hashing-based baselines for recommender systems. The implementation code is available at https://github.com/AnonyFeb/dsml.",
        "translated": "度量学习因其能够基于观测到的用户-物品交互来提供个性化推荐而受到了广泛关注。当前的度量学习方法旨在通过一个绝对的几何距离间隔将负样本物品推离相应的用户和正样本物品。然而，物品可能来自不平衡的类别，并具有不同的类内变异性。因此，绝对距离间隔可能不适合估计用户对不平衡物品的偏好差异。为此，我们提出了一种新方法，名为离散尺度不变度量学习（DSIML），该方法通过对用户和物品添加二值约束，将它们映射到共享汉明子空间的二值编码中，以加速在线推荐。具体而言，我们首先在共享汉明子空间中，基于负样本物品点处的角度，提出了一个尺度不变的间隔。然后，我们基于该间隔推导出了一个尺度不变的三元合页损失。为了捕获更多的偏好差异信息，我们在所提出的模型中，将一个成对排序损失整合到尺度不变损失中。由于直接优化使用 \\textit{log-sum-exp} 函数表述的混合整数优化问题存在困难，我们寻求优化其变分二次上界，并采用交替优化策略来学习哈希码。在基准数据集上的实验清楚地表明，我们提出的方法在推荐系统方面优于具有竞争力的度量学习和基于哈希的基线方法。实施代码已在 https://github.com/AnonyFeb/dsml 发布。"
    },
    {
        "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data\n  Augmentation Strategies for Knowledge Graph Question Answering",
        "url": "http://arxiv.org/abs/2506.09414v1",
        "pub_date": "2025-06-11",
        "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural language processing that requires reasoning over knowledge graphs (KGs) to answer natural language questions. Recent methods utilizing large language models (LLMs) have shown remarkable semantic parsing capabilities but are limited by the scarcity of diverse annotated data and multi-hop reasoning samples. Traditional data augmentation approaches are focus mainly on single-hop questions and prone to semantic distortion, while LLM-based methods primarily address semantic distortion but usually neglect multi-hop reasoning, thus limiting data diversity. The scarcity of multi-hop samples further weakens models' generalization. To address these issues, we propose PGDA-KGQA, a prompt-guided generative framework with multiple data augmentation strategies for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by crafting meticulously engineered prompts that integrate the provided textual content, it leverages LLMs to generate large-scale (question, logical form) pairs for model training. Specifically, PGDA-KGQA enriches its training set by: (1) generating single-hop pseudo questions to improve the alignment of question semantics with KG relations; (2) applying semantic-preserving question rewriting to improve robustness against linguistic variations; (3) employing answer-guided reverse path exploration to create realistic multi-hop questions. By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA utilizes the augmented data to enhance the accuracy of logical form generation and thus improve answer retrieval performance. Experiments demonstrate that outperforms state-of-the-art methods on standard KGQA datasets, achieving improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by 1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.",
        "translated": "知识图谱问答（KGQA）是自然语言处理中的一项关键任务，它需要对知识图谱（KGs）进行推理以回答自然语言问题。近期利用大语言模型（LLMs）的方法已展现出卓越的语义解析能力，但受限于多样化标注数据和多跳推理样本的稀缺性。传统数据增强方法主要侧重于单跳问题且易产生语义失真，而基于LLM的方法主要解决语义失真但通常忽略多跳推理，从而限制了数据多样性。多跳样本的稀缺性进一步削弱了模型的泛化能力。\n\n为了解决这些问题，我们提出了PGDA-KGQA，这是一个提示引导的生成框架，其中包含多种KGQA数据增强策略。其核心是，PGDA-KGQA采用统一的提示设计范式：通过精心设计整合了所提供文本内容的提示，它利用LLMs生成大规模的（问题，逻辑形式）对以用于模型训练。具体而言，PGDA-KGQA通过以下方式丰富其训练集：(1) 生成单跳伪问题，以提高问题语义与知识图谱关系的对齐度；(2) 应用语义保持的问题重写，以提高对语言变体的鲁棒性；(3) 采用答案引导的逆向路径探索，以创建真实的多跳问题。通过采用增强-生成-检索的语义解析流水线，PGDA-KGQA利用增强后的数据提高逻辑形式生成的准确性，从而提升答案检索性能。实验表明，PGDA-KGQA在标准KGQA数据集上优于最先进的方法，在WebQSP数据集上F1、Hits@1和Accuracy分别提升了2.8%、1.2%和3.1%，在ComplexWebQuestions数据集上则分别提升了1.8%、1.1%和2.4%。"
    },
    {
        "title": "MAGMaR Shared Task System Description: Video Retrieval with OmniEmbed",
        "url": "http://arxiv.org/abs/2506.09409v1",
        "pub_date": "2025-06-11",
        "summary": "Effective video retrieval remains challenging due to the complexity of integrating visual, auditory, and textual modalities. In this paper, we explore unified retrieval methods using OmniEmbed, a powerful multimodal embedding model from the Tevatron 2.0 toolkit, in the context of the MAGMaR shared task. Evaluated on the comprehensive MultiVENT 2.0 dataset, OmniEmbed generates unified embeddings for text, images, audio, and video, enabling robust multimodal retrieval. By finetuning OmniEmbed with the combined multimodal data--visual frames, audio tracks, and textual descriptions provided in MultiVENT 2.0, we achieve substantial improvements in complex, multilingual video retrieval tasks. Our submission achieved the highest score on the MAGMaR shared task leaderboard among public submissions as of May 20th, 2025, highlighting the practical effectiveness of our unified multimodal retrieval approach. Model checkpoint in this work is opensourced.",
        "translated": "由于视觉、听觉和文本模态整合的复杂性，有效的视频检索仍然面临挑战。本文探讨了在 MAGMaR 共享任务背景下，使用 Tevatron 2.0 工具包中强大的多模态嵌入模型 OmniEmbed 来实现统一检索的方法。\n\nOmniEmbed 在综合性的 MultiVENT 2.0 数据集上进行评估，能够为文本、图像、音频和视频生成统一的嵌入表示，从而实现鲁棒的多模态检索。通过利用 MultiVENT 2.0 中提供的视觉帧、音频轨和文本描述等组合多模态数据对 OmniEmbed 进行微调，我们在复杂的、多语言视频检索任务中取得了显著提升。\n\n截至 2025 年 5 月 20 日，我们的提交方案在 MAGMaR 共享任务排行榜的公开提交中获得了最高分，这凸显了我们统一多模态检索方法的实用有效性。本文使用的模型检查点已开源。"
    },
    {
        "title": "ThinkQE: Query Expansion via an Evolving Thinking Process",
        "url": "http://arxiv.org/abs/2506.09260v1",
        "pub_date": "2025-06-10",
        "summary": "Effective query expansion for web search benefits from promoting both exploration and result diversity to capture multiple interpretations and facets of a query. While recent LLM-based methods have improved retrieval performance and demonstrate strong domain generalization without additional training, they often generate narrowly focused expansions that overlook these desiderata. We propose ThinkQE, a test-time query expansion framework addressing this limitation through two key components: a thinking-based expansion process that encourages deeper and comprehensive semantic exploration, and a corpus-interaction strategy that iteratively refines expansions using retrieval feedback from the corpus. Experiments on diverse web search benchmarks (DL19, DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches, including training-intensive dense retrievers and rerankers.",
        "translated": "网络搜索中有效的查询扩展，得益于促进探索性和结果多样性，以便捕获查询的多重释义和多重维度。尽管近期基于大语言模型（LLM）的方法在无需额外训练的情况下，提升了检索性能并展现出强大的领域泛化能力，但它们通常会生成范围过于狭窄的扩展，从而忽略了上述理想特性。\n\n为此，我们提出了ThinkQE，一个测试时（test-time）查询扩展框架，旨在解决这一局限性。该框架通过两个关键组件来实现：一是基于思考的扩展过程，旨在促进更深层次、更全面的语义探索；二是语料库交互策略，该策略利用来自语料库的检索反馈迭代地完善查询扩展。在多样化的网络搜索基准测试集（DL19、DL20和BRIGHT）上的实验表明，ThinkQE持续优于现有方法，包括那些需要大量训练的稠密检索器和重排序器。"
    },
    {
        "title": "In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation",
        "url": "http://arxiv.org/abs/2506.09221v1",
        "pub_date": "2025-06-10",
        "summary": "The spread of online misinformation poses serious threats to democratic societies. Traditionally, expert fact-checkers verify the truthfulness of information through investigative processes. However, the volume and immediacy of online content present major scalability challenges. Crowdsourcing offers a promising alternative by leveraging non-expert judgments, but it introduces concerns about bias, accuracy, and interpretability. This thesis investigates how human intelligence can be harnessed to assess the truthfulness of online information, focusing on three areas: misinformation assessment, cognitive biases, and automated fact-checking systems. Through large-scale crowdsourcing experiments and statistical modeling, it identifies key factors influencing human judgments and introduces a model for the joint prediction and explanation of truthfulness. The findings show that non-expert judgments often align with expert assessments, particularly when factors such as timing and experience are considered. By deepening our understanding of human judgment and bias in truthfulness assessment, this thesis contributes to the development of more transparent, trustworthy, and interpretable systems for combating misinformation.",
        "translated": "网络虚假信息的传播对民主社会构成严重威胁。传统上，专家事实核查员通过调查流程核实信息的真实性。然而，网络内容的海量体量和即时性带来了重大的可扩展性挑战。众包通过利用非专业人士的判断，提供了一种有前景的替代方案，但它也引发了对偏见、准确性和可解释性的担忧。\n\n本论文研究了如何利用人类智能来评估网络信息的真实性，侧重于三个领域：虚假信息评估、认知偏差和自动化事实核查系统。通过大规模众包实验和统计建模，本论文识别出影响人类判断的关键因素，并引入了一个用于真实性联合预测和解释的模型。研究结果表明，非专业人士的判断往往与专家评估一致，尤其是在考虑时效性和经验等因素时。通过深化我们对真实性评估中人类判断和偏见的理解，本论文为开发更透明、更值得信赖、更可解释的打击虚假信息系统做出了贡献。"
    },
    {
        "title": "Revisiting Graph Projections for Effective Complementary Product\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.09209v1",
        "pub_date": "2025-06-10",
        "summary": "Complementary product recommendation is a powerful strategy to improve customer experience and retail sales. However, recommending the right product is not a simple task because of the noisy and sparse nature of user-item interactions. In this work, we propose a simple yet effective method to predict a list of complementary products given a query item, based on the structure of a directed weighted graph projected from the user-item bipartite graph. We revisit bipartite graph projections for recommender systems and propose a novel approach for inferring complementarity relationships from historical user-item interactions. We compare our model with recent methods from the literature and show, despite the simplicity of our approach, an average improvement of +43% and +38% over sequential and graph-based recommenders, respectively, over different benchmarks.",
        "translated": "互补商品推荐是提升客户体验和零售额的强有力策略。然而，由于用户-商品交互中存在的噪声和稀疏性，推荐合适的产品并非易事。在本文中，我们提出了一种简单而有效的方法，用于在给定一个查询商品的情况下，基于从用户-商品二分图（user-item bipartite graph）投影得到的有向加权图（directed weighted graph）结构来预测互补商品列表。我们重新审视了用于推荐系统的二分图投影，并提出了一种从历史用户-商品交互中推断互补关系的新颖方法。我们将我们的模型与文献中最新的方法进行比较，结果表明，尽管我们的方法很简单，但在不同的基准数据集上，相较于序列推荐器（sequential recommenders）和基于图的推荐器（graph-based recommenders），我们的方法平均分别取得了43%和38%的提升。"
    },
    {
        "title": "Multimodal Representation Alignment for Cross-modal Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.08774v1",
        "pub_date": "2025-06-10",
        "summary": "Different machine learning models can represent the same underlying concept in different ways. This variability is particularly valuable for in-the-wild multimodal retrieval, where the objective is to identify the corresponding representation in one modality given another modality as input. This challenge can be effectively framed as a feature alignment problem. For example, given a sentence encoded by a language model, retrieve the most semantically aligned image based on features produced by an image encoder, or vice versa. In this work, we first investigate the geometric relationships between visual and textual embeddings derived from both vision-language models and combined unimodal models. We then align these representations using four standard similarity metrics as well as two learned ones, implemented via neural networks. Our findings indicate that the Wasserstein distance can serve as an informative measure of the modality gap, while cosine similarity consistently outperforms alternative metrics in feature alignment tasks. Furthermore, we observe that conventional architectures such as multilayer perceptrons are insufficient for capturing the complex interactions between image and text representations. Our study offers novel insights and practical considerations for researchers working in multimodal information retrieval, particularly in real-world, cross-modal applications.",
        "translated": "不同的机器学习模型能够以不同的方式表征相同的底层概念。这种多样性对于实际应用场景下的多模态检索尤为重要，其目标是在给定一种模态作为输入时，识别出另一种模态中对应的表征。这一挑战可以被有效地建模为一个特征对齐问题。例如，给定一个由语言模型编码的句子，基于图像编码器生成的特征检索语义上最对齐的图像，反之亦然。\n\n在这项工作中，我们首先探究了分别从视觉-语言模型和组合式单模态模型中获得的视觉嵌入和文本嵌入之间的几何关系。随后，我们使用四种标准相似性度量以及两种通过神经网络实现的学习型度量对这些表征进行对齐。我们的研究结果表明，Wasserstein距离可以作为模态间隙的有效衡量标准，而余弦相似度在特征对齐任务中始终优于其他备选度量。此外，我们观察到多层感知机（MLP）等传统架构不足以捕获图像和文本表征之间复杂的交互作用。我们的研究为多模态信息检索领域的研究人员提供了新颖的见解和实用性考量，尤其是在真实世界的跨模态应用中。"
    },
    {
        "title": "Paths to Causality: Finding Informative Subgraphs Within Knowledge\n  Graphs for Knowledge-Based Causal Discovery",
        "url": "http://arxiv.org/abs/2506.08771v1",
        "pub_date": "2025-06-10",
        "summary": "Inferring causal relationships between variable pairs is crucial for understanding multivariate interactions in complex systems. Knowledge-based causal discovery -- which involves inferring causal relationships by reasoning over the metadata of variables (e.g., names or textual context) -- offers a compelling alternative to traditional methods that rely on observational data. However, existing methods using Large Language Models (LLMs) often produce unstable and inconsistent results, compromising their reliability for causal inference. To address this, we introduce a novel approach that integrates Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery. Our approach identifies informative metapath-based subgraphs within KGs and further refines the selection of these subgraphs using Learning-to-Rank-based models. The top-ranked subgraphs are then incorporated into zero-shot prompts, improving the effectiveness of LLMs in inferring the causal relationship. Extensive experiments on biomedical and open-domain datasets demonstrate that our method outperforms most baselines by up to 44.4 points in F1 scores, evaluated across diverse LLMs and KGs. Our code and datasets are available on GitHub: https://github.com/susantiyuni/path-to-causality",
        "translated": "推断变量对之间的因果关系对于理解复杂系统中的多变量交互至关重要。基于知识的因果发现——通过对变量的元数据（例如，名称或文本上下文）进行推理来推断因果关系——为依赖观测数据的传统方法提供了一种引人注目的替代方法。然而，现有使用大型语言模型（LLM）的方法经常产生不稳定和不一致的结果，损害了它们进行因果推断的可靠性。\n\n为此，我们提出了一种新颖的方法，将知识图谱（KG）与LLM相结合，以增强基于知识的因果发现。我们的方法识别KG中信息丰富的元路径子图，并使用基于学习排序（Learning-to-Rank）的模型进一步优化这些子图的选择。排名靠前的子图随后被纳入零样本提示中，从而提高了LLM在推断因果关系方面的有效性。在生物医学和开放域数据集上进行的大量实验表明，我们的方法在F1分数上最高可优于大多数基线44.4点，并在多种LLM和KG上进行了评估。我们的代码和数据集可在GitHub上获取：https://github.com/susantiyuni/path-to-causality"
    },
    {
        "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and\n  Re-ranking",
        "url": "http://arxiv.org/abs/2506.09944v1",
        "pub_date": "2025-06-11",
        "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. We identify QRHEAD by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). We further introduce QR- RETRIEVER, an efficient and effective retriever that uses the accumulated attention mass of QRHEAD as retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. We also evaluate QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the querycontext attention scoring and task selection are crucial for identifying QRHEAD with strong downstream utility. Overall, our work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs.",
        "translated": "近期研究（Wu 等人，2025b）已识别出“检索头”（retrieval heads），它们是注意力头的一个子集，负责在长上下文语言模型（LM）中检索关键信息，并通过其在“大海捞针”任务中的复制粘贴行为进行衡量。在本文中，我们引入了 QRHEAD（查询聚焦检索头），这是一组改进的注意力头，能够增强从长上下文中的信息检索能力。我们通过结合输入查询聚合注意力分数来识别 QRHEAD，并利用少量真实世界任务（例如长上下文问答）的示例。我们进一步引入了 QR-RETRIEVER，这是一种高效且有效的检索器，它使用 QRHEAD 的累积注意力权重作为检索分数。我们将 QR-RETRIEVER 用于长上下文推理，通过选择检索分数最高的、最相关的部分。在多跳推理任务 LongMemEval 和 CLIPPER 上，这使得性能相较于完整上下文提升了 10% 以上，并优于强大的稠密检索器。我们还将 QR-RETRIEVER 作为重排器在 BEIR 基准测试上进行了评估，发现它实现了强大的零样本性能，优于其他基于大型语言模型（LLM）的重排器，例如 RankGPT。进一步分析表明，查询-上下文注意力评分和任务选择对于识别具有强大下游效用的 QRHEAD 都至关重要。总而言之，我们的工作贡献了一个通用检索器，并为语言模型的长上下文能力提供了可解释性见解。"
    },
    {
        "title": "Aspect-Based Opinion Summarization with Argumentation Schemes",
        "url": "http://arxiv.org/abs/2506.09917v1",
        "pub_date": "2025-06-11",
        "summary": "Reviews are valuable resources for customers making purchase decisions in online shopping. However, it is impractical for customers to go over the vast number of reviews and manually conclude the prominent opinions, which prompts the need for automated opinion summarization systems. Previous approaches, either extractive or abstractive, face challenges in automatically producing grounded aspect-centric summaries. In this paper, we propose a novel summarization system that not only captures predominant opinions from an aspect perspective with supporting evidence, but also adapts to varying domains without relying on a pre-defined set of aspects. Our proposed framework, ASESUM, summarizes viewpoints relevant to the critical aspects of a product by extracting aspect-centric arguments and measuring their salience and validity. We conduct experiments on a real-world dataset to demonstrate the superiority of our approach in capturing diverse perspectives of the original reviews compared to new and existing methods.",
        "translated": "在线购物中，评论是顾客做出购买决策的宝贵资源。然而，顾客逐一查阅海量评论并从中人工归纳出主要观点是不切实际的，这促使了对自动化观点摘要系统的需求。现有的抽取式或生成式方法在自动生成有事实依据的方面中心摘要方面面临挑战。在本文中，我们提出了一种新颖的摘要系统，它不仅能够从方面角度捕获主要观点并附带支持证据，而且无需依赖预定义的方面集合即可适应不同领域。我们提出的框架ASESUM通过提取方面中心论据并衡量其显著性和有效性，从而总结出与产品关键方面相关的观点。我们在真实世界数据集上进行了实验，结果表明，与新方法和现有方法相比，我们的方法在捕获原始评论多样化视角方面表现出优越性。"
    },
    {
        "title": "Precise Zero-Shot Pointwise Ranking with LLMs through Post-Aggregated\n  Global Context Information",
        "url": "http://arxiv.org/abs/2506.10859v1",
        "pub_date": "2025-06-12",
        "summary": "Recent advancements have successfully harnessed the power of Large Language Models (LLMs) for zero-shot document ranking, exploring a variety of prompting strategies. Comparative approaches like pairwise and listwise achieve high effectiveness but are computationally intensive and thus less practical for larger-scale applications. Scoring-based pointwise approaches exhibit superior efficiency by independently and simultaneously generating the relevance scores for each candidate document. However, this independence ignores critical comparative insights between documents, resulting in inconsistent scoring and suboptimal performance. In this paper, we aim to improve the effectiveness of pointwise methods while preserving their efficiency through two key innovations: (1) We propose a novel Global-Consistent Comparative Pointwise Ranking (GCCP) strategy that incorporates global reference comparisons between each candidate and an anchor document to generate contrastive relevance scores. We strategically design the anchor document as a query-focused summary of pseudo-relevant candidates, which serves as an effective reference point by capturing the global context for document comparison. (2) These contrastive relevance scores can be efficiently Post-Aggregated with existing pointwise methods, seamlessly integrating essential Global Context information in a training-free manner (PAGC). Extensive experiments on the TREC DL and BEIR benchmark demonstrate that our approach significantly outperforms previous pointwise methods while maintaining comparable efficiency. Our method also achieves competitive performance against comparative methods that require substantially more computational resources. More analyses further validate the efficacy of our anchor construction strategy.",
        "translated": "近期进展已成功驾驭大语言模型（LLM）进行零样本文档排序，并探索了多种提示策略。成对比较和列表比较等对比方法虽然能取得高有效性，但其计算开销大，因此不适用于大规模应用。基于评分的逐点方法通过独立且并行地生成每个候选文档的相关性分数，展现出卓越的效率。然而，这种独立性忽略了文档间关键的比较信息，从而导致评分不一致和次优性能。\n\n在本文中，我们旨在通过两项关键创新，在保持效率的同时提升逐点方法的有效性：(1) 我们提出了一种新颖的全局一致比较逐点排序（Global-Consistent Comparative Pointwise Ranking, GCCP）策略，该策略引入了每个候选文档与一个锚点文档之间的全局参考比较，以生成对比相关性分数。我们策略性地将锚点文档设计为伪相关候选文档的查询中心摘要，通过捕获文档比较的全局上下文，它可作为一个有效的参考点。(2) 这些对比相关性分数可以与现有逐点方法进行高效的后聚合（Post-Aggregated with Global Context, PAGC），以免训练的方式无缝集成关键的全局上下文信息。\n\n在TREC DL和BEIR基准数据集上的大量实验表明，我们的方法显著超越了以往的逐点方法，同时保持了可比的效率。此外，我们的方法与需要大量计算资源的对比方法相比也达到了具有竞争力的性能。进一步的分析也验证了我们锚点构建策略的有效性。"
    },
    {
        "title": "CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation\n  through Self-Training",
        "url": "http://arxiv.org/abs/2506.10844v1",
        "pub_date": "2025-06-12",
        "summary": "This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG) framework composed of specialized agents for subtasks such as planning, searching, reasoning, and coordination. Our system uses a self-training paradigm with reward-guided trajectory sampling to optimize inter-agent collaboration and enhance response generation. Evaluated on DataMorgana-derived datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms conventional RAG baselines. We further analyze competition outcomes and showcase the framework's strengths with case studies, demonstrating its efficacy for complex, real-world RAG tasks.",
        "translated": "本文提出了mRAG，这是一个多智能体检索增强生成（RAG）框架，它由专门用于规划、搜索、推理和协调等子任务的智能体组成。我们的系统采用自训练范式，通过奖励引导的轨迹采样来优化智能体间的协作并提升响应生成能力。在SIGIR 2025 LiveRAG 竞赛期间，mRAG 在源自DataMorgana的数据集上进行评估，其性能超越了传统的RAG基线。我们进一步分析了竞赛结果，并通过案例研究展示了该框架的优势，证明了其在复杂、真实世界的RAG任务中的有效性。"
    },
    {
        "title": "Constructing and Evaluating Declarative RAG Pipelines in PyTerrier",
        "url": "http://arxiv.org/abs/2506.10802v1",
        "pub_date": "2025-06-12",
        "summary": "Search engines often follow a pipeline architecture, where complex but effective reranking components are used to refine the results of an initial retrieval. Retrieval augmented generation (RAG) is an exciting application of the pipeline architecture, where the final component generates a coherent answer for the users from the retrieved documents. In this demo paper, we describe how such RAG pipelines can be formulated in the declarative PyTerrier architecture, and the advantages of doing so. Our PyTerrier-RAG extension for PyTerrier provides easy access to standard RAG datasets and evaluation measures, state-of-the-art LLM readers, and using PyTerrier's unique operator notation, easy-to-build pipelines. We demonstrate the succinctness of indexing and RAG pipelines on standard datasets (including Natural Questions) and how to build on the larger PyTerrier ecosystem with state-of-the-art sparse, learned-sparse, and dense retrievers, and other neural rankers.",
        "translated": "搜索引擎通常遵循流水线架构，其中使用复杂但有效的重排序组件来优化初始检索的结果。检索增强生成（RAG）是流水线架构的一种令人兴奋的应用，其最终组件从检索到的文档中为用户生成连贯的答案。在这篇演示论文中，我们描述了如何在声明式 PyTerrier 架构中构建此类 RAG 流水线，并阐述了这样做的好处。\n\n我们的 PyTerrier-RAG 扩展为 PyTerrier 提供了便捷的访问方式，可用于标准 RAG 数据集和评估指标、最先进的 LLM 阅读器，并且利用 PyTerrier 独特的运算符表示法，可以轻松构建流水线。我们展示了在标准数据集（包括 Natural Questions）上构建索引和 RAG 流水线的简洁性，以及如何利用更大的 PyTerrier 生态系统，结合最先进的稀疏、学习型稀疏和稠密检索器以及其他神经网络排序器。"
    },
    {
        "title": "TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to\n  Evolving Research Corpora",
        "url": "http://arxiv.org/abs/2506.10737v1",
        "pub_date": "2025-06-12",
        "summary": "The rapid evolution of scientific fields introduces challenges in organizing and retrieving scientific literature. While expert-curated taxonomies have traditionally addressed this need, the process is time-consuming and expensive. Furthermore, recent automatic taxonomy construction methods either (1) over-rely on a specific corpus, sacrificing generalizability, or (2) depend heavily on the general knowledge of large language models (LLMs) contained within their pre-training datasets, often overlooking the dynamic nature of evolving scientific domains. Additionally, these approaches fail to account for the multi-faceted nature of scientific literature, where a single research paper may contribute to multiple dimensions (e.g., methodology, new tasks, evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a framework that dynamically adapts an LLM-generated taxonomy to a given corpus across multiple dimensions. TaxoAdapt performs iterative hierarchical classification, expanding both the taxonomy width and depth based on corpus' topical distribution. We demonstrate its state-of-the-art performance across a diverse set of computer science conferences over the years to showcase its ability to structure and capture the evolution of scientific fields. As a multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more granularity-preserving and 50.41% more coherent than the most competitive baselines judged by LLMs.",
        "translated": "科学领域的快速演进给科学文献的组织和检索带来了挑战。尽管专家人工构建的分类体系（taxonomies）传统上能满足这一需求，但其过程耗时且昂贵。此外，近期自动分类体系构建方法存在两类问题：(1) 过度依赖特定语料库，牺牲了泛化能力；(2) 严重依赖大型语言模型（LLM）预训练数据中包含的通用知识，常常忽视不断演进的科学领域的动态特性。此外，这些方法未能考虑到科学文献的多维度特性，即一篇研究论文可能在多个维度上有所贡献（例如，方法论、新任务、评估指标、基准）。\n\n为了弥补这些不足，我们提出了TaxoAdapt，一个能将LLM生成的分类体系动态适应到给定语料库的跨维度框架。TaxoAdapt执行迭代分层分类，基于语料库的主题分布扩展分类体系的广度和深度。我们通过在多年间各种计算机科学会议上的实验，展示了其最先进的性能，以彰显其构建结构和捕捉科学领域演进的能力。作为一种多维度方法，TaxoAdapt生成的分类体系在LLM评判下，比最具竞争力的基线方法在粒度保持性上高出26.51%，在连贯性上高出50.41%。"
    },
    {
        "title": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of\n  Nuanced Claims",
        "url": "http://arxiv.org/abs/2506.10728v1",
        "pub_date": "2025-06-12",
        "summary": "Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely \"true\" or \"false\" -- as is frequently the case with scientific and political claims. However, a claim (e.g., \"vaccine A is better than vaccine B\") can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., \"how many biomedical papers believe vaccine A is more transportable than B?\"). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines.",
        "translated": "个人或实体提出的主张往往细致入微，无法被明确地标记为完全“真”或“假”——科学和政治领域的主张尤其如此。然而，一项主张（例如，“疫苗A优于疫苗B”）可以被剖析为其组成方面和子方面（例如，功效、安全性、分发），这些细分方面更容易单独验证。这种方法能够促成更全面、结构化的回应，为特定问题提供多维度的视角，同时允许读者优先关注主张中感兴趣的特定角度（例如，对儿童的安全性）。\n\n为此，我们提出ClaimSpect，一个基于检索增强生成（RAG）的框架，旨在自动构建处理主张时通常会考虑的方面层级结构，并利用特定语料库的视角来丰富这些方面。该结构能够对输入语料库进行分层分区，以检索相关片段，从而有助于发现新的子方面。此外，这些片段还能揭示对主张某个方面的不同观点（例如，支持、中立或反对），以及这些观点的普遍程度（例如，“有多少生物医学论文认为疫苗A比疫苗B更易于运输？”）。我们将ClaimSpect应用于我们所构建数据集中涵盖的各类真实世界科学和政治主张，展示了其在解构细致入微的主张和表示语料库内观点方面的鲁棒性和准确性。通过真实世界案例研究和人工评估，我们验证了其相对于多个基线的有效性。"
    },
    {
        "title": "Contrastive Matrix Completion with Denoising and Augmented Graph Views\n  for Robust Recommendation",
        "url": "http://arxiv.org/abs/2506.10658v1",
        "pub_date": "2025-06-12",
        "summary": "Matrix completion is a widely adopted framework in recommender systems, as predicting the missing entries in the user-item rating matrix enables a comprehensive understanding of user preferences. However, current graph neural network (GNN)-based approaches are highly sensitive to noisy or irrelevant edges--due to their inherent message-passing mechanisms--and are prone to overfitting, which limits their generalizability. To overcome these challenges, we propose a novel method called Matrix Completion using Contrastive Learning (MCCL). Our approach begins by extracting local neighborhood subgraphs for each interaction and subsequently generates two distinct graph representations. The first representation emphasizes denoising by integrating GNN layers with an attention mechanism, while the second is obtained via a graph variational autoencoder that aligns the feature distribution with a standard prior. A mutual learning loss function is employed during training to gradually harmonize these representations, enabling the model to capture common patterns and significantly enhance its generalizability. Extensive experiments on several real-world datasets demonstrate that our approach not only improves the numerical accuracy of the predicted scores--achieving up to a 0.8% improvement in RMSE--but also produces superior rankings with improvements of up to 36% in ranking metrics.",
        "translated": "矩阵补全作为推荐系统中广泛采用的框架，通过预测用户-物品评分矩阵中的缺失项，能够全面理解用户偏好。然而，当前基于图神经网络（GNN）的方法由于其固有的消息传递机制，对噪声或不相关边高度敏感，且容易过拟合，这限制了它们的泛化能力。为了克服这些挑战，我们提出了一种名为“使用对比学习的矩阵补全”（MCCL）的新颖方法。我们的方法首先为每个交互提取局部邻域子图，随后生成两种不同的图表示。第一个表示通过集成GNN层和注意力机制来侧重于去噪，而第二个则通过图变分自编码器获得，该编码器将特征分布与标准先验对齐。在训练过程中，我们采用互学习损失函数来逐步协调这些表示，使模型能够捕获共同模式并显著增强其泛化能力。在多个真实世界数据集上进行的广泛实验表明，我们的方法不仅提高了预测分数的数值精度——在RMSE上实现了高达0.8%的改进——而且在排名指标上提升高达36%，产生了更优越的排名。"
    },
    {
        "title": "Conversational Search: From Fundamentals to Frontiers in the LLM Era",
        "url": "http://arxiv.org/abs/2506.10635v1",
        "pub_date": "2025-06-12",
        "summary": "Conversational search enables multi-turn interactions between users and systems to fulfill users' complex information needs. During this interaction, the system should understand the users' search intent within the conversational context and then return the relevant information through a flexible, dialogue-based interface. The recent powerful large language models (LLMs) with capacities of instruction following, content generation, and reasoning, attract significant attention and advancements, providing new opportunities and challenges for building up intelligent conversational search systems. This tutorial aims to introduce the connection between fundamentals and the emerging topics revolutionized by LLMs in the context of conversational search. It is designed for students, researchers, and practitioners from both academia and industry. Participants will gain a comprehensive understanding of both the core principles and cutting-edge developments driven by LLMs in conversational search, equipping them with the knowledge needed to contribute to the development of next-generation conversational search systems.",
        "translated": "对话式搜索使得用户与系统之间能够进行多轮交互，以满足用户复杂的信息需求。在此交互过程中，系统应理解用户在对话上下文中的搜索意图，然后通过灵活的、基于对话的界面返回相关信息。\n\n近期强大的大型语言模型（LLMs），具备指令遵循、内容生成和推理的能力，吸引了广泛关注并取得了显著进展，为构建智能对话式搜索系统提供了新的机遇和挑战。本教程旨在介绍在对话式搜索领域中，基础知识与由LLMs带来变革的新兴主题之间的联系。\n\n本教程面向来自学术界和工业界的学生、研究人员和实践者。参与者将获得对对话式搜索中由LLMs驱动的核心原则和前沿发展的全面理解，从而掌握所需的知识，以助力下一代对话式搜索系统的发展。"
    },
    {
        "title": "Macro Graph of Experts for Billion-Scale Multi-Task Recommendation",
        "url": "http://arxiv.org/abs/2506.10520v1",
        "pub_date": "2025-06-12",
        "summary": "Graph-based multi-task learning at billion-scale presents a significant challenge, as different tasks correspond to distinct billion-scale graphs. Traditional multi-task learning methods often neglect these graph structures, relying solely on individual user and item embeddings. However, disregarding graph structures overlooks substantial potential for improving performance. In this paper, we introduce the Macro Graph of Expert (MGOE) framework, the first approach capable of leveraging macro graph embeddings to capture task-specific macro features while modeling the correlations between task-specific experts. Specifically, we propose the concept of a Macro Graph Bottom, which, for the first time, enables multi-task learning models to incorporate graph information effectively. We design the Macro Prediction Tower to dynamically integrate macro knowledge across tasks. MGOE has been deployed at scale, powering multi-task learning for the homepage of a leading billion-scale recommender system. Extensive offline experiments conducted on three public benchmark datasets demonstrate its superiority over state-of-the-art multi-task learning methods, establishing MGOE as a breakthrough in multi-task graph-based recommendation. Furthermore, online A/B tests confirm the superiority of MGOE in billion-scale recommender systems.",
        "translated": "在十亿级规模下进行基于图的多任务学习面临巨大挑战，因为不同的任务对应着各自独立的十亿级图。传统的的多任务学习方法常忽略这些图结构，仅依赖于独立的用户和物品嵌入。然而，忽视图结构会错失巨大的性能提升潜力。本文提出宏观专家图（Macro Graph of Expert, MGOE）框架，这是首个能够利用宏观图嵌入捕获任务特定宏观特征，并同时建模任务特定专家之间关联性的方法。具体而言，我们提出了宏观图底部（Macro Graph Bottom）的概念，这首次使多任务学习模型能够有效地融入图信息。我们设计了宏观预测塔（Macro Prediction Tower）来动态整合跨任务的宏观知识。MGOE已实现大规模部署，驱动着某领先十亿级推荐系统首页的多任务学习。在三个公开基准数据集上进行的广泛离线实验证明了MGOE优于最先进的多任务学习方法，使其成为基于图的多任务推荐领域的突破性进展。此外，在线A/B测试进一步证实了MGOE在十亿级推荐系统中的优越性。"
    },
    {
        "title": "Generative Representational Learning of Foundation Models for\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.11999v1",
        "pub_date": "2025-06-13",
        "summary": "Developing a single foundation model with the capability to excel across diverse tasks has been a long-standing objective in the field of artificial intelligence. As the wave of general-purpose foundation models sweeps across various domains, their influence has significantly extended to the field of recommendation systems. While recent efforts have explored recommendation foundation models for various generative tasks, they often overlook crucial embedding tasks and struggle with the complexities of multi-task learning, including knowledge sharing &amp; conflict resolution, and convergence speed inconsistencies. To address these limitations, we introduce RecFound, a generative representational learning framework for recommendation foundation models. We construct the first comprehensive dataset for recommendation foundation models covering both generative and embedding tasks across diverse scenarios. Based on this dataset, we propose a novel multi-task training scheme featuring a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge sharing &amp; conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched) to address inconsistent convergence, and a Model Merge module to balance the performance across tasks. Experiments demonstrate that RecFound achieves state-of-the-art performance across various recommendation tasks, outperforming existing baselines.",
        "translated": "在人工智能领域，开发一个能够胜任多样化任务的统一基础模型，一直是一个长期以来的目标。随着通用基础模型浪潮席卷各个领域，其影响力已深刻延伸至推荐系统领域。尽管近期研究已探索推荐基础模型在多种生成任务上的应用，但它们却往往忽视了关键的嵌入任务，并且在多任务学习的复杂性方面表现不足，例如知识共享与冲突消解、以及收敛速度不一致性等问题。\n\n为解决这些局限，我们提出了RecFound，一种用于推荐基础模型的生成式表征学习框架。我们构建了首个面向推荐基础模型的综合性数据集，该数据集同时涵盖了多种场景下的生成任务和嵌入任务。基于此数据集，我们提出了一种新颖的多任务训练方案，该方案包含一个任务级低秩专家混合（Task-wise Mixture of Low-rank Experts, TMoLE）模块以处理知识共享与冲突，一个分步式收敛导向采样调度器（Step-wise Convergence-oriented Sample Scheduler, S2Sched）以解决收敛不一致的问题，以及一个模型合并模块（Model Merge）以平衡跨任务的性能。实验结果表明，RecFound在各种推荐任务中均达到了最先进的水平，并优于现有基线。"
    },
    {
        "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
        "url": "http://arxiv.org/abs/2506.11763v1",
        "pub_date": "2025-06-13",
        "summary": "Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.",
        "translated": "深度研究智能体是基于大型语言模型（LLM）的智能体中的一个突出类别。通过自主编排多步骤网络探索、定向检索和高阶综合，它们能够将海量在线信息转化为分析师级别的、富含引用的报告——将数小时的人工桌面研究压缩至数分钟。然而，目前仍然缺乏一个能够系统性评估这些智能体能力的综合基准。\n\n为了弥补这一空白，我们提出了DeepResearch Bench，这是一个包含100个博士级研究任务的基准，每个任务均由22个不同领域的领域专家精心设计。评估深度研究智能体（DRA）本质上是复杂且劳动密集型的。因此，我们提出了两种新颖的方法，它们与人类判断具有高度一致性。第一种是基于参考的评估方法，采用自适应标准来评估生成研究报告的质量。另一个框架旨在通过评估其有效引用数量和整体引用准确性来评估DRA的信息检索和收集能力。我们已将DeepResearch Bench和这些框架的关键组件在https://github.com/Ayanami0730/deep_research_bench上开源，以加速实用的大型语言模型智能体的开发。"
    },
    {
        "title": "Forgetful by Design? A Critical Audit of YouTube's Search API for\n  Academic Research",
        "url": "http://arxiv.org/abs/2506.11727v1",
        "pub_date": "2025-06-13",
        "summary": "This paper critically audits the search endpoint of YouTube's Data API (v3), a common tool for academic research. Through systematic weekly searches over six months using eleven queries, we identify major limitations regarding completeness, representativeness, consistency, and bias. Our findings reveal substantial differences between ranking parameters like relevance and date in terms of video recall and precision, with relevance often retrieving numerous off-topic videos. We also find severe temporal decay, as the number of findable videos for a specific period dramatically decreases after just 20-60 days from the publication date, potentially hampering many different research designs. Furthermore, search results lack consistency, with identical queries yielding different video sets over time, compromising replicability. A case study on the European Parliament elections highlights how these issues impact research outcomes. While the paper offers several mitigation strategies, it concludes that the API's search function, potentially prioritizing \"freshness\" over comprehensive retrieval, is not adequate for robust academic research, especially concerning Digital Services Act requirements.",
        "translated": "本文批判性地审查了YouTube Data API (v3) 的搜索端点，该API是学术研究中常用的工具。通过系统地进行为期六个月的每周搜索，并使用十一个查询，我们发现了其在完整性、代表性、一致性和偏差方面的主要局限性。我们的研究结果揭示了相关性（relevance）和日期（date）等排序参数在视频召回率和精确率方面存在显著差异，其中相关性排序经常检索到大量偏离主题的视频。我们还发现严重的时效性衰减，即特定时期内可被找到的视频数量在发布日期后仅仅20-60天就急剧减少，这可能会阻碍许多不同的研究设计。此外，搜索结果缺乏一致性，相同的查询在不同时间会产生不同的视频集合，从而损害了研究的可复现性。一项关于欧洲议会选举的案例研究强调了这些问题如何影响研究成果。尽管本文提出了一些缓解策略，但它总结认为，该API的搜索功能（可能优先考虑“新鲜度”而非全面的检索）不足以支持严谨的学术研究，尤其是在满足《数字服务法案》要求方面。"
    },
    {
        "title": "TongSearch-QR: Reinforced Query Reasoning for Retrieval",
        "url": "http://arxiv.org/abs/2506.11603v1",
        "pub_date": "2025-06-13",
        "summary": "Traditional information retrieval (IR) methods excel at textual and semantic matching but struggle in reasoning-intensive retrieval tasks that require multi-hop inference or complex semantic understanding between queries and documents. One promising solution is to explicitly rewrite or augment queries using large language models (LLMs) to elicit reasoning-relevant content prior to retrieval. However, the widespread use of large-scale language models like GPT-4 or LLaMA3-70B remains impractical due to their high inference cost and limited deployability in real-world systems. In this work, we introduce TongSearch QR (Previously Known as \"TongSearch Reasoner\"), a family of small-scale language models for query reasoning and rewriting in reasoning-intensive retrieval. With a novel semi-rule-based reward function, we employ reinforcement learning approaches enabling smaller language models, e,g, Qwen2.5-7B-Instruct and Qwen2.5-1.5B-Instruct, to achieve query reasoning performance rivaling large-scale language models without their prohibitive inference costs. Experiment results on BRIGHT benchmark show that with BM25 as retrievers, both TongSearch QR-7B and TongSearch QR-1.5B models significantly outperform existing baselines, including prompt-based query reasoners and some latest dense retrievers trained for reasoning-intensive retrieval tasks, offering superior adaptability for real-world deployment.",
        "translated": "传统信息检索 (IR) 方法擅长文本和语义匹配，但在需要多跳推理或查询与文档之间复杂语义理解的推理密集型检索任务中表现不佳。一个有前景的解决方案是，在检索之前使用大型语言模型 (LLM) 显式地重写或增强查询，以引出与推理相关的内容。然而，由于像 GPT-4 或 LLaMA3-70B 这样的大型语言模型具有高昂的推理成本和在实际系统中有限的部署能力，它们的广泛应用仍然不切实际。\n\n在这项工作中，我们引入了 TongSearch QR（原名“TongSearch Reasoner”），这是一系列用于推理密集型检索中查询推理和重写的小型语言模型。借助一种新颖的半基于规则的奖励函数，我们采用了强化学习方法，使得像 Qwen2.5-7B-Instruct 和 Qwen2.5-1.5B-Instruct 这样的小型语言模型能够实现与大型语言模型相媲美的查询推理性能，而无需承担其高昂的推理成本。BRIGHT 基准测试上的实验结果表明，以 BM25 作为检索器时，TongSearch QR-7B 和 TongSearch QR-1.5B 模型均显著优于现有基线，包括基于提示的查询推理器以及一些为推理密集型检索任务训练的最新密集检索器，从而为实际部署提供了卓越的适应性。"
    },
    {
        "title": "GraphRAG-Causal: A novel graph-augmented framework for causal reasoning\n  and annotation in news",
        "url": "http://arxiv.org/abs/2506.11600v1",
        "pub_date": "2025-06-13",
        "summary": "GraphRAG-Causal introduces an innovative framework that combines graph-based retrieval with large language models to enhance causal reasoning in news analysis. Traditional NLP approaches often struggle with identifying complex, implicit causal links, especially in low-data scenarios. Our approach addresses these challenges by transforming annotated news headlines into structured causal knowledge graphs. It then employs a hybrid retrieval system that merges semantic embeddings with graph-based structural cues leveraging Neo4j to accurately match and retrieve relevant events. The framework is built on a three-stage pipeline: First, during Data Preparation, news sentences are meticulously annotated and converted into causal graphs capturing cause, effect, and trigger relationships. Next, the Graph Retrieval stage stores these graphs along with their embeddings in a Neo4j database and utilizes hybrid Cypher queries to efficiently identify events that share both semantic and structural similarities with a given query. Finally, the LLM Inference stage utilizes these retrieved causal graphs in a few-shot learning setup with XML-based prompting, enabling robust classification and tagging of causal relationships. Experimental evaluations demonstrate that GraphRAG-Causal achieves an impressive F1-score of 82.1% on causal classification using just 20 few-shot examples. This approach significantly boosts accuracy and consistency, making it highly suitable for real-time applications in news reliability assessment, misinformation detection, and policy analysis.",
        "translated": "GraphRAG-Causal 引入了一个创新的框架，该框架结合了图基检索与大型语言模型，旨在增强新闻分析中的因果推理能力。传统的自然语言处理（NLP）方法在识别复杂、隐式的因果关系方面常常面临挑战，特别是在数据稀缺的场景下。我们的方法通过将标注的新闻标题转化为结构化的因果知识图谱，解决了这些挑战。它随后采用了一种混合检索系统，该系统将语义嵌入与图基结构线索相结合，并利用 Neo4j 精确匹配和检索相关事件。\n\n该框架基于一个三阶段的流水线构建：\n\n1.  **数据准备**：新闻语句被精心标注并转换为因果图，捕获原因、结果和触发关系。\n2.  **图检索**：将这些图及其嵌入存储在 Neo4j 数据库中，并利用混合 Cypher 查询高效地识别与给定查询同时具有语义和结构相似性的事件。\n3.  **LLM 推理**：在基于 XML 提示的少样本学习设置中利用这些检索到的因果图，从而实现因果关系的鲁棒分类和标记。\n\n实验评估表明，GraphRAG-Causal 在仅使用 20 个少样本示例的情况下，在因果分类上取得了高达 82.1% 的 F1 分数。这种方法显著提升了准确性和一致性，使其非常适用于新闻可靠性评估、虚假信息检测和政策分析等实时应用。"
    },
    {
        "title": "Dual-View Disentangled Multi-Intent Learning for Enhanced Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2506.11538v1",
        "pub_date": "2025-06-13",
        "summary": "Disentangling user intentions from implicit feedback has become a promising strategy to enhance recommendation accuracy and interpretability. Prior methods often model intentions independently and lack explicit supervision, thus failing to capture the joint semantics that drive user-item interactions. To address these limitations, we propose DMICF, a unified framework that explicitly models interaction-level intent alignment while leveraging structural signals from both user and item perspectives. DMICF adopts a dual-view architecture that jointly encodes user-item interaction graphs from both sides, enabling bidirectional information fusion. This design enhances robustness under data sparsity by allowing the structural redundancy of one view to compensate for the limitations of the other. To model fine-grained user-item compatibility, DMICF introduces an intent interaction encoder that performs sub-intent alignment within each view, uncovering shared semantic structures that underlie user decisions. This localized alignment enables adaptive refinement of intent embeddings based on interaction context, thus improving the model's generalization and expressiveness, particularly in long-tail scenarios. Furthermore, DMICF integrates an intent-aware scoring mechanism that aggregates compatibility signals from matched intent pairs across user and item subspaces, enabling personalized prediction grounded in semantic congruence rather than entangled representations. To facilitate semantic disentanglement, we design a discriminative training signal via multi-negative sampling and softmax normalization, which pulls together semantically aligned intent pairs while pushing apart irrelevant or noisy ones. Extensive experiments demonstrate that DMICF consistently delivers robust performance across datasets with diverse interaction distributions.",
        "translated": "将用户意图从隐式反馈中解耦，已成为提升推荐准确性和可解释性的一种有前景的策略。以往的方法通常独立建模意图，且缺乏显式监督，因此未能捕捉驱动用户-物品交互的联合语义。为解决这些局限性，我们提出了DMICF，一个统一框架，它显式建模交互层面的意图对齐，同时利用用户和物品两个视角的结构信号。\n\nDMICF采用双视图架构，从用户和物品两侧共同编码用户-物品交互图，实现双向信息融合。这种设计通过允许一个视图的结构冗余来弥补另一个视图的局限性，从而增强了数据稀疏性下的鲁棒性。为了建模细粒度的用户-物品兼容性，DMICF引入了一个意图交互编码器，它在每个视图内部执行子意图对齐，从而揭示用户决策背后的共享语义结构。这种局部对齐能够基于交互上下文对意图嵌入进行自适应细化，进而提升模型的泛化能力和表达性，尤其在长尾场景中表现突出。\n\n此外，DMICF集成了意图感知评分机制，它聚合了用户和物品子空间中匹配意图对的兼容性信号，从而实现了基于语义一致性而非纠缠表示的个性化预测。为了促进语义解耦，我们通过多负采样和softmax归一化设计了一种判别性训练信号，该信号能拉近语义对齐的意图对，同时推开不相关或带有噪声的意图对。大量实验表明，DMICF在具有多样交互分布的数据集上始终展现出强大的鲁棒性能。"
    },
    {
        "title": "Leveraging Reference Documents for Zero-Shot Ranking via Large Language\n  Models",
        "url": "http://arxiv.org/abs/2506.11452v1",
        "pub_date": "2025-06-13",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in the task of text ranking for information retrieval. While Pointwise ranking approaches offer computational efficiency by scoring documents independently, they often yield biased relevance estimates due to the lack of inter-document comparisons. In contrast, Pairwise methods improve ranking accuracy by explicitly comparing document pairs, but suffer from substantial computational overhead with quadratic complexity ($O(n^2)$). To address this tradeoff, we propose \\textbf{RefRank}, a simple and effective comparative ranking method based on a fixed reference document. Instead of comparing all document pairs, RefRank prompts the LLM to evaluate each candidate relative to a shared reference anchor. By selecting the reference anchor that encapsulates the core query intent, RefRank implicitly captures relevance cues, enabling indirect comparison between documents via this common anchor. This reduces computational cost to linear time ($O(n)$) while importantly, preserving the advantages of comparative evaluation. To further enhance robustness, we aggregate multiple RefRank outputs using a weighted averaging scheme across different reference choices. Experiments on several benchmark datasets and with various LLMs show that RefRank significantly outperforms Pointwise baselines and could achieve performance at least on par with Pairwise approaches with a significantly lower computational cost.",
        "translated": "大型语言模型（LLM）在信息检索的文本排序任务中展现出卓越的性能。尽管逐点排序（Pointwise）方法通过独立地为文档评分来提供计算效率，但由于缺乏文档间比较，它们通常会产生有偏的相关性估计。相比之下，成对（Pairwise）方法通过显式比较文档对来提高排序准确性，但会带来巨大的计算开销，具有二次复杂度（$O(n^2)$）。为了解决这种权衡，我们提出了 \\textbf{RefRank}，一种基于固定参考文档的简单有效比较排序方法。RefRank 不再比较所有文档对，而是提示LLM评估每个候选文档相对于一个共享的参考锚点。通过选择能封装核心查询意图的参考锚点，RefRank 隐式地捕获相关性线索，从而通过这个共同锚点实现文档间的间接比较。这使得计算成本降低到线性时间（$O(n)$），同时重要的是，保留了比较评估的优势。为了进一步增强鲁棒性，我们采用加权平均方案，聚合了在不同参考选择下的多个 RefRank 输出。在多个基准数据集和各种LLM上的实验表明，RefRank 显著优于逐点基线方法，并且在计算成本显著降低的情况下，其性能至少与成对方法持平。"
    },
    {
        "title": "Deep Learning Model Acceleration and Optimization Strategies for\n  Real-Time Recommendation Systems",
        "url": "http://arxiv.org/abs/2506.11421v1",
        "pub_date": "2025-06-13",
        "summary": "With the rapid growth of Internet services, recommendation systems play a central role in delivering personalized content. Faced with massive user requests and complex model architectures, the key challenge for real-time recommendation systems is how to reduce inference latency and increase system throughput without sacrificing recommendation quality. This paper addresses the high computational cost and resource bottlenecks of deep learning models in real-time settings by proposing a combined set of modeling- and system-level acceleration and optimization strategies. At the model level, we dramatically reduce parameter counts and compute requirements through lightweight network design, structured pruning, and weight quantization. At the system level, we integrate multiple heterogeneous compute platforms and high-performance inference libraries, and we design elastic inference scheduling and load-balancing mechanisms based on real-time load characteristics. Experiments show that, while maintaining the original recommendation accuracy, our methods cut latency to less than 30% of the baseline and more than double system throughput, offering a practical solution for deploying large-scale online recommendation services.",
        "translated": "随着互联网服务的快速发展，推荐系统在提供个性化内容方面扮演着核心角色。面对海量用户请求和复杂的模型架构，实时推荐系统的关键挑战在于如何在不牺牲推荐质量的前提下，降低推理延迟并提高系统吞吐量。本文针对深度学习模型在实时场景中的高计算开销和资源瓶颈问题，提出了一套结合模型级和系统级的加速与优化策略。在模型层面，我们通过轻量级网络设计、结构化剪枝和权重量化等技术，显著减少了模型参数量和计算需求。在系统层面，我们整合了多个异构计算平台和高性能推理库，并基于实时负载特性设计了弹性推理调度和负载均衡机制。实验结果表明，在保持原有推荐精度的同时，我们的方法将延迟降低到基线的30%以下，并将系统吞吐量提升了一倍以上，为部署大规模在线推荐服务提供了一种实用的解决方案。"
    },
    {
        "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
        "url": "http://arxiv.org/abs/2506.12015v1",
        "pub_date": "2025-06-13",
        "summary": "Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.",
        "translated": "开源基础模型已得到快速采用和发展，在多样化领域实现了强大的通用能力。然而，由于微调大型基础模型所需的内存开销远超推理，对于大多数用户而言，将其微调用于领域特定或个性化任务仍然成本过高，令人望而却步。我们引入EMLoC（带有LoRA校正的基于模拟器的内存高效微调框架），它使得模型微调能够在与推理所需的相同内存预算内进行。\n\nEMLoC通过在小型下游校准集上应用激活感知奇异值分解（SVD），构建了一个任务特定的轻量级模拟器。随后，通过LoRA在该轻量级模拟器上执行微调。为解决原始模型与压缩模拟器之间的失配问题，我们提出了一种新颖的补偿算法来校正微调后的LoRA模块，从而可以将该模块合并到原始模型中进行推理。EMLoC支持灵活的压缩比和标准训练流程，使其能够适应广泛的应用。\n\n大量实验表明，EMLoC在多个数据集和模态上均优于其他基线方法。此外，在无需量化的前提下，EMLoC实现了在单一24GB消费级GPU上微调380亿参数模型，为个人用户带来了高效实用的模型适应。"
    },
    {
        "title": "Generative Representational Learning of Foundation Models for\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.11999v1",
        "pub_date": "2025-06-13",
        "summary": "Developing a single foundation model with the capability to excel across diverse tasks has been a long-standing objective in the field of artificial intelligence. As the wave of general-purpose foundation models sweeps across various domains, their influence has significantly extended to the field of recommendation systems. While recent efforts have explored recommendation foundation models for various generative tasks, they often overlook crucial embedding tasks and struggle with the complexities of multi-task learning, including knowledge sharing &amp; conflict resolution, and convergence speed inconsistencies. To address these limitations, we introduce RecFound, a generative representational learning framework for recommendation foundation models. We construct the first comprehensive dataset for recommendation foundation models covering both generative and embedding tasks across diverse scenarios. Based on this dataset, we propose a novel multi-task training scheme featuring a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge sharing &amp; conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched) to address inconsistent convergence, and a Model Merge module to balance the performance across tasks. Experiments demonstrate that RecFound achieves state-of-the-art performance across various recommendation tasks, outperforming existing baselines.",
        "translated": "人工智能领域的一个长期目标是开发能够胜任各种任务的单一基础模型。随着通用型基础模型浪潮席卷各个领域，它们的影响力已显著扩展到推荐系统领域。尽管最近的研究探索了用于各种生成任务的推荐基础模型，但它们常常忽视关键的嵌入任务，并且难以应对多任务学习的复杂性，包括知识共享与冲突解决，以及收敛速度不一致性。\n\n为了解决这些局限性，我们引入了RecFound，一个专为推荐基础模型设计的生成式表示学习框架。我们构建了首个综合性数据集，专用于推荐基础模型，涵盖了跨越不同场景的生成任务和嵌入任务。基于该数据集，我们提出了一种新颖的多任务训练方案，其特点包括：任务级低秩专家混合（TMoLE）以处理知识共享与冲突、逐步收敛导向型样本调度器（S2Sched）以解决不一致的收敛问题，以及一个模型合并模块以平衡各项任务的性能。实验表明，RecFound在各种推荐任务中均达到了最先进的性能，超越了现有基线。"
    },
    {
        "title": "LTRR: Learning To Rank Retrievers for LLMs",
        "url": "http://arxiv.org/abs/2506.13743v1",
        "pub_date": "2025-06-16",
        "summary": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed retriever, despite growing evidence that no single retriever performs optimally across all query types. In this paper, we explore a query routing approach that dynamically selects from a pool of retrievers based on the query, using both train-free heuristics and learned routing models. We frame routing as a learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to rank retrievers by their expected utility gain to downstream LLM performance. Our experiments, conducted on synthetic QA data with controlled query type variations, show that routing-based RAG systems can outperform the best single-retriever-based systems. Performance gains are especially pronounced in models trained with the Answer Correctness (AC) metric and with pairwise learning approaches, especially with XGBoost. We also observe improvements in generalization to out-of-distribution queries. As part of the SIGIR 2025 LiveRAG challenge, our submitted system demonstrated the practical viability of our approach, achieving competitive performance in both answer correctness and faithfulness. These findings highlight the importance of both training methodology and metric selection in query routing for RAG systems.",
        "translated": "检索增强生成（RAG）系统通常依赖于单个固定的检索器，尽管越来越多的证据表明没有哪个单一检索器能在所有查询类型上都达到最佳性能。在本文中，我们探索了一种查询路由方法，该方法能够根据查询，结合使用免训练启发式方法和学习型路由模型，动态地从检索器池中选择检索器。我们将路由问题构建为一个学习排序（LTR）问题，并引入了LTRR，这是一个学习根据检索器对下游大型语言模型（LLM）性能的预期效用增益对其进行排序的框架。\n\n我们的实验在具有受控查询类型变化的合成问答数据上进行，结果表明基于路由的RAG系统能够超越性能最佳的单一检索器系统。性能提升在使用答案正确性（AC）指标训练的模型以及采用成对学习方法时尤其显著，特别是使用XGBoost时。我们还观察到在对分布外查询的泛化能力方面有所改进。作为SIGIR 2025 LiveRAG挑战赛的一部分，我们提交的系统展示了我们方法的实际可行性，在答案正确性和忠实性方面均取得了有竞争力的性能。这些发现强调了训练方法和度量指标选择在RAG系统查询路由中的重要性。"
    },
    {
        "title": "OneRec Technical Report",
        "url": "http://arxiv.org/abs/2506.13695v1",
        "pub_date": "2025-06-16",
        "summary": "Recommender systems have been widely used in various large-scale user-oriented platforms for many years. However, compared to the rapid developments in the AI community, recommendation systems have not achieved a breakthrough in recent years. For instance, they still rely on a multi-stage cascaded architecture rather than an end-to-end approach, leading to computational fragmentation and optimization inconsistencies, and hindering the effective application of key breakthrough technologies from the AI community in recommendation scenarios.   To address these issues, we propose OneRec, which reshapes the recommendation system through an end-to-end generative approach and achieves promising results. Firstly, we have enhanced the computational FLOPs of the current recommendation model by 10 $\\times$ and have identified the scaling laws for recommendations within certain boundaries. Secondly, reinforcement learning techniques, previously difficult to apply for optimizing recommendations, show significant potential in this framework. Lastly, through infrastructure optimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU) on flagship GPUs during training and inference, respectively, aligning closely with the LLM community. This architecture significantly reduces communication and storage overhead, resulting in operating expense that is only 10.6% of traditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP, it handles 25% of total queries per second, enhancing overall App Stay Time by 0.54% and 1.24%, respectively. Additionally, we have observed significant increases in metrics such as 7-day Lifetime, which is a crucial indicator of recommendation experience. We also provide practical lessons and insights derived from developing, optimizing, and maintaining a production-scale recommendation system with significant real-world impact.",
        "translated": "推荐系统多年来已广泛应用于各种大规模面向用户的平台。然而，与人工智能领域的快速发展相比，推荐系统近年来并未取得突破性进展。例如，它们仍依赖于多阶段级联架构而非端到端方法，这导致了计算碎片化和优化不一致性，阻碍了人工智能领域关键突破性技术在推荐场景中的有效应用。\n\n为解决这些问题，我们提出了OneRec，它通过端到端生成式方法重塑了推荐系统，并取得了喜人的成果。首先，我们将当前推荐模型的计算浮点运算次数（FLOPs）提升了10倍，并在特定范围内发现了推荐的缩放定律。其次，此前难以应用于推荐优化的强化学习技术，在此框架中展现出巨大潜力。最后，通过基础设施优化，我们在旗舰级GPU上实现了训练和推理过程中23.7%和28.8%的模型浮点运算利用率（MFU），这与大语言模型（LLM）领域高度一致。该架构显著降低了通信和存储开销，使得运营支出（OpEx）仅为传统推荐流水线的10.6%。它已部署于快手/快手极速版APP，处理了总查询量（QPS）的25%，分别将总App停留时长提升了0.54%和1.24%。此外，我们观察到7日留存等指标也显著增长，这是衡量推荐体验的关键指标。我们还分享了在开发、优化和维护具有重大实际影响的生产规模推荐系统过程中获得的实践经验和见解。"
    },
    {
        "title": "Tree-Based Text Retrieval via Hierarchical Clustering in RAGFrameworks:\n  Application on Taiwanese Regulations",
        "url": "http://arxiv.org/abs/2506.13607v1",
        "pub_date": "2025-06-16",
        "summary": "Traditional Retrieval-Augmented Generation (RAG) systems employ brute-force inner product search to retrieve the top-k most similar documents, then combined with the user query and passed to a language model. This allows the model to access external knowledge and reduce hallucinations. However, selecting an appropriate k value remains a significant challenge in practical applications: a small k may fail to retrieve sufficient information, while a large k can introduce excessive and irrelevant content. To address this, we propose a hierarchical clustering-based retrieval method that eliminates the need to predefine k. Our approach maintains the accuracy and relevance of system responses while adaptively selecting semantically relevant content. In the experiment stage, we applied our method to a Taiwanese legal dataset with expert-graded queries. The results show that our approach achieves superior performance in expert evaluations and maintains high precision while eliminating the need to predefine k, demonstrating improved accuracy and interpretability in legal text retrieval tasks. Our framework is simple to implement and easily integrates with existing RAG pipelines, making it a practical solution for real-world applications under limited resources.",
        "translated": "传统的检索增强生成（RAG）系统采用暴力内积搜索来检索最相似的top-k个文档，随后将这些文档与用户查询结合并输入给语言模型。这使得模型能够访问外部知识并减少幻觉（hallucinations）。然而，在实际应用中，选择一个合适的k值仍然是一个重大挑战：k值过小可能无法检索到足够的信息，而k值过大则可能引入过多无关内容。为解决此问题，我们提出了一种基于层次聚类的检索方法，该方法无需预定义k值。我们的方法在保持系统响应的准确性和相关性的同时，自适应地选择语义相关内容。\n\n在实验阶段，我们将该方法应用于一个包含专家评级查询的台湾法律数据集。结果表明，我们的方法在专家评估中表现出卓越的性能，并在无需预定义k值的情况下保持了高精度，从而证明了其在法律文本检索任务中提高了准确性和可解释性。我们的框架易于实现，并且可以轻松集成到现有的RAG管道中，使其成为在有限资源下实际应用的实用解决方案。"
    },
    {
        "title": "Hierarchical Multi-Positive Contrastive Learning for Patent Image\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.13496v1",
        "pub_date": "2025-06-16",
        "summary": "Patent images are technical drawings that convey information about a patent's innovation. Patent image retrieval systems aim to search in vast collections and retrieve the most relevant images. Despite recent advances in information retrieval, patent images still pose significant challenges due to their technical intricacies and complex semantic information, requiring efficient fine-tuning for domain adaptation. Current methods neglect patents' hierarchical relationships, such as those defined by the Locarno International Classification (LIC) system, which groups broad categories (e.g., \"furnishing\") into subclasses (e.g., \"seats\" and \"beds\") and further into specific patent designs. In this work, we introduce a hierarchical multi-positive contrastive loss that leverages the LIC's taxonomy to induce such relations in the retrieval process. Our approach assigns multiple positive pairs to each patent image within a batch, with varying similarity scores based on the hierarchical taxonomy. Our experimental analysis with various vision and multimodal models on the DeepPatent2 dataset shows that the proposed method enhances the retrieval results. Notably, our method is effective with low-parameter models, which require fewer computational resources and can be deployed on environments with limited hardware.",
        "translated": "专利图像是技术图纸，用于传达专利创新的信息。专利图像检索系统旨在海量图像集合中搜索并检索出最相关的图像。尽管信息检索领域取得了最新进展，但专利图像因其技术复杂性和复杂的语义信息，仍然带来了巨大挑战，需要高效的领域适应性微调。\n\n现有方法忽略了专利的层级关系，例如由洛迦诺国际分类（LIC）系统定义的层级关系，该系统将大类（例如“家具”）分为子类（例如“座椅”和“床”），并进一步细分为具体的专利设计。在这项工作中，我们引入了一种层级多正例对比损失（hierarchical multi-positive contrastive loss），它利用LIC的分类体系在检索过程中引入此类关系。我们的方法在一个批次内为每个专利图像分配多个正例对，并根据层级分类体系赋予不同的相似度分数。\n\n我们在DeepPatent2数据集上使用各种视觉和多模态模型进行的实验分析表明，所提出的方法显著提升了检索结果。值得注意的是，我们的方法对低参数模型同样有效，这使得它所需的计算资源更少，并能够部署在硬件资源有限的环境中。"
    },
    {
        "title": "Beyond One-Size-Fits-All: A Study of Neural and Behavioural Variability\n  Across Different Recommendation Categories",
        "url": "http://arxiv.org/abs/2506.13409v1",
        "pub_date": "2025-06-16",
        "summary": "Traditionally, Recommender Systems (RS) have primarily measured performance based on the accuracy and relevance of their recommendations. However, this algorithmic-centric approach overlooks how different types of recommendations impact user engagement and shape the overall quality of experience. In this paper, we shift the focus to the user and address for the first time the challenge of decoding the neural and behavioural variability across distinct recommendation categories, considering more than just relevance. Specifically, we conducted a controlled study using a comprehensive e-commerce dataset containing various recommendation types, and collected Electroencephalography and behavioural data. We analysed both neural and behavioural responses to recommendations that were categorised as Exact, Substitute, Complement, or Irrelevant products within search query results. Our findings offer novel insights into user preferences and decision-making processes, revealing meaningful relationships between behavioural and neural patterns for each category, but also indicate inter-subject variability.",
        "translated": "传统上，推荐系统（RS）主要根据推荐的准确性和相关性来衡量其性能。然而，这种以算法为中心的方法忽视了不同类型的推荐如何影响用户参与度并塑造整体用户体验质量。在本文中，我们将重点转向用户，并首次解决了在考虑相关性之外，解码不同推荐类别中神经和行为变异性的挑战。具体而言，我们利用一个包含各种推荐类型的综合电商数据集，进行了一项对照研究，并收集了脑电图（Electroencephalography）和行为数据。我们分析了在搜索查询结果中被归类为精确（Exact）、替代（Substitute）、互补（Complement）或不相关（Irrelevant）产品的推荐所引起的神经和行为反应。我们的研究结果为用户偏好和决策过程提供了新颖的见解，揭示了每种类别中行为和神经模式之间有意义的关系，但也表明了主体间的变异性。"
    },
    {
        "title": "Decompositional Reasoning for Graph Retrieval with Large Language Models",
        "url": "http://arxiv.org/abs/2506.13380v1",
        "pub_date": "2025-06-16",
        "summary": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with multi-hop reasoning and factual consistency, limiting their effectiveness on knowledge-intensive tasks like complex question answering (QA). Linking Knowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally lack the ability to reason efficiently over graph-structured information. To tackle this problem, we propose a novel retrieval approach that integrates textual knowledge graphs into the LLM reasoning process via query decomposition. Our method decomposes complex questions into sub-questions, retrieves relevant textual subgraphs, and composes a question-specific knowledge graph to guide answer generation. For that, we use a weighted similarity function that focuses on both the complex question and the generated subquestions to extract a relevant subgraph, which allows efficient and precise retrieval for complex questions and improves the performance of LLMs on multi-hop QA tasks. This structured reasoning pipeline enhances factual grounding and interpretability while leveraging the generative strengths of LLMs. We evaluate our method on standard multi-hop QA benchmarks and show that it achieves comparable or superior performance to competitive existing methods, using smaller models and fewer LLM calls.",
        "translated": "**大型语言模型（LLMs）** 在众多自然语言处理（NLP）任务中表现出色，但在**多跳推理**和**事实一致性**方面存在困难，从而限制了它们在复杂问答（QA）等**知识密集型任务**上的有效性。将**知识图谱（KG）** 与LLMs结合已展现出有前景的结果，但LLMs通常缺乏对图结构信息进行高效推理的能力。\n\n为解决此问题，我们提出了一种新颖的**检索方法**，通过**查询分解**将文本知识图谱整合到LLM的推理过程中。我们的方法将复杂问题分解为**子问题**，检索相关的**文本子图**，并构建一个与问题相关的知识图谱以指导**答案生成**。为此，我们使用一个**加权相似性函数**，该函数同时关注复杂问题和生成的子问题，以提取相关子图，从而实现了复杂问题的高效精确检索，并提高了LLMs在多跳问答（QA）任务上的性能。这种**结构化推理流程**增强了**事实基础**和**可解释性**，同时充分利用了LLMs的生成能力。我们在标准多跳问答（QA）基准上评估了所提出的方法，结果表明，与现有有竞争力的SOTA方法相比，该方法实现了可比或更优的性能，且使用的模型更小，LLM调用次数更少。"
    },
    {
        "title": "Gated Rotary-Enhanced Linear Attention for Long-term Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.13315v1",
        "pub_date": "2025-06-16",
        "summary": "In Sequential Recommendation Systems (SRSs), Transformer models show remarkable performance but face computation cost challenges when modeling long-term user behavior sequences due to the quadratic complexity of the dot-product attention mechanism. By approximating the dot-product attention, linear attention provides an efficient option with linear complexity. However, existing linear attention methods face two limitations: 1) they often use learnable position encodings, which incur extra computational costs in long-term sequence scenarios, and 2) they may not consider the user's fine-grained local preferences and confuse these with the actual change of long-term interests. To remedy these drawbacks, we propose a long-term sequential Recommendation model with Gated Rotary Enhanced Linear Attention (RecGRELA). Specifically, we first propose a Rotary-Enhanced Linear Attention (RELA) module to model long-range dependency within the user's historical information using rotary position encodings. We then introduce a local short operation to incorporate local preferences and demonstrate the theoretical insight. We further introduce a SiLU-based Gated mechanism for RELA (GRELA) to help the model determine whether a user's behavior indicates local interest or a genuine shift in long-term preferences. Experimental results on four public datasets demonstrate that our RecGRELA achieves state-of-the-art performance compared to existing SRSs while maintaining low memory overhead.",
        "translated": "在序列推荐系统（SRSs）中，Transformer模型展现出卓越性能，但在建模长期用户行为序列时面临计算成本挑战，这源于点积注意力机制的二次复杂度。通过近似点积注意力，线性注意力提供了一种具有线性复杂度的高效选择。然而，现有的线性注意力方法面临两个局限性：1) 它们通常使用可学习的位置编码，这在长期序列场景中会带来额外的计算成本；2) 它们可能未考虑用户的细粒度局部偏好，并将其与长期兴趣的实际变化混淆。\n\n为了弥补这些不足，我们提出了一种带有门控旋转增强线性注意力的长期序列推荐模型（RecGRELA）。具体而言，我们首先提出了一个旋转增强线性注意力（RELA）模块，该模块使用旋转位置编码来建模用户历史信息中的长距离依赖关系。接着，我们引入了一个局部短操作来纳入局部偏好，并阐明了其理论洞察。此外，我们还为RELA引入了一个基于SiLU的门控机制（GRELA），以帮助模型判断用户的行为是表明局部兴趣还是长期偏好的真正转变。在四个公共数据集上的实验结果表明，与现有SRSs相比，我们的RecGRELA实现了最先进的性能，同时保持了低内存开销。"
    },
    {
        "title": "Vector Ontologies as an LLM world view extraction method",
        "url": "http://arxiv.org/abs/2506.13252v1",
        "pub_date": "2025-06-16",
        "summary": "Large Language Models (LLMs) possess intricate internal representations of the world, yet these latent structures are notoriously difficult to interpret or repurpose beyond the original prediction task. Building on our earlier work (Rothenfusser, 2025), which introduced the concept of vector ontologies as a framework for translating high-dimensional neural representations into interpretable geometric structures, this paper provides the first empirical validation of that approach. A vector ontology defines a domain-specific vector space spanned by ontologically meaningful dimensions, allowing geometric analysis of concepts and relationships within a domain. We construct an 8-dimensional vector ontology of musical genres based on Spotify audio features and test whether an LLM's internal world model of music can be consistently and accurately projected into this space. Using GPT-4o-mini, we extract genre representations through multiple natural language prompts and analyze the consistency of these projections across linguistic variations and their alignment with ground-truth data. Our results show (1) high spatial consistency of genre projections across 47 query formulations, (2) strong alignment between LLM-inferred genre locations and real-world audio feature distributions, and (3) evidence of a direct relationship between prompt phrasing and spatial shifts in the LLM's inferred vector ontology. These findings demonstrate that LLMs internalize structured, repurposable knowledge and that vector ontologies offer a promising method for extracting and analyzing this knowledge in a transparent and verifiable way.",
        "translated": "大语言模型（LLM）拥有复杂精密的内部世界表征，然而，这些潜在结构在原始预测任务之外却难以解释或复用。本文基于我们之前的研究（Rothenfusser, 2025），该研究引入了向量本体论（vector ontologies）的概念，将其作为一种将高维神经表征转换为可解释几何结构的框架。在此基础上，本文首次提供了对该方法的实证验证。向量本体论定义了一个领域特定的向量空间，该空间由本体论意义上的维度所张成，从而能够对领域内的概念和关系进行几何分析。\n\n我们基于Spotify音频特征构建了一个8维的音乐流派向量本体论，并测试了大语言模型的内部音乐世界模型是否能被一致且准确地投影到这个空间中。我们使用GPT-4o-mini，通过多个自然语言提示提取流派表征，并分析这些投影在语言变体之间的一致性及其与真实数据的对齐情况。我们的结果表明：（1）流派投影在47种查询表述中表现出高度的空间一致性；（2）大语言模型推断的流派位置与真实世界的音频特征分布之间存在很强的对齐；以及（3）提示措辞与大语言模型推断的向量本体论中空间偏移之间存在直接关系的证据。这些发现表明，大语言模型内化了结构化、可复用的知识，并且向量本体论为以透明和可验证的方式提取和分析这些知识提供了一种有前景的方法。"
    },
    {
        "title": "PB$^2$: Preference Space Exploration via Population-Based Methods in\n  Preference-Based Reinforcement Learning",
        "url": "http://arxiv.org/abs/2506.13741v1",
        "pub_date": "2025-06-16",
        "summary": "Preference-based reinforcement learning (PbRL) has emerged as a promising approach for learning behaviors from human feedback without predefined reward functions. However, current PbRL methods face a critical challenge in effectively exploring the preference space, often converging prematurely to suboptimal policies that satisfy only a narrow subset of human preferences. In this work, we identify and address this preference exploration problem through population-based methods. We demonstrate that maintaining a diverse population of agents enables more comprehensive exploration of the preference landscape compared to single-agent approaches. Crucially, this diversity improves reward model learning by generating preference queries with clearly distinguishable behaviors, a key factor in real-world scenarios where humans must easily differentiate between options to provide meaningful feedback. Our experiments reveal that current methods may fail by getting stuck in local optima, requiring excessive feedback, or degrading significantly when human evaluators make errors on similar trajectories, a realistic scenario often overlooked by methods relying on perfect oracle teachers. Our population-based approach demonstrates robust performance when teachers mislabel similar trajectory segments and shows significantly enhanced preference exploration capabilities,particularly in environments with complex reward landscapes.",
        "translated": "Preference-based reinforcement learning (PbRL) has emerged as a promising approach for learning behaviors from human feedback without predefined reward functions. However, current PbRL methods face a critical challenge in effectively exploring the preference space, often converging prematurely to suboptimal policies that satisfy only a narrow subset of human preferences. In this work, we identify and address this preference exploration problem through population-based methods. We demonstrate that maintaining a diverse population of agents enables more comprehensive exploration of the preference landscape compared to single-agent approaches. Crucially, this diversity improves reward model learning by generating preference queries with clearly distinguishable behaviors, a key factor in real-world scenarios where humans must easily differentiate between options to provide meaningful feedback. Our experiments reveal that current methods may fail by getting stuck in local optima, requiring excessive feedback, or degrading significantly when human evaluators make errors on similar trajectories, a realistic scenario often overlooked by methods relying on perfect oracle teachers. Our population-based approach demonstrates robust performance when teachers mislabel similar trajectory segments and shows significantly enhanced preference exploration capabilities, particularly in environments with complex reward landscapes.\n\n---\n\n**中文翻译：**\n\n基于偏好的强化学习（PbRL）已成为一种有前景的方法，用于在没有预定义奖励函数的情况下从人类反馈中学习行为。然而，当前的PbRL方法在有效探索偏好空间方面面临一个严峻挑战，它们经常过早地收敛到次优策略，这些策略仅能满足人类偏好的狭隘子集。在本工作中，我们通过基于种群的方法识别并解决了这一偏好探索问题。我们证明，与单智能体方法相比，维护多样化的智能体种群能够实现对偏好景观更全面的探索。至关重要的是，这种多样性通过生成具有易于区分行为的偏好查询来改善奖励模型学习，这在人类必须轻松区分不同选项才能提供有意义反馈的现实场景中是一个关键因素。我们的实验表明，当前方法可能会陷入局部最优、需要过多反馈，或者当人类评估者对相似轨迹进行误判时性能显著下降。这种情况是一个现实场景，但往往被那些依赖完美预言机教师的方法所忽视。我们的基于种群的方法在教师误标记相似轨迹片段时表现出鲁棒的性能，并显示出显著增强的偏好探索能力，尤其是在奖励景观复杂的环境中。"
    },
    {
        "title": "BanditWare: A Contextual Bandit-based Framework for Hardware Prediction",
        "url": "http://arxiv.org/abs/2506.13730v1",
        "pub_date": "2025-06-16",
        "summary": "Distributed computing systems are essential for meeting the demands of modern applications, yet transitioning from single-system to distributed environments presents significant challenges. Misallocating resources in shared systems can lead to resource contention, system instability, degraded performance, priority inversion, inefficient utilization, increased latency, and environmental impact.   We present BanditWare, an online recommendation system that dynamically selects the most suitable hardware for applications using a contextual multi-armed bandit algorithm. BanditWare balances exploration and exploitation, gradually refining its hardware recommendations based on observed application performance while continuing to explore potentially better options. Unlike traditional statistical and machine learning approaches that rely heavily on large historical datasets, BanditWare operates online, learning and adapting in real-time as new workloads arrive.   We evaluated BanditWare on three workflow applications: Cycles (an agricultural science scientific workflow) BurnPro3D (a web-based platform for fire science) and a matrix multiplication application. Designed for seamless integration with the National Data Platform (NDP), BanditWare enables users of all experience levels to optimize resource allocation efficiently.",
        "translated": "分布式计算系统对于满足现代应用的需求至关重要，然而，从单系统环境向分布式环境的转变带来了巨大的挑战。在共享系统中资源分配不当可能导致资源争用、系统不稳定、性能下降、优先级反转、资源利用效率低下、延迟增加以及环境影响。\n\n我们提出了 BanditWare，一个在线推荐系统，它采用上下文多臂老虎机算法，为应用动态选择最合适的硬件。BanditWare 平衡了探索与利用，基于观察到的应用性能逐步优化其硬件推荐，同时继续探索潜在更优的选择。与严重依赖大量历史数据集的传统统计和机器学习方法不同，BanditWare 在线运行，随着新工作负载的到来，实时学习和适应。\n\n我们在三个工作流应用上评估了 BanditWare：Cycles（一个农业科学科研工作流）、BurnPro3D（一个基于网络的火灾科学平台）以及一个矩阵乘法应用。BanditWare 旨在与国家数据平台（NDP）无缝集成，使各种经验水平的用户能够高效优化资源分配。"
    },
    {
        "title": "LTRR: Learning To Rank Retrievers for LLMs",
        "url": "http://arxiv.org/abs/2506.13743v1",
        "pub_date": "2025-06-16",
        "summary": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed retriever, despite growing evidence that no single retriever performs optimally across all query types. In this paper, we explore a query routing approach that dynamically selects from a pool of retrievers based on the query, using both train-free heuristics and learned routing models. We frame routing as a learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to rank retrievers by their expected utility gain to downstream LLM performance. Our experiments, conducted on synthetic QA data with controlled query type variations, show that routing-based RAG systems can outperform the best single-retriever-based systems. Performance gains are especially pronounced in models trained with the Answer Correctness (AC) metric and with pairwise learning approaches, especially with XGBoost. We also observe improvements in generalization to out-of-distribution queries. As part of the SIGIR 2025 LiveRAG challenge, our submitted system demonstrated the practical viability of our approach, achieving competitive performance in both answer correctness and faithfulness. These findings highlight the importance of both training methodology and metric selection in query routing for RAG systems.",
        "translated": "检索增强生成 (RAG) 系统通常依赖于单个固定检索器，尽管越来越多的证据表明，没有一个检索器能够在所有查询类型上都达到最佳性能。在本文中，我们探索了一种查询路由方法，该方法利用免训练启发式方法和学习型路由模型，根据查询动态地从检索器池中选择检索器。我们将路由框定为一个排序学习 (LTR) 问题，并引入了 LTRR 框架，该框架学习根据检索器对下游大型语言模型 (LLM) 性能的预期效用增益来对其进行排序。\n\n我们的实验在具有受控查询类型变体的合成问答数据上进行，结果表明基于路由的 RAG 系统能够超越表现最佳的单检索器系统。性能提升在使用答案正确性 (AC) 指标和成对学习方法（特别是 XGBoost）训练的模型中尤为显著。我们还观察到系统对分布外 (OOD) 查询的泛化能力有所提高。作为 SIGIR 2025 LiveRAG 挑战的一部分，我们提交的系统展示了所提方法的实际可行性，在答案正确性和忠实性方面均取得了有竞争力的性能。这些发现强调了训练方法和指标选择在 RAG 系统查询路由中的重要性。"
    },
    {
        "title": "A Systematic Replicability and Comparative Study of BSARec and SASRec\n  for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2506.14692v1",
        "pub_date": "2025-06-17",
        "summary": "This study aims at comparing two sequential recommender systems: Self-Attention based Sequential Recommendation (SASRec), and Beyond Self-Attention based Sequential Recommendation (BSARec) in order to check the improvement frequency enhancement - the added element in BSARec - has on recommendations. The models in the study, have been re-implemented with a common base-structure from EasyRec, with the aim of obtaining a fair and reproducible comparison. The results obtained displayed how BSARec, by including bias terms for frequency enhancement, does indeed outperform SASRec, although the increases in performance obtained, are not as high as those presented by the authors. This work aims at offering an overview on existing methods, and most importantly at underlying the importance of implementation details for performance comparison.",
        "translated": "本研究旨在比较两种序列推荐系统：基于自注意力机制的序列推荐（SASRec）和超越自注意力机制的序列推荐（BSARec），以验证频率增强（BSARec中新增的元素）对推荐效果的提升作用。研究中的模型基于EasyRec的通用基础架构进行了重新实现，旨在实现公平且可复现的比较。所获得的结果表明，BSARec通过引入用于频率增强的偏置项，性能确实优于SASRec，尽管所实现的性能提升并未达到原作者所报告的水平。本研究旨在对现有方法进行概述，更重要的是强调实现细节对于性能比较的重要性。"
    },
    {
        "title": "Refining music sample identification with a self-supervised graph neural\n  network",
        "url": "http://arxiv.org/abs/2506.14684v1",
        "pub_date": "2025-06-17",
        "summary": "Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.   In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.   To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.",
        "translated": "自动采样识别（ASID），即检测并识别在新音乐作品中被重用的音频片段，是音频查询检索领域一项重要但充满挑战的任务。尽管一项相关任务——音频指纹识别，已在“真实世界”（例如噪声、混响）条件下准确检索音乐内容方面取得了显著进展，但ASID系统在识别经过音乐修改的采样时仍面临困难。因此，开发一个能够抵抗常见音乐制作变换（如时间拉伸、音高转换、效果处理以及底层或叠加音乐）的鲁棒系统，是一个重要的开放性挑战。\n\n在这项工作中，我们提出了一种轻量级且可扩展的编码架构，它在对比学习框架内采用了图神经网络（GNN）。与当前最先进的系统相比，我们的模型仅使用 9% 的可训练参数，同时实现了可比的性能，达到了 44.2% 的平均精度（mAP）。\n\n为了提高检索质量，我们引入了一种两阶段方法：首先进行初步的粗粒度相似性搜索以进行候选选择，随后是一个交叉注意力分类器，用于拒绝不相关的匹配并优化检索到候选的排序——这是现有模型所缺乏的关键能力。此外，由于现实世界应用中的查询通常持续时间较短，我们使用为 Sample100 数据集新增的细粒度标注，对系统进行了短查询的基准测试，这些标注也作为我们工作的一部分公开发布。"
    },
    {
        "title": "RMIT-ADM+S at the SIGIR 2025 LiveRAG Challenge",
        "url": "http://arxiv.org/abs/2506.14516v1",
        "pub_date": "2025-06-17",
        "summary": "This paper presents the RMIT--ADM+S participation in the SIGIR 2025 LiveRAG Challenge. Our Generation-Retrieval-Augmented Generation (GRAG) approach relies on generating a hypothetical answer that is used in the retrieval phase, alongside the original question. GRAG also incorporates a pointwise large language model (LLM)-based re-ranking step prior to final answer generation. We describe the system architecture and the rationale behind our design choices. In particular, a systematic evaluation using the Grid of Points (GoP) framework and N-way ANOVA enabled comparison across multiple configurations, including query variant generation, question decomposition, rank fusion strategies, and prompting techniques for answer generation. Our system achieved a Relevance score of 1.199 and a Faithfulness score of 0.477 on the private leaderboard, placing among the top four finalists in the LiveRAG 2025 Challenge.",
        "translated": "以下是论文摘要的中文翻译：\n\n本文介绍了 RMIT-ADM+S 团队在 SIGIR 2025 LiveRAG 挑战赛中的参与情况。我们提出的生成-检索增强生成（GRAG）方法，基于生成一个假设性答案，该答案与原始问题一同用于检索阶段。GRAG 还结合了一个基于逐点式大语言模型（LLM）的重排序步骤，用于最终答案生成之前。我们描述了系统架构以及我们设计选择背后的原理。特别是，一项利用网格点（GoP）框架和 N 向方差分析的系统性评估，实现了多种配置间的比较，包括查询变体生成、问题分解、排序融合策略以及答案生成的提示技术。我们的系统在私人排行榜上取得了 1.199 的相关性得分和 0.477 的忠实性得分，在 LiveRAG 2025 挑战赛中跻身前四名决赛队伍。"
    },
    {
        "title": "Vela: Scalable Embeddings with Voice Large Language Models for\n  Multimodal Retrieval",
        "url": "http://arxiv.org/abs/2506.14445v1",
        "pub_date": "2025-06-17",
        "summary": "Multimodal large language models (MLLMs) have seen substantial progress in recent years. However, their ability to represent multimodal information in the acoustic domain remains underexplored. In this work, we introduce Vela, a novel framework designed to adapt MLLMs for the generation of universal multimodal embeddings. By leveraging MLLMs with specially crafted prompts and selected in-context learning examples, Vela effectively bridges the modality gap across various modalities. We then propose a single-modality training approach, where the model is trained exclusively on text pairs. Our experiments show that Vela outperforms traditional CLAP models in standard text-audio retrieval tasks. Furthermore, we introduce new benchmarks that expose CLAP models' limitations in handling long texts and complex retrieval tasks. In contrast, Vela, by harnessing the capabilities of MLLMs, demonstrates robust performance in these scenarios. Our code will soon be available.",
        "translated": "多模态大型语言模型（MLLMs）近年来取得了显著进展。然而，它们在音频领域表示多模态信息的能力仍未得到充分探索。本工作引入了Vela，一个新颖的框架，旨在通过适应多模态大型语言模型（MLLMs）来生成通用多模态嵌入。Vela通过利用MLLM，并结合精心设计的提示（prompts）和选定的上下文学习（in-context learning）示例，有效地弥合了不同模态间的模态鸿沟。接着，我们提出了一种单模态训练方法，其中模型仅通过文本对进行训练。我们的实验表明，在标准文本-音频检索任务中，Vela的表现优于传统的CLAP模型。此外，我们还引入了新的基准，揭示了CLAP模型在处理长文本和复杂检索任务时的局限性。相比之下，Vela凭借MLLM的强大能力，在这些场景中展现出稳健的性能。我们的代码即将开源。"
    },
    {
        "title": "Similarity = Value? Consultation Value Assessment and Alignment for\n  Personalized Search",
        "url": "http://arxiv.org/abs/2506.14437v1",
        "pub_date": "2025-06-17",
        "summary": "Personalized search systems in e-commerce platforms increasingly involve user interactions with AI assistants, where users consult about products, usage scenarios, and more. Leveraging consultation to personalize search services is trending. Existing methods typically rely on semantic similarity to align historical consultations with current queries due to the absence of 'value' labels, but we observe that semantic similarity alone often fails to capture the true value of consultation for personalization. To address this, we propose a consultation value assessment framework that evaluates historical consultations from three novel perspectives: (1) Scenario Scope Value, (2) Posterior Action Value, and (3) Time Decay Value. Based on this, we introduce VAPS, a value-aware personalized search model that selectively incorporates high-value consultations through a consultation-user action interaction module and an explicit objective that aligns consultations with user actions. Experiments on both public and commercial datasets show that VAPS consistently outperforms baselines in both retrieval and ranking tasks.",
        "translated": "电商平台中的个性化搜索系统越来越多地涉及用户与AI助手的交互，用户在其中咨询产品、使用场景等。利用咨询内容来个性化搜索服务正呈现上升趋势。现有方法由于缺乏“价值”标签，通常依赖语义相似性将历史咨询与当前查询对齐。但我们观察到，仅凭语义相似性往往无法捕获咨询内容对于个性化搜索的真正价值。为解决这一问题，我们提出了一个咨询价值评估框架，从三个新颖的视角评估历史咨询内容：(1) 场景范围价值，(2) 后续行动价值，和 (3) 时间衰减价值。在此基础上，我们引入了VAPS，一个价值感知个性化搜索模型。VAPS通过一个咨询-用户行为交互模块以及一个将咨询内容与用户行为对齐的明确目标，选择性地整合高价值咨询。在公开和商业数据集上的实验表明，VAPS在检索和排序任务中均始终优于基线模型。"
    },
    {
        "title": "RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG\n  Systems for the SIGIR LiveRAG Competition",
        "url": "http://arxiv.org/abs/2506.14412v1",
        "pub_date": "2025-06-17",
        "summary": "Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR 2025 LiveRAG Challenge.",
        "translated": "检索增强生成 (RAG) 通过结合大型语言模型 (LLMs) 内部的参数化知识与外部的非参数化来源，旨在提高事实准确性并最大程度地减少幻觉。LiveRAG 2025 挑战赛旨在探索 RAG 解决方案，以最大限度地提高在 DataMorgana 问答对上的准确性，这些问答对由单跳和多跳问题组成。该挑战赛提供了 Fineweb 10BT 数据集的稀疏 OpenSearch 索引和稠密 Pinecone 索引的访问权限。它限制模型使用参数量不超过 100 亿的 LLM，并且最终答案的生成必须使用 Falcon-3-10B 模型。提交的答案由一个判官 LLM 和人类评估者共同评估。通过在挑战赛条件下探索不同的检索器组合和 RAG 解决方案，我们最终的解决方案是采用了 InstructRAG，并结合了 Pinecone 检索器和 BGE 重排序器。我们的解决方案取得了 1.13 的正确性分数和 0.55 的忠实度分数，最终在 SIGIR 2025 LiveRAG 挑战赛中位列第四。"
    },
    {
        "title": "hyperFA*IR: A hypergeometric approach to fair rankings with finite\n  candidate pool",
        "url": "http://arxiv.org/abs/2506.14349v1",
        "pub_date": "2025-06-17",
        "summary": "Ranking algorithms play a pivotal role in decision-making processes across diverse domains, from search engines to job applications. When rankings directly impact individuals, ensuring fairness becomes essential, particularly for groups that are marginalised or misrepresented in the data. Most of the existing group fairness frameworks often rely on ensuring proportional representation of protected groups. However, these approaches face limitations in accounting for the stochastic nature of ranking processes or the finite size of candidate pools. To this end, we present hyperFA*IR, a framework for assessing and enforcing fairness in rankings drawn from a finite set of candidates. It relies on a generative process based on the hypergeometric distribution, which models real-world scenarios by sampling without replacement from fixed group sizes. This approach improves fairness assessment when top-$k$ selections are large relative to the pool or when protected groups are small. We compare our approach to the widely used binomial model, which treats each draw as independent with fixed probability, and demonstrate$-$both analytically and empirically$-$that our method more accurately reproduces the statistical properties of sampling from a finite population. To operationalise this framework, we propose a Monte Carlo-based algorithm that efficiently detects unfair rankings by avoiding computationally expensive parameter tuning. Finally, we adapt our generative approach to define affirmative action policies by introducing weights into the sampling process.",
        "translated": "排名算法在从搜索引擎到求职申请等各种决策过程中发挥着举足轻重的作用。当排名直接影响个人时，确保公平性至关重要，特别是对于那些在数据中被边缘化或代表性不足的群体。大多数现有的群体公平性框架通常依赖于确保受保护群体的比例代表性。然而，这些方法在考虑排名过程的随机性或候选池的有限大小时面临局限性。\n\n为此，我们提出了 hyperFA*IR，一个用于评估和保证从有限候选集中生成的排名公平性的框架。它依赖于一个基于超几何分布的生成过程，该过程通过从固定群体规模中进行不放回抽样来模拟现实世界场景。这种方法在top-k选择相对于整个候选池较大或受保护群体规模较小时，改进了公平性评估。我们将我们的方法与广泛使用的二项式模型进行比较，后者将每次抽取视为具有固定概率的独立事件，并从理论和经验上证明，我们的方法更准确地再现了从有限总体中抽样的统计特性。为了将该框架付诸实践，我们提出了一种基于蒙特卡洛的算法，该算法通过避免计算成本高昂的参数调优来有效地检测不公平排名。最后，我们调整了我们的生成方法，通过在抽样过程中引入权重来定义平权行动政策。"
    },
    {
        "title": "A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive,\n  Transparent, and Reproducible Geo-Temporal Information Synthesis",
        "url": "http://arxiv.org/abs/2506.14345v1",
        "pub_date": "2025-06-17",
        "summary": "The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.",
        "translated": "大型语言模型（LLM）的兴起已彻底改变了信息获取方式。当前，LLM还驱动着深度研究系统，这些系统能够通过规划的迭代搜索、检索和推理，生成全面、报告式的答案。然而，当前的深度研究系统仍缺乏时空能力，而这种能力对于回答涉及地理和/或时间约束的上下文丰富问题至关重要，这些问题常见于公共卫生、环境科学或社会经济分析等领域。\n\n本文提出了我们对下一代系统的愿景，明确了将时空推理整合到深度研究流程中的重要技术、基础设施和评估挑战。我们主张，应通过开放和可复现的基础设施以及严格的评估协议，增强检索和合成过程处理时空约束的能力。我们的愿景为构建更先进、更具时空感知能力的深度研究系统指明了一条路径，有望对人工智能驱动的信息获取的未来产生深远影响。"
    },
    {
        "title": "ImpReSS: Implicit Recommender System for Support Conversations",
        "url": "http://arxiv.org/abs/2506.14231v1",
        "pub_date": "2025-06-17",
        "summary": "Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.",
        "translated": "随着大语言模型（LLM）的最新进展，基于LLM的聊天机器人通过自动化交互并提供一致、可扩展的服务，彻底改变了客户支持。虽然基于LLM的对话推荐系统（CRS）因其提升推荐质量的能力而备受关注，但针对推荐在客户支持交互中隐式集成的研究却十分有限。在本文中，我们提出了ImpReSS，一个专为客户支持对话设计的隐式推荐系统。ImpReSS与现有支持聊天机器人并行运行，在用户报告问题、聊天机器人提供解决方案的场景中发挥作用。基于客户支持对话，ImpReSS识别出推荐相关解决方案产品类别（SPC）的机会，这些类别有助于解决问题或预防问题再次发生，从而也支持业务增长。与传统CRS不同，ImpReSS完全隐式地运作，不依赖于用户购买意图的任何假设。我们对ImpReSS在支持对话中推荐有助于解决所提出问题的相关SPC的能力进行了实证评估，结果显示出可喜的成果，包括：在通用问题解决方面，MRR@1（以及recall@3）分别为0.72（0.89）；在信息安全支持方面为0.82（0.83）；在网络安全故障排除方面为0.85（0.67）。为了支持未来的研究，我们的数据和代码将应要求共享。"
    },
    {
        "title": "InsertRank: LLMs can reason over BM25 scores to Improve Listwise\n  Reranking",
        "url": "http://arxiv.org/abs/2506.14086v1",
        "pub_date": "2025-06-17",
        "summary": "Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.",
        "translated": "大语言模型（LLMs）在各类信息检索任务中展现出显著进展，尤其是在作为重排序器（reranker）方面，这得益于它们通过大规模预训练获得的强大泛化能力和知识迁移能力。与此同时，基于LLM的聊天界面的兴起提升了用户预期，促使用户提出更复杂的查询，这些查询需要通过对文档进行“推理”来检索，而非通过简单的关键词匹配或语义相似性。尽管一些近期的研究工作已利用LLM的推理能力来对这类查询进行重排序，但仍有相当大的改进空间。\n\n为此，我们引入了InsertRank，这是一种基于LLM的重排序器，它在重排序过程中利用了BM25分数等词法信号，以进一步提升检索性能。InsertRank在BRIGHT（一个涵盖12个不同领域的推理基准）和R2MED（一个涵盖8个不同任务的专用医学推理检索基准）上均展现出更高的检索有效性。我们进行了详尽的评估和多项消融研究，并证明InsertRank在包括GPT、Gemini和Deepseek模型在内的多种LLM家族中持续提升了检索有效性。此外，我们还对通过改变BM25分数尺度进行的归一化（normalization）以及通过打乱文档顺序引起的位置偏差（positional bias）进行了消融研究。使用Deepseek-R1模型，InsertRank在BRIGHT基准上获得了37.5分，在R2MED基准上获得了51.1分，超越了以往的方法。"
    },
    {
        "title": "Refining music sample identification with a self-supervised graph neural\n  network",
        "url": "http://arxiv.org/abs/2506.14684v1",
        "pub_date": "2025-06-17",
        "summary": "Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.   In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.   To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.",
        "translated": "自动样本识别（ASID）旨在检测和识别在新音乐作品中被重用的音频片段，是音频查询检索领域中一项重要但具有挑战性的任务。尽管相关任务“音频指纹”在“真实世界”（例如，有噪声、混响）条件下准确检索音乐内容方面取得了显著进展，但ASID系统在识别经过音乐修改的样本时面临困难。因此，开发一个对时间拉伸、音高偏移、效果处理以及伴奏或叠加音乐等常见音乐制作变换具有鲁棒性的系统，是一个重要的开放性挑战。\n\n在这项工作中，我们提出了一种轻量级且可扩展的编码架构，该架构在对比学习框架内采用图神经网络。与当前最先进的系统相比，我们的模型仅使用了9%的可训练参数，同时达到了可比的性能，平均精度（mAP）达到44.2%。\n\n为了提升检索质量，我们引入了一种两阶段方法：首先进行用于候选选择的初步粗粒度相似性搜索，然后是一个交叉注意力分类器，用于拒绝不相关的匹配并细化检索到的候选排名——这是以往模型所缺乏的关键能力。此外，由于实际应用中的查询通常持续时间较短，我们利用为Sample100数据集制作的新的细粒度标注，对系统在短查询场景下的性能进行了基准测试，这些标注也作为我们工作的一部分公开发布。"
    },
    {
        "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets",
        "url": "http://arxiv.org/abs/2506.14761v1",
        "pub_date": "2025-06-17",
        "summary": "Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.",
        "translated": "分词（Tokenization）为输入文本强制设定了固定的粒度，这限制了语言模型对数据的操作方式以及其未来预测的范围。字节对编码（Byte Pair Encoding, BPE）及类似方案对文本进行一次性分割，构建静态词表，这使得模型受限于该选择。我们通过引入一个自回归 U-Net 来打破了这种僵化，该网络在训练过程中学习嵌入自身的标记。该网络读取原始字节，将其汇聚成单词，然后是单词对，再到最多4个单词，从而为其提供了序列的多尺度视图。在更深的阶段，模型必须预测更远的未来——即预测接下来的几个单词而非下一个字节——因此更深的阶段侧重于更广泛的语义模式，而较早的阶段则处理细粒度细节。在仔细调整并控制预训练计算量的情况下，浅层层次结构的模型能与强大的 BPE 基线模型性能持平，而更深层次结构则展现出有前景的趋势。由于分词现在内嵌于模型中，相同的系统能够处理字符级任务，并能在低资源语言之间传递知识。"
    },
    {
        "title": "DiscRec: Disentangled Semantic-Collaborative Modeling for Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.15576v1",
        "pub_date": "2025-06-18",
        "summary": "Generative recommendation is emerging as a powerful paradigm that directly generates item predictions, moving beyond traditional matching-based approaches. However, current methods face two key challenges: token-item misalignment, where uniform token-level modeling ignores item-level granularity that is critical for collaborative signal learning, and semantic-collaborative signal entanglement, where collaborative and semantic signals exhibit distinct distributions yet are fused in a unified embedding space, leading to conflicting optimization objectives that limit the recommendation performance.   To address these issues, we propose DiscRec, a novel framework that enables Disentangled Semantic-Collaborative signal modeling with flexible fusion for generative Recommendation.First, DiscRec introduces item-level position embeddings, assigned based on indices within each semantic ID, enabling explicit modeling of item structure in input token sequences.Second, DiscRec employs a dual-branch module to disentangle the two signals at the embedding layer: a semantic branch encodes semantic signals using original token embeddings, while a collaborative branch applies localized attention restricted to tokens within the same item to effectively capture collaborative signals. A gating mechanism subsequently fuses both branches while preserving the model's ability to model sequential dependencies. Extensive experiments on four real-world datasets demonstrate that DiscRec effectively decouples these signals and consistently outperforms state-of-the-art baselines. Our codes are available on https://github.com/Ten-Mao/DiscRec.",
        "translated": "生成式推荐正成为一种强大的范式，它直接生成物品预测，超越了传统的基于匹配的方法。然而，当前方法面临两个关键挑战：一是词元-物品错位，即统一的词元级建模忽略了物品级粒度，而这对于协同信号学习至关重要；二是语义-协同信号纠缠，即协同信号和语义信号表现出不同的分布，却在一个统一的嵌入空间中融合，这导致了冲突的优化目标，从而限制了推荐性能。\n\n为解决这些问题，我们提出了DiscRec，这是一个新颖的框架，旨在实现生成式推荐中语义-协同信号的解耦建模与灵活融合。首先，DiscRec引入了物品级位置嵌入，这些嵌入根据每个语义ID内的索引进行分配，从而能够在输入词元序列中显式地建模物品结构。其次，DiscRec采用一个双分支模块，在嵌入层解耦这两种信号：一个语义分支使用原始词元嵌入来编码语义信号，而一个协同分支则对同一物品内的词元应用局部注意力，以有效捕获协同信号。一个门控机制随后融合了两个分支，同时保留了模型建模序列依赖性的能力。在四个真实世界数据集上进行的大量实验表明，DiscRec有效地解耦了这些信号，并且持续优于最先进的基线方法。我们的代码可在 https://github.com/Ten-Mao/DiscRec 获取。"
    },
    {
        "title": "Multi-Interest Recommendation: A Survey",
        "url": "http://arxiv.org/abs/2506.15284v1",
        "pub_date": "2025-06-18",
        "summary": "Existing recommendation methods often struggle to model users' multifaceted preferences due to the diversity and volatility of user behavior, as well as the inherent uncertainty and ambiguity of item attributes in practical scenarios. Multi-interest recommendation addresses this challenge by extracting multiple interest representations from users' historical interactions, enabling fine-grained preference modeling and more accurate recommendations. It has drawn broad interest in recommendation research. However, current recommendation surveys have either specialized in frontier recommendation methods or delved into specific tasks and downstream applications. In this work, we systematically review the progress, solutions, challenges, and future directions of multi-interest recommendation by answering the following three questions: (1) Why is multi-interest modeling significantly important for recommendation? (2) What aspects are focused on by multi-interest modeling in recommendation? and (3) How can multi-interest modeling be applied, along with the technical details of the representative modules? We hope that this survey establishes a fundamental framework and delivers a preliminary overview for researchers interested in this field and committed to further exploration. The implementation of multi-interest recommendation summarized in this survey is maintained at https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey.",
        "translated": "现有推荐方法在实际应用中，由于用户行为的多样性和易变性，以及物品属性固有的不确定性和模糊性，往往难以建模用户多兴趣偏好。多兴趣推荐（Multi-interest recommendation）通过从用户历史交互中提取多个兴趣表示，解决了这一挑战，从而实现细粒度偏好建模和更准确的推荐。它在推荐研究领域引起了广泛关注。然而，当前的推荐综述要么专注于前沿推荐方法，要么深入探讨特定任务和下游应用。\n\n在这项工作中，我们通过回答以下三个问题，系统地综述了多兴趣推荐的进展、解决方案、挑战和未来方向：\n(1) 多兴趣建模对推荐为何如此重要？\n(2) 推荐中的多兴趣建模关注哪些方面？\n(3) 多兴趣建模如何应用，以及代表性模块的技术细节？\n\n我们希望这篇综述能为对该领域感兴趣并致力于深入探索的研究人员建立一个基本框架，并提供初步概述。本综述所总结的多兴趣推荐相关实现维护在：https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey。"
    },
    {
        "title": "Next-User Retrieval: Enhancing Cold-Start Recommendations via Generative\n  Next-User Modeling",
        "url": "http://arxiv.org/abs/2506.15267v1",
        "pub_date": "2025-06-18",
        "summary": "The item cold-start problem is critical for online recommendation systems, as the success of this phase determines whether high-quality new items can transition to popular ones, receive essential feedback to inspire creators, and thus lead to the long-term retention of creators. However, modern recommendation systems still struggle to address item cold-start challenges due to the heavy reliance on item and historical interactions, which are non-trivial for cold-start items lacking sufficient exposure and feedback. Lookalike algorithms provide a promising solution by extending feedback for new items based on lookalike users. Traditional lookalike algorithms face such limitations: (1) failing to effectively model the lookalike users and further improve recommendations with the existing rule- or model-based methods; and (2) struggling to utilize the interaction signals and incorporate diverse features in modern recommendation systems.   Inspired by lookalike algorithms, we propose Next-User Retrieval, a novel framework for enhancing cold-start recommendations via generative next-user modeling. Specifically, we employ a transformer-based model to capture the unidirectional relationships among recently interacted users and utilize these sequences to generate the next potential user who is most likely to interact with the item. The additional item features are also integrated as prefix prompt embeddings to assist the next-user generation. The effectiveness of Next-User Retrieval is evaluated through both offline experiments and online A/B tests. Our method achieves significant improvements with increases of 0.0142% in daily active users and +0.1144% in publications in Douyin, showcasing its practical applicability and scalability.",
        "translated": "物品冷启动问题对在线推荐系统至关重要，因为这一阶段的成功决定了高质量的新物品能否发展为热门物品，能否获得关键反馈以激励创作者，从而实现创作者的长期留存。然而，现代推荐系统仍难以有效解决物品冷启动挑战，因为它们严重依赖物品和历史交互信息，而对于缺乏足够曝光和反馈的冷启动物品而言，这些信息是难以获取的。相似用户算法（Lookalike algorithms）通过基于相似用户为新物品扩展反馈，提供了一种有前景的解决方案。传统的相似用户算法面临以下局限性：(1) 无法有效建模相似用户，也无法利用现有基于规则或基于模型的方法进一步改进推荐；(2) 难以在现代推荐系统中有效利用交互信号并整合多样化特征。\n\n受相似用户算法启发，我们提出了“下一位用户检索”（Next-User Retrieval），这是一个新颖的框架，旨在通过生成式下一位用户建模来增强冷启动推荐。具体而言，我们采用基于Transformer的模型来捕捉最近交互用户之间的单向关系，并利用这些序列生成最有可能与该物品交互的下一位潜在用户。额外的物品特征也被整合为前缀提示嵌入（prefix prompt embeddings），以辅助下一位用户的生成。“下一位用户检索”的有效性通过离线实验和在线A/B测试进行了评估。我们的方法取得了显著改进，使抖音的日活跃用户（DAU）增加了0.0142%，发布量增加了0.1144%，充分展示了其在实际应用中的可行性和可扩展性。"
    },
    {
        "title": "Advancing Loss Functions in Recommender Systems: A Comparative Study\n  with a Rényi Divergence-Based Solution",
        "url": "http://arxiv.org/abs/2506.15120v1",
        "pub_date": "2025-06-18",
        "summary": "Loss functions play a pivotal role in optimizing recommendation models. Among various loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are particularly effective. Their theoretical connections and differences warrant in-depth exploration. This work conducts comprehensive analyses of these losses, yielding significant insights: 1) Common strengths -- both can be viewed as augmentations of traditional losses with Distributional Robust Optimization (DRO), enhancing robustness to distributional shifts; 2) Respective limitations -- stemming from their use of different distribution distance metrics in DRO optimization, SL exhibits high sensitivity to false negative instances, whereas CCL suffers from low data utilization. To address these limitations, this work proposes a new loss function, DrRL, which generalizes SL and CCL by leveraging R\\'enyi-divergence in DRO optimization. DrRL incorporates the advantageous structures of both SL and CCL, and can be demonstrated to effectively mitigate their limitations. Extensive experiments have been conducted to validate the superiority of DrRL on both recommendation accuracy and robustness.",
        "translated": "损失函数在优化推荐模型中扮演着关键角色。在各种损失函数中，Softmax损失 (SL) 和余弦对比损失 (CCL) 尤为有效。它们的理论联系和区别值得深入探讨。本文对这些损失函数进行了全面分析，获得了重要的见解：1) 共同优点——两者都可以被视为结合了分布鲁棒优化 (DRO) 的传统损失函数的增强，从而增强了对分布偏移的鲁棒性；2) 各自的局限性——源于它们在DRO优化中使用了不同的分布距离度量，SL对假阴性实例表现出高敏感性，而CCL则存在数据利用率低的问题。为解决这些局限性，本文提出了一种新的损失函数DrRL，它通过在DRO优化中利用Rényi散度，泛化了SL和CCL。DrRL融合了SL和CCL的优势结构，并且可以证明能够有效缓解它们的局限性。广泛的实验验证了DrRL在推荐准确性和鲁棒性两方面的优越性。"
    },
    {
        "title": "Dense SAE Latents Are Features, Not Bugs",
        "url": "http://arxiv.org/abs/2506.15679v1",
        "pub_date": "2025-06-18",
        "summary": "Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \\emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.",
        "translated": "稀疏自编码器（SAE）旨在通过施加稀疏性约束，从语言模型中提取可解释的特征。理想情况下，SAE的训练应生成既稀疏又语义有意义的隐变量。然而，许多SAE隐变量频繁激活（即是“密集”的），这引发了它们可能是训练过程中不良产物的担忧。\n\n本研究系统地调查了密集隐变量的几何、功能和起源，并表明它们不仅持久存在，而且通常反映了有意义的模型表示。我们首先证明，密集隐变量倾向于形成对立对，这些对立对能够重构残差流中的特定方向。并且，消融其子空间会抑制在重新训练的SAE中出现新的密集特征——这表明高密度特征是残差空间的一种内在属性。随后，我们引入了密集隐变量的分类法，识别出与位置跟踪、上下文绑定、熵调节、特定字母输出信号、词性以及主成分重构相关的类别。最后，我们分析了这些特征在不同层级间的演变，揭示了从早期层级的结构特征，到中间层级的语义特征，再到模型最后层级面向输出的信号的转变。\n\n我们的发现表明，密集隐变量在语言模型计算中扮演着功能性角色，不应被视为训练噪声而忽视。"
    },
    {
        "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards\n  Improve Preference Learning",
        "url": "http://arxiv.org/abs/2506.15651v1",
        "pub_date": "2025-06-18",
        "summary": "Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. We present AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, we employ language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. Our analysis confirms that the extracted rules exhibit good agreement with dataset preference. We find that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, our case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix, and the code is open-sourced at https://github.com/cxcscmu/AutoRule.",
        "translated": "基于规则的奖励为改进人类反馈强化学习（RLHF）提供了一种有前景的策略，但当前方法通常依赖手动规则工程。我们提出了 AutoRule，一种从偏好反馈中提取规则并将其转化为基于规则的奖励的全自动方法。AutoRule 的提取过程分为三个阶段：它利用推理模型解释用户偏好，从这些解释的推理链中识别候选规则，并将它们综合成一个统一的规则集。利用最终确定的规则集，我们采用语言模型验证器来计算每个输出满足规则的比例，并在策略优化期间将此指标作为辅助奖励，与学习到的奖励模型并行使用。与使用相同学习奖励模型但没有基于规则辅助奖励进行训练的 GRPO 基线模型相比，使用 AutoRule 训练 Llama-3-8B 模型，在 AlpacaEval2.0 上实现了长度受控胜率 28.6% 的相对提升，并在一个保留的 MT-Bench 子集上实现了第二轮性能 6.1% 的相对提升。我们的分析证实，提取的规则与数据集偏好表现出良好的一致性。我们发现，在两轮运行中，AutoRule 相比于学习到的奖励模型，展现出更低的奖励作弊（reward hacking）现象。最后，我们的案例研究表明，提取的规则捕捉到了不同数据集中所重视的独特质量。提取的规则已在附录中提供，代码已在 https://github.com/cxcscmu/AutoRule 开源。"
    },
    {
        "title": "Towards AI Search Paradigm",
        "url": "http://arxiv.org/abs/2506.17188v1",
        "pub_date": "2025-06-20",
        "summary": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.",
        "translated": "本论文引入了AI搜索范式，这是一个全面的蓝图，旨在构建能够模拟人类信息处理和决策的下一代搜索系统。该范式采用了一种由四个LLM（大型语言模型）驱动的智能体（Master、Planner、Executor和Writer）组成的模块化架构，这些智能体能够动态适应从简单事实查询到复杂多阶段推理任务的各种信息需求。这些智能体通过协调的工作流动态协作，以评估查询复杂性、将问题分解为可执行计划，并协调工具使用、任务执行和内容合成。我们系统地介绍了实现这一范式的关键方法，包括任务规划和工具集成、执行策略、对齐且鲁棒的检索增强生成，以及高效的LLM推理，涵盖了算法技术和基础设施层面的优化。通过为这些基础组件提供深入指导，本工作旨在为开发可信赖、自适应和可扩展的AI搜索系统提供参考。"
    },
    {
        "title": "PersonalAI: Towards digital twins in the graph form",
        "url": "http://arxiv.org/abs/2506.17001v1",
        "pub_date": "2025-06-20",
        "summary": "The challenge of personalizing language models, specifically the ability to account for a user's history during interactions, is of significant interest. Despite recent advancements in large language models (LLMs) and Retrieval Augmented Generation that have enhanced the factual base of LLMs, the task of retaining extensive personal information and using it to generate personalized responses remains pertinent. To address this, we propose utilizing external memory in the form of knowledge graphs, which are constructed and updated by the LLM itself. We have expanded upon ideas of AriGraph architecture and for the first time introduced a combined graph featuring both standard edges and two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and DiaASQ benchmarks indicates that this approach aids in making the process of graph construction and knowledge extraction unified and robust. Furthermore, we augmented the DiaASQ benchmark by incorporating parameters such as time into dialogues and introducing contradictory statements made by the same speaker at different times. Despite these modifications, the performance of the question-answering system remained robust, demonstrating the proposed architecture's ability to maintain and utilize temporal dependencies.",
        "translated": "个性化语言模型，特别是如何在交互过程中考虑用户历史信息的能力，备受关注。尽管大语言模型（LLMs）和检索增强生成（Retrieval Augmented Generation，RAG）的最新进展已显著提升了LLMs的事实基础，但保留大量个人信息并利用其生成个性化回复的任务仍然具有重要意义。\n\n为解决这一问题，我们提出利用外部记忆，具体形式为由LLM自身构建和更新的知识图谱。我们在AriGraph架构思想的基础上进行了扩展，并首次引入了一种结合了标准边和两种类型超边的组合图。在TriviaQA、HotpotQA和DiaASQ基准测试集上进行的实验表明，该方法有助于使图谱构建和知识提取过程更加统一和鲁棒。此外，我们通过在对话中引入时间等参数，并加入同一说话者在不同时间做出的矛盾性陈述，对DiaASQ基准测试集进行了增强。尽管进行了这些修改，问答系统的性能依然保持鲁棒，这证明了所提出的架构能够维护和利用时间依赖性。"
    },
    {
        "title": "RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed\n  Question Answering",
        "url": "http://arxiv.org/abs/2506.16988v1",
        "pub_date": "2025-06-20",
        "summary": "We present RAGentA, a multi-agent retrieval-augmented generation (RAG) framework for attributed question answering (QA). With the goal of trustworthy answer generation, RAGentA focuses on optimizing answer correctness, defined by coverage and relevance to the question and faithfulness, which measures the extent to which answers are grounded in retrieved documents. RAGentA uses a multi-agent architecture that iteratively filters retrieved documents, generates attributed answers with in-line citations, and verifies completeness through dynamic refinement. Central to the framework is a hybrid retrieval strategy that combines sparse and dense methods, improving Recall@20 by 12.5% compared to the best single retrieval model, resulting in more correct and well-supported answers. Evaluated on a synthetic QA dataset derived from the FineWeb index, RAGentA outperforms standard RAG baselines, achieving gains of 1.09% in correctness and 10.72% in faithfulness. These results demonstrate the effectiveness of the multi-agent architecture and hybrid retrieval in advancing trustworthy QA.",
        "translated": "我们提出了 RAGentA，一个用于归因问答（QA）的多智能体检索增强生成（RAG）框架。RAGentA 以生成可信答案为目标，致力于优化答案的正确性（由覆盖度和与问题的相关性定义）和忠实性（衡量答案在多大程度上基于检索到的文档）。RAGentA 采用多智能体架构，该架构迭代地过滤检索到的文档，生成带有行内引用的归因答案，并通过动态优化验证其完整性。该框架的核心是一种混合检索策略，它结合了稀疏和密集方法，与最佳单一检索模型相比，将 Recall@20 提高了 12.5%，从而生成了更正确且有充分支撑的答案。在源自 FineWeb 索引的合成问答数据集上进行评估，RAGentA 优于标准 RAG 基线，在正确性方面提升了 1.09%，在忠实性方面提升了 10.72%。这些结果证明了多智能体架构和混合检索在推动可信问答方面的有效性。"
    },
    {
        "title": "Pyramid Mixer: Multi-dimensional Multi-period Interest Modeling for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2506.16942v1",
        "pub_date": "2025-06-20",
        "summary": "Sequential recommendation, a critical task in recommendation systems, predicts the next user action based on the understanding of the user's historical behaviors. Conventional studies mainly focus on cross-behavior modeling with self-attention based methods while neglecting comprehensive user interest modeling for more dimensions. In this study, we propose a novel sequential recommendation model, Pyramid Mixer, which leverages the MLP-Mixer architecture to achieve efficient and complete modeling of user interests. Our method learns comprehensive user interests via cross-behavior and cross-feature user sequence modeling. The mixer layers are stacked in a pyramid way for cross-period user temporal interest learning. Through extensive offline and online experiments, we demonstrate the effectiveness and efficiency of our method, and we obtain a +0.106% improvement in user stay duration and a +0.0113% increase in user active days in the online A/B test. The Pyramid Mixer has been successfully deployed on the industrial platform, demonstrating its scalability and impact in real-world applications.",
        "translated": "序列推荐是推荐系统中的一项关键任务，它基于对用户历史行为的理解来预测用户的下一个行为。传统研究主要关注使用基于自注意力的方法进行跨行为建模，却忽略了从更多维度进行综合用户兴趣建模。在这项研究中，我们提出了一个新颖的序列推荐模型——Pyramid Mixer，它利用MLP-Mixer架构来实现对用户兴趣的有效且完整的建模。我们的方法通过跨行为和跨特征的用户序列建模来学习综合用户兴趣。混合器层以金字塔方式堆叠，用于学习跨周期的用户时间兴趣。通过大量的离线和在线实验，我们证明了我们方法的有效性和效率，并且在在线A/B测试中，我们获得了用户停留时长0.106%的提升以及用户活跃天数0.0113%的增长。Pyramid Mixer已成功部署到工业平台，证明了其在实际应用中的可扩展性和影响力。"
    },
    {
        "title": "Multi-Objective Recommendation in the Era of Generative AI: A Survey of\n  Recent Progress and Future Prospects",
        "url": "http://arxiv.org/abs/2506.16893v1",
        "pub_date": "2025-06-20",
        "summary": "With the recent progress in generative artificial intelligence (Generative AI), particularly in the development of large language models, recommendation systems are evolving to become more versatile. Unlike traditional techniques, generative AI not only learns patterns and representations from complex data but also enables content generation, data synthesis, and personalized experiences. This generative capability plays a crucial role in the field of recommendation systems, helping to address the issue of data sparsity and improving the overall performance of recommendation systems. Numerous studies on generative AI have already emerged in the field of recommendation systems. Meanwhile, the current requirements for recommendation systems have surpassed the single utility of accuracy, leading to a proliferation of multi-objective research that considers various goals in recommendation systems. However, to the best of our knowledge, there remains a lack of comprehensive studies on multi-objective recommendation systems based on generative AI technologies, leaving a significant gap in the literature. Therefore, we investigate the existing research on multi-objective recommendation systems involving generative AI to bridge this gap. We compile current research on multi-objective recommendation systems based on generative techniques, categorizing them by objectives. Additionally, we summarize relevant evaluation metrics and commonly used datasets, concluding with an analysis of the challenges and future directions in this domain.",
        "translated": "随着生成式人工智能（Generative AI）的最新进展，特别是在大语言模型（LLMs）的开发方面，推荐系统正在演进得更加多功能。与传统技术不同，生成式AI不仅能从复杂数据中学习模式和表征，还能实现内容生成、数据合成和个性化体验。这种生成能力在推荐系统领域发挥着关键作用，有助于解决数据稀疏性问题，并提升推荐系统的整体性能。目前，推荐系统领域已经涌现出大量关于生成式AI的研究。同时，当前对推荐系统的要求已超越了单一的准确性效用，这导致了考虑推荐系统中多种目标的多目标研究激增。然而，据我们所知，目前仍缺乏对基于生成式AI技术的多目标推荐系统的全面研究，这在现有文献中留下了一个显著的空白。因此，我们旨在调研涉及生成式AI的多目标推荐系统领域的现有研究，以弥补这一空白。我们整理了基于生成式技术的多目标推荐系统的现有研究，并根据其目标进行分类。此外，我们总结了相关的评估指标和常用数据集，最后分析了该领域的挑战和未来方向。"
    },
    {
        "title": "eSapiens: A Real-World NLP Framework for Multimodal Document\n  Understanding and Enterprise Knowledge Processing",
        "url": "http://arxiv.org/abs/2506.16768v1",
        "pub_date": "2025-06-20",
        "summary": "We introduce eSapiens, a unified question-answering system designed for enterprise settings, which bridges structured databases and unstructured textual corpora via a dual-module architecture. The system combines a Text-to-SQL planner with a hybrid Retrieval-Augmented Generation (RAG) pipeline, enabling natural language access to both relational data and free-form documents. To enhance answer faithfulness, the RAG module integrates dense and sparse retrieval, commercial reranking, and a citation verification loop that ensures grounding consistency. We evaluate eSapiens on the RAGTruth benchmark across five leading large language models (LLMs), analyzing performance across key dimensions such as completeness, hallucination, and context utilization. Results demonstrate that eSapiens outperforms a FAISS baseline in contextual relevance and generation quality, with optional strict-grounding controls for high-stakes scenarios. This work provides a deployable framework for robust, citation-aware question answering in real-world enterprise applications.",
        "translated": "本文推出eSapiens，一个专为企业环境设计的统一问答系统，它通过双模块架构桥接了结构化数据库和非结构化文本语料库。该系统整合了Text-to-SQL规划器和混合式检索增强生成（RAG）流水线，从而实现了对关系数据和自由格式文档的自然语言访问。为提升回答忠实性，RAG模块融合了稠密和稀疏检索、商用重排序以及一个引用验证循环，旨在确保接地一致性。我们使用五个领先的大型语言模型（LLMs），在RAGTruth基准上对eSapiens进行了评估，并在完整性、幻觉和上下文利用率等关键维度上分析了其性能。结果表明，eSapiens在上下文相关性和生成质量方面超越了FAISS基线，并针对高风险场景提供了可选的严格接地控制。本研究为实际企业应用中鲁棒、引用感知的问答提供了一个可部署的框架。"
    },
    {
        "title": "A Simple Contrastive Framework Of Item Tokenization For Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.16683v1",
        "pub_date": "2025-06-20",
        "summary": "Generative retrieval-based recommendation has emerged as a promising paradigm aiming at directly generating the identifiers of the target candidates. However, in large-scale recommendation systems, this approach becomes increasingly cumbersome due to the redundancy and sheer scale of the token space. To overcome these limitations, recent research has explored the use of semantic tokens as an alternative to ID tokens, which typically leveraged reconstruction-based strategies, like RQ-VAE, to quantize content embeddings and significantly reduce the embedding size. However, reconstructive quantization aims for the precise reconstruction of each item embedding independently, which conflicts with the goal of generative retrieval tasks focusing more on differentiating among items. Moreover, multi-modal side information of items, such as descriptive text and images, geographical knowledge in location-based recommendation services, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Nevertheless, effectively integrating such complementary knowledge into existing generative recommendation frameworks remains challenging. To overcome these challenges, we propose a novel unsupervised deep quantization exclusively based on contrastive learning, named SimCIT (a Simple Contrastive Item Tokenization framework). Specifically, different from existing reconstruction-based strategies, SimCIT propose to use a learnable residual quantization module to align with the signals from different modalities of the items, which combines multi-modal knowledge alignment and semantic tokenization in a mutually beneficial contrastive learning framework. Extensive experiments across public datasets and a large-scale industrial dataset from various domains demonstrate SimCIT's effectiveness in LLM-based generative recommendation.",
        "translated": "基于生成式检索的推荐作为一种有前景的范式崭露头角，旨在直接生成目标候选项目的标识符。然而，在大规模推荐系统中，由于令牌空间的冗余和庞大规模，这种方法变得越来越繁琐。为了克服这些局限性，近期研究探索了使用语义令牌作为ID令牌的替代，这些方法通常利用基于重建的策略（如RQ-VAE）来量化内容嵌入并显著减小嵌入大小。然而，重建式量化旨在独立精确重建每个项目嵌入，这与生成式检索任务更侧重于区分项目的目标相冲突。此外，项目的多模态辅助信息，例如描述性文本和图像，以及基于位置的推荐服务中的地理知识，已被证明通过为交互提供更丰富的上下文，能有效改善推荐效果。尽管如此，将此类互补知识有效整合到现有生成式推荐框架中仍然具有挑战性。\n\n为了克服这些挑战，我们提出了一种完全基于对比学习的新颖无监督深度量化方法，命名为SimCIT（一个简单的对比项目令牌化框架）。具体来说，SimCIT不同于现有基于重建的策略，提出使用一个可学习的残差量化模块来对齐来自项目不同模态的信号，这在一个互利共赢的对比学习框架中结合了多模态知识对齐和语义令牌化。在来自不同领域的大量公开数据集和大规模工业数据集上的广泛实验表明，SimCIT在基于大语言模型的生成式推荐中表现出卓越的有效性。"
    },
    {
        "title": "Semantic Outlier Removal with Embedding Models and LLMs",
        "url": "http://arxiv.org/abs/2506.16644v1",
        "pub_date": "2025-06-19",
        "summary": "Modern text processing pipelines demand robust methods to remove extraneous content while preserving a document's core message. Traditional approaches such as HTML boilerplate extraction or keyword filters often fail in multilingual settings and struggle with context-sensitive nuances, whereas Large Language Models (LLMs) offer improved quality at high computational cost. We introduce SORE (Semantic Outlier Removal), a cost-effective, transparent method that leverages multilingual sentence embeddings and approximate nearest-neighbor search to identify and excise unwanted text segments. By first identifying core content via metadata embedding and then flagging segments that either closely match predefined outlier groups or deviate significantly from the core, SORE achieves near-LLM extraction precision at a fraction of the cost. Experiments on HTML datasets demonstrate that SORE outperforms structural methods and yield high precision in diverse scenarios. Our system is currently deployed in production, processing millions of documents daily across multiple languages while maintaining both efficiency and accuracy. To facilitate reproducibility and further research, we release our implementation and evaluation datasets.",
        "translated": "现代文本处理流水线需要鲁棒的方法来移除冗余内容，同时保留文档的核心信息。传统的如HTML样板内容提取或关键词过滤方法在多语言环境下往往失效，并且难以处理上下文敏感的细微差别；而大语言模型（LLM）虽然能提供更高的质量，但计算成本高昂。\n\n我们引入了SORE（语义离群点剔除），这是一种经济高效且透明的方法，它利用多语言句子嵌入和近似最近邻搜索来识别并剔除不需要的文本片段。SORE首先通过元数据嵌入识别核心内容，然后标记那些与预定义离群点组高度匹配或显著偏离核心内容的片段，从而以极低的成本实现了接近大语言模型的提取精度。在HTML数据集上的实验表明，SORE优于结构化方法，并在多样化场景中展现出高精度。\n\n我们的系统目前已投入生产使用，每天处理数百万份跨多种语言的文档，同时保持了高效性和准确性。为了促进可复现性和进一步研究，我们发布了我们的实现代码和评估数据集。"
    },
    {
        "title": "Revela: Dense Retriever Learning via Language Modeling",
        "url": "http://arxiv.org/abs/2506.16552v1",
        "pub_date": "2025-06-19",
        "summary": "Dense retrievers play a vital role in accessing external and specialized knowledge to augment language models (LMs). Training dense retrievers typically requires annotated query-document pairs, which are costly and hard to obtain in specialized domains such as code-motivating growing interest in self-supervised retriever learning. Since LMs are trained to capture token-level dependencies through a self-supervised learning objective (i.e., next-token prediction), we can analogously cast retrieval as learning dependencies among chunks of tokens. This analogy naturally leads to the question: How can we adapt self-supervised learning objectives in the spirit of language modeling to train retrievers?   To answer this question, we introduce Revela, a unified and scalable training framework for self-supervised retriever learning via language modeling. Revela models semantic dependencies among documents by conditioning next-token prediction on both local and cross-document context through an in-batch attention mechanism. This attention is weighted by retriever-computed similarity scores, enabling the retriever to be optimized as part of language modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific (CoIR) benchmarks across various retriever backbones. At a comparable parameter scale, Revela outperforms the previous best method with absolute improvements of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10, respectively, underscoring its effectiveness. Performance increases with model size, highlighting both the scalability of our approach and its promise for self-supervised retriever learning.",
        "translated": "稠密检索器在使语言模型（LMs）获取外部和专业知识以增强其能力方面发挥着至关重要的作用。训练稠密检索器通常需要标注的查询-文档对，这在代码等专业领域获取成本高昂且难度大，因此促使人们对自监督检索器学习的兴趣日益增长。鉴于语言模型通过自监督学习目标（即下词元预测）训练以捕获词元级依赖，我们可以类比地将检索视为学习词元块之间的依赖关系。这种类比自然引出了一个问题：我们如何将沿用语言建模思路的自监督学习目标应用于检索器训练？\n\n为了回答这个问题，我们引入了Revela，这是一个基于语言建模的自监督检索器学习的统一且可扩展的训练框架。Revela通过批内注意力机制，以局部和跨文档上下文为条件进行下词元预测，从而建模文档间的语义依赖。这种注意力由检索器计算的相似度分数加权，使得检索器能够作为语言建模的一部分进行优化。我们使用通用领域（BEIR）和领域特定（CoIR）基准，针对多种检索器骨干网络对Revela进行了评估。在可比较的参数规模下，Revela在NDCG@10上分别以5.2%（相对18.3%）和5.6%（相对14.4%）的绝对提升，超越了此前表现最佳的方法，强调了其有效性。性能随模型大小的增加而提升，突显了我们方法的可扩展性及其在自监督检索器学习方面的广阔前景。"
    },
    {
        "title": "Towards AI Search Paradigm",
        "url": "http://arxiv.org/abs/2506.17188v1",
        "pub_date": "2025-06-20",
        "summary": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.",
        "translated": "本文中，我们提出了**人工智能搜索范式**（AI Search Paradigm），这是一个全面的蓝图，旨在构建能够模拟人类信息处理和决策的下一代搜索系统。该范式采用模块化架构，由四个LLM驱动的智能体（Master、Planner、Executor和Writer）构成，这些智能体能够动态适应从简单事实查询到复杂多阶段推理任务的各类信息需求。这些智能体通过协调的工作流动态协作，以评估查询复杂度，将问题分解为可执行计划，并编排工具使用、任务执行和内容合成。\n\n我们系统性地介绍了实现这一范式的关键方法，包括任务规划与工具集成、执行策略、对齐且鲁棒的检索增强生成，以及高效的LLM推理，涵盖了算法技术和基础设施层面的优化。通过为这些基础组件提供深入指导，本工作旨在为开发可信赖、自适应和可扩展的AI搜索系统提供参考。"
    }
]