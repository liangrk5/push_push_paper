[
    {
        "title": "RecGPT: A Foundation Model for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2506.06270v1",
        "pub_date": "2025-06-06",
        "summary": "This work addresses a fundamental barrier in recommender systems: the inability to generalize across domains without extensive retraining. Traditional ID-based approaches fail entirely in cold-start and cross-domain scenarios where new users or items lack sufficient interaction history. Inspired by foundation models' cross-domain success, we develop a foundation model for sequential recommendation that achieves genuine zero-shot generalization capabilities. Our approach fundamentally departs from existing ID-based methods by deriving item representations exclusively from textual features. This enables immediate embedding of any new item without model retraining. We introduce unified item tokenization with Finite Scalar Quantization that transforms heterogeneous textual descriptions into standardized discrete tokens. This eliminates domain barriers that plague existing systems. Additionally, the framework features hybrid bidirectional-causal attention that captures both intra-item token coherence and inter-item sequential dependencies. An efficient catalog-aware beam search decoder enables real-time token-to-item mapping. Unlike conventional approaches confined to their training domains, RecGPT naturally bridges diverse recommendation contexts through its domain-invariant tokenization mechanism. Comprehensive evaluations across six datasets and industrial scenarios demonstrate consistent performance advantages.",
        "translated": "本工作旨在解决推荐系统面临的一个根本性障碍：即在不进行大量再训练的情况下，难以实现跨领域泛化。传统的基于ID的方法在冷启动和跨域场景中完全失效，因为新用户或新物品缺乏足够的交互历史。受基础模型跨域成功的启发，我们开发了一个用于序列推荐的基础模型，它实现了真正的零样本泛化能力。\n\n我们的方法与现有基于ID的方法从根本上不同，因为它仅从文本特征中推导物品表征。这使得任何新物品都无需模型再训练即可立即嵌入。我们引入了采用有限标量量化（FSQ）的统一物品分词方法，将异构文本描述转换为标准化离散词元。这消除了困扰现有系统的领域障碍。此外，该框架还具有混合双向-因果注意力机制，能够同时捕捉物品内词元连贯性和物品间序列依赖性。一个高效的目录感知束搜索解码器能够实现实时词元到物品的映射。\n\n与局限于其训练领域的传统方法不同，RecGPT通过其领域不变的分词机制自然地弥合了多样化的推荐上下文。在六个数据集和工业场景中的全面评估表明了其持续的性能优势。"
    },
    {
        "title": "Optimizing Recall or Relevance? A Multi-Task Multi-Head Approach for\n  Item-to-Item Retrieval in Recommendation",
        "url": "http://arxiv.org/abs/2506.06239v1",
        "pub_date": "2025-06-06",
        "summary": "The task of item-to-item (I2I) retrieval is to identify a set of relevant and highly engaging items based on a given trigger item. It is a crucial component in modern recommendation systems, where users' previously engaged items serve as trigger items to retrieve relevant content for future engagement. However, existing I2I retrieval models in industry are primarily built on co-engagement data and optimized using the recall measure, which overly emphasizes co-engagement patterns while failing to capture semantic relevance. This often leads to overfitting short-term co-engagement trends at the expense of long-term benefits such as discovering novel interests and promoting content diversity. To address this challenge, we propose MTMH, a Multi-Task and Multi-Head I2I retrieval model that achieves both high recall and semantic relevance. Our model consists of two key components: 1) a multi-task learning loss for formally optimizing the trade-off between recall and semantic relevance, and 2) a multi-head I2I retrieval architecture for retrieving both highly co-engaged and semantically relevant items. We evaluate MTMH using proprietary data from a commercial platform serving billions of users and demonstrate that it can improve recall by up to 14.4% and semantic relevance by up to 56.6% compared with prior state-of-the-art models. We also conduct live experiments to verify that MTMH can enhance both short-term consumption metrics and long-term user-experience-related metrics. Our work provides a principled approach for jointly optimizing I2I recall and semantic relevance, which has significant implications for improving the overall performance of recommendation systems.",
        "translated": "物品-物品（I2I）召回的任务是基于给定的触发物品，识别出一组相关且高度吸引人的物品。它是现代推荐系统中的一个关键组成部分，其中用户的历史互动物品可作为触发物品，用于召回相关内容以供未来互动。然而，现有行业中的I2I召回模型主要基于协同互动数据构建，并使用召回率指标进行优化，这过度强调了协同互动模式，却未能捕捉到语义相关性。这通常会导致模型过拟合短期协同互动趋势，从而牺牲了发现新兴趣和促进内容多样性等长期效益。为应对这一挑战，我们提出了MTMH，一个多任务多头（Multi-Task and Multi-Head）I2I召回模型，它能够同时实现高召回率和语义相关性。我们的模型包含两个关键组成部分：1) 一个多任务学习损失函数，用于正式优化召回率和语义相关性之间的权衡；2) 一个多头I2I召回架构，用于召回高度协同互动和语义相关的物品。我们使用来自一个服务数十亿用户的商业平台的专有数据对MTMH进行了评估，结果表明，相比于现有最先进的模型，它能将召回率提升高达14.4%，将语义相关性提升高达56.6%。我们还进行了在线实验，验证MTMH能够提升短期消费指标和长期用户体验相关指标。我们的工作为联合优化I2I召回率和语义相关性提供了一种原则性的方法，这对提升推荐系统的整体性能具有重要意义。"
    },
    {
        "title": "Recommender systems, stigmergy, and the tyranny of popularity",
        "url": "http://arxiv.org/abs/2506.06162v1",
        "pub_date": "2025-06-06",
        "summary": "Scientific recommender systems, such as Google Scholar and Web of Science, are essential tools for discovery. Search algorithms that power work through stigmergy, a collective intelligence mechanism that surfaces useful paths through repeated engagement. While generally effective, this ``rich-get-richer'' dynamic results in a small number of high-profile papers that dominate visibility. This essay argues argue that these algorithm over-reliance on popularity fosters intellectual homogeneity and exacerbates structural inequities, stifling innovative and diverse perspectives critical for scientific progress. We propose an overhaul of search platforms to incorporate user-specific calibration, allowing researchers to manually adjust the weights of factors like popularity, recency, and relevance. We also advise platform developers on how word embeddings and LLMs could be implemented in ways that increase user autonomy. While our suggestions are particularly pertinent to aligning recommender systems with scientific values, these ideas are broadly applicable to information access systems in general. Designing platforms that increase user autonomy is an important step toward more robust and dynamic information",
        "translated": "诸如 Google 学术（Google Scholar）和 Web of Science 等科学推荐系统，是重要的科研发现工具。驱动这些系统运行的搜索算法通过“触发式协作”（stigmergy）机制发挥作用，这是一种通过重复互动来揭示有用路径的集体智能机制。尽管这种机制通常有效，但其“富者愈富”（rich-get-richer）的动态会导致少数备受关注的论文占据主导可见度。\n\n本文认为，这些算法对流行度的过度依赖助长了学术思想的同质化，加剧了结构性不平等，从而扼杀了对科学进步至关重要的创新和多样化视角。我们建议对搜索平台进行彻底改革，以纳入用户特定的校准功能，允许研究人员手动调整流行度、时新度、相关性等因素的权重。我们还就平台开发者如何实施词嵌入（word embeddings）和大型语言模型（LLMs）以提高用户自主性提出建议。\n\n尽管我们的建议在使推荐系统与科学价值观保持一致方面尤其适用，但这些理念也普遍适用于一般的信息获取系统。设计能够提高用户自主性的平台是迈向更强大、更动态信息获取的重要一步。"
    },
    {
        "title": "CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval",
        "url": "http://arxiv.org/abs/2506.06144v1",
        "pub_date": "2025-06-06",
        "summary": "Online video web content is richly multimodal: a single video blends vision, speech, ambient audio, and on-screen text. Retrieval systems typically treat these modalities as independent retrieval sources, which can lead to noisy and subpar retrieval. We explore multimodal video content retrieval, where relevance can be scored from one particular modality or jointly across multiple modalities simultaneously. Consequently, an effective retriever must dynamically choose which modality (or set of modalities) best addresses the query. We introduce CLaMR, a multimodal, late-interaction retriever that jointly indexes 4 modalities: video frames, transcribed speech, on-screen text, and metadata. CLaMR jointly encodes all modalities with a unified multimodal backbone for improved contextualization and is trained to enhance dynamic modality selection via two key innovations. First, given the lack of training data for multimodal retrieval, we introduce MultiVENT 2.0++, a large-scale synthetic training dataset built on MultiVENT 2.0 (event-centric videos in various languages paired with queries) with modality-targeted queries. Next, we propose a modality-aware loss that jointly trains according to a standard contrastive objective alongside an objective for learning correct modality usage. On the test sets of MultiVENT 2.0++ and MSRVTT, conventional aggregation strategies, such as averaging similarities for baseline retrievers, degrade performance by introducing noise from irrelevant modalities. In contrast, CLaMR consistently outperforms existing retrievers: on MultiVENT 2.0++, CLaMR improves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4 over the best multi-modality retriever. We illustrate CLaMR's downstream utility on long-video QA, retrieving relevant frames and obtaining a 3.50% boost over LanguageBind on Video-MME and 1.42% over dense sampling on LongVideoBench.",
        "translated": "在线视频网络内容模态丰富：单个视频融合了视觉、语音、环境音频和屏幕文本。传统的检索系统通常将这些模态视为独立的检索源，这可能导致检索结果嘈杂且不理想。我们探索多模态视频内容检索，其中相关性可以从某一特定模态或同时从多个模态联合评估。因此，一个有效的检索器必须动态地选择哪个模态（或模态组合）最能满足查询需求。我们引入了CLaMR，一个多模态的晚期交互检索器，它联合索引了四种模态：视频帧、转录语音、屏幕文本和元数据。CLaMR通过统一的多模态骨干网络对所有模态进行联合编码，以提升上下文理解能力，并通过两项关键创新来增强动态模态选择的训练。首先，鉴于多模态检索训练数据的缺乏，我们引入了MultiVENT 2.0++，这是一个基于MultiVENT 2.0（包含多种语言的事件中心视频与查询配对）构建的大规模合成训练数据集，并加入了模态导向的查询。其次，我们提出了一种模态感知损失，它根据标准对比目标与学习正确模态使用的目标进行联合训练。在MultiVENT 2.0++和MSRVTT的测试集上，传统的聚合策略（例如对基线检索器进行相似度平均）会因为引入不相关模态的噪声而导致性能下降。相比之下，CLaMR持续超越现有检索器：在MultiVENT 2.0++上，CLaMR相较于最佳单模态检索器，nDCG@10提升了25.6；相较于最佳多模态检索器，提升了35.4。我们展示了CLaMR在长视频问答中的下游应用价值，通过检索相关帧，在Video-MME上比LanguageBind提升了3.50%，在LongVideoBench上比密集采样提升了1.42%。"
    },
    {
        "title": "Phonetically-Augmented Discriminative Rescoring for Voice Search Error\n  Correction",
        "url": "http://arxiv.org/abs/2506.06117v1",
        "pub_date": "2025-06-06",
        "summary": "End-to-end (E2E) Automatic Speech Recognition (ASR) models are trained using paired audio-text samples that are expensive to obtain, since high-quality ground-truth data requires human annotators. Voice search applications, such as digital media players, leverage ASR to allow users to search by voice as opposed to an on-screen keyboard. However, recent or infrequent movie titles may not be sufficiently represented in the E2E ASR system's training data, and hence, may suffer poor recognition.   In this paper, we propose a phonetic correction system that consists of (a) a phonetic search based on the ASR model's output that generates phonetic alternatives that may not be considered by the E2E system, and (b) a rescorer component that combines the ASR model recognition and the phonetic alternatives, and select a final system output.   We find that our approach improves word error rate between 4.4 and 7.6% relative on benchmarks of popular movie titles over a series of competitive baselines.",
        "translated": "端到端（E2E）自动语音识别（ASR）模型使用配对的音频-文本样本进行训练，但这些样本获取成本高昂，因为高质量的真实标注数据需要人工标注者。数字媒体播放器等语音搜索应用利用ASR技术，使用户能够通过语音进行搜索，而非使用屏幕键盘。然而，最近上映或不常见的电影名称可能在E2E ASR系统的训练数据中没有得到充分体现，因此可能导致较差的识别效果。\n\n本文中，我们提出了一种语音校正系统，该系统包含：(a) 基于ASR模型输出的语音搜索，用于生成E2E系统可能未考虑的语音备选项；以及 (b) 一个重打分组件，用于结合ASR模型识别结果和语音备选项，并选择最终的系统输出。\n\n我们发现，与一系列具有竞争力的基线相比，我们的方法在流行电影名称的基准测试中，相对降低了词错误率（WER）4.4%至7.6%。"
    },
    {
        "title": "On the Merits of LLM-Based Corpus Enrichment",
        "url": "http://arxiv.org/abs/2506.06015v1",
        "pub_date": "2025-06-06",
        "summary": "Generative AI (genAI) technologies -- specifically, large language models (LLMs) -- and search have evolving relations. We argue for a novel perspective: using genAI to enrich a document corpus so as to improve query-based retrieval effectiveness. The enrichment is based on modifying existing documents or generating new ones. As an empirical proof of concept, we use LLMs to generate documents relevant to a topic which are more retrievable than existing ones. In addition, we demonstrate the potential merits of using corpus enrichment for retrieval augmented generation (RAG) and answer attribution in question answering.",
        "translated": "生成式人工智能（genAI）技术——特别是大型语言模型（LLMs）——与搜索的关系正在不断演变。我们提出一种新颖的视角：利用生成式人工智能（genAI）技术来丰富文档语料库，以提高基于查询的检索有效性。这种丰富方法基于修改现有文档或生成新文档。作为一项实证性概念验证，我们使用大型语言模型（LLMs）生成了针对特定主题的文档，这些文档比现有文档更易于检索。此外，我们展示了将语料库丰富应用于检索增强生成（RAG）以及问答系统中的答案归因的潜在优势。"
    },
    {
        "title": "Respecting Temporal-Causal Consistency: Entity-Event Knowledge Graphs\n  for Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2506.05939v1",
        "pub_date": "2025-06-06",
        "summary": "Retrieval-augmented generation (RAG) based on large language models often falters on narrative documents with inherent temporal structures. Standard unstructured RAG methods rely solely on embedding-similarity matching and lack any general mechanism to encode or exploit chronological information, while knowledge graph RAG (KG-RAG) frameworks collapse every mention of an entity into a single node, erasing the evolving context that drives many queries. To formalize this challenge and draw the community's attention, we construct ChronoQA, a robust and discriminative QA benchmark that measures temporal, causal, and character consistency understanding in narrative documents (e.g., novels) under the RAG setting. We then introduce Entity-Event RAG (E^2RAG), a dual-graph framework that keeps separate entity and event subgraphs linked by a bipartite mapping, thereby preserving the temporal and causal facets needed for fine-grained reasoning. Across ChronoQA, our approach outperforms state-of-the-art unstructured and KG-based RAG baselines, with notable gains on causal and character consistency queries. E^2RAG therefore offers a practical path to more context-aware retrieval for tasks that require precise answers grounded in chronological information.",
        "translated": "基于大语言模型的检索增强生成（RAG）在处理具有固有时间结构的叙事文档时常常表现不佳。标准的非结构化RAG方法仅依赖于嵌入相似度匹配，并缺乏编码或利用时间信息的通用机制；而知识图谱RAG（KG-RAG）框架则将实体的每一次提及都合并为一个单一节点，抹去了驱动许多查询的演变上下文。\n\n为了形式化这一挑战并引起社区关注，我们构建了ChronoQA，这是一个鲁棒且具有区分度的问答基准，用于衡量RAG设置下叙事文档（例如小说）中的时间、因果和人物一致性理解能力。随后，我们提出了实体-事件RAG（E^2RAG），这是一个双图框架，它通过二分图映射将独立的实体子图和事件子图连接起来，从而保留了细粒度推理所需的时间和因果方面。\n\n在ChronoQA基准测试中，我们的方法优于最先进的非结构化和基于KG的RAG基线，在因果和人物一致性查询上取得了显著提升。因此，E^2RAG为需要基于时间信息提供精确答案的任务，提供了一条更具上下文感知能力的实用检索路径。"
    },
    {
        "title": "Research on Personalized Financial Product Recommendation by Integrating\n  Large Language Models and Graph Neural Networks",
        "url": "http://arxiv.org/abs/2506.05873v1",
        "pub_date": "2025-06-06",
        "summary": "With the rapid growth of fintech, personalized financial product recommendations have become increasingly important. Traditional methods like collaborative filtering or content-based models often fail to capture users' latent preferences and complex relationships. We propose a hybrid framework integrating large language models (LLMs) and graph neural networks (GNNs). A pre-trained LLM encodes text data (e.g., user reviews) into rich feature vectors, while a heterogeneous user-product graph models interactions and social ties. Through a tailored message-passing mechanism, text and graph information are fused within the GNN to jointly optimize embeddings. Experiments on public and real-world financial datasets show our model outperforms standalone LLM or GNN in accuracy, recall, and NDCG, with strong interpretability. This work offers new insights for personalized financial recommendations and cross-modal fusion in broader recommendation tasks.",
        "translated": "随着金融科技的快速发展，个性化金融产品推荐变得愈发重要。传统的协同过滤或基于内容的模型通常难以捕获用户的潜在偏好和复杂的相互关系。为此，我们提出一种融合大语言模型（LLM）和图神经网络（GNN）的混合框架。预训练的LLM负责将文本数据（如用户评论）编码为丰富的特征向量，而异构用户-产品图则用于建模用户与产品之间的交互以及用户的社交关系。通过定制化的消息传递机制，文本和图信息在GNN内部得到融合，从而联合优化嵌入表示。在公开和真实世界的金融数据集上进行的实验表明，我们的模型在准确率、召回率和NDCG方面均优于单独的LLM或GNN，并展现出强大的可解释性。这项工作为个性化金融推荐以及更广泛推荐任务中的跨模态融合提供了新的见解。"
    },
    {
        "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
        "url": "http://arxiv.org/abs/2506.06266v1",
        "pub_date": "2025-06-06",
        "summary": "Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.",
        "translated": "大型语言模型常用于回答基于大型文本语料库（例如代码库、法律文档或聊天记录）的查询，其方法是将整个语料库放入上下文窗口中，并利用上下文学习（ICL）的能力。尽管当前模型支持10万到100万个词元的上下文，但这种设置的服务成本很高，因为KV缓存的内存消耗随输入长度线性增长。\n\n我们探索了一种替代方案：为每个语料库离线训练一个更小的KV缓存。在推理时，我们加载这个经过训练的KV缓存（我们称之为Cartridge），并解码生成响应。关键在于，Cartridge的训练成本可以分摊到所有引用相同语料库的查询中。\n\n然而，我们发现，使用朴素的下一个词元预测方法在语料库上训练Cartridge，其效果无法与ICL媲美。取而代之的是，我们提出了一种名为“自学习”（self-study）的训练方案，其中我们生成关于语料库的合成对话，并利用上下文蒸馏目标来训练Cartridge。我们发现，通过自学习训练的Cartridge能够复现ICL的功能，同时服务成本显著降低。\n\n在具有挑战性的长上下文基准测试中，通过自学习训练的Cartridge在性能上与ICL相当，同时内存使用量减少了38.6倍，吞吐量提高了26.4倍。自学习还扩展了模型的有效上下文长度（例如在MTOB上从12.8万个词元扩展到48.4万个词元），并且令人惊讶的是，它使得Cartridge无需重新训练即可在推理时进行组合。"
    },
    {
        "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at\n  Test Time",
        "url": "http://arxiv.org/abs/2506.06254v1",
        "pub_date": "2025-06-06",
        "summary": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.",
        "translated": "大语言模型（LLM）驱动的智能体近期作为先进范式涌现，在广泛的领域和任务中展现出令人印象深刻的能力。尽管其潜力巨大，当前的LLM智能体却普遍采用“一刀切”的方法，缺乏根据用户多样化需求和偏好进行响应的灵活性。这一局限性促使我们开发了PersonaAgent，这是首个旨在解决多样化个性化任务的LLM个性化智能体框架。具体而言，PersonaAgent整合了两个互补的组件：一个包含情景记忆和语义记忆机制的个性化记忆模块；以及一个使智能体能够执行为用户量身定制的工具行动的个性化行动模块。其核心在于，角色（定义为每个用户的独特系统提示）充当着中介：它利用个性化记忆中的洞察来控制智能体的行动，而这些行动的结果反过来又会优化记忆。基于该框架，我们提出了一种测试时用户偏好对齐策略，该策略通过模拟最近的n次交互来优化角色提示，并利用模拟响应与真实响应之间的文本损失反馈，确保实时用户偏好对齐。实验评估表明，PersonaAgent不仅有效实现了行动空间的个性化，而且在测试时的实际应用中具有良好的可扩展性，显著优于其他基线方法。这些结果凸显了我们方法在提供量身定制、动态用户体验方面的可行性和潜力。"
    },
    {
        "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval",
        "url": "http://arxiv.org/abs/2506.06220v1",
        "pub_date": "2025-06-06",
        "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image retrieval benchmarks. However, bridging this success to real-world applications remains a challenge. In practice, human search behavior is rarely a one-shot action. Instead, it is often a multi-round process guided by clues in mind, that is, a mental image ranging from vague recollections to vivid mental representations of the target image. Motivated by this gap, we study the task of Mental Image Retrieval (MIR), which targets the realistic yet underexplored setting where users refine their search for a mentally envisioned image through multi-round interactions with an image search engine. Central to successful interactive retrieval is the capability of machines to provide users with clear, actionable feedback; however, existing methods rely on indirect or abstract verbal feedback, which can be ambiguous, misleading, or ineffective for users to refine the query. To overcome this, we propose GenIR, a generative multi-round retrieval paradigm leveraging diffusion-based image generation to explicitly reify the AI system's understanding at each round. These synthetic visual representations provide clear, interpretable feedback, enabling users to refine their queries intuitively and effectively. We further introduce a fully automated pipeline to generate a high-quality multi-round MIR dataset. Experimental results demonstrate that GenIR significantly outperforms existing interactive methods in the MIR scenario. This work establishes a new task with a dataset and an effective generative retrieval method, providing a foundation for future research in this direction.",
        "translated": "视觉-语言模型（VLM）在文本到图像检索基准测试中表现出强大的性能。然而，将这一成功推广到现实世界应用仍然是一个挑战。在实践中，人类的搜索行为很少是单次操作。相反，它通常是一个多轮过程，由脑海中的线索所引导，即一种心理图像，从模糊的回忆到目标图像生动的心理表征。受此差距启发，我们研究了心理图像检索（MIR）任务，该任务旨在解决用户通过与图像搜索引擎进行多轮交互来细化其对脑海中构想的图像的搜索这一现实但未充分探索的场景。\n\n成功的交互式检索关键在于机器能够向用户提供清晰、可操作的反馈；然而，现有方法依赖于间接或抽象的语言反馈，这可能对用户细化查询而言是模糊、误导性或低效的。为了克服这一问题，我们提出了GenIR，这是一种生成式多轮检索范式，它利用基于扩散的图像生成技术来明确地具象化AI系统在每一轮中的理解。这些合成视觉表示提供了清晰、可解释的反馈，使用户能够直观且有效地细化其查询。我们进一步引入了一个全自动流程来生成高质量的多轮心理图像检索数据集。实验结果表明，GenIR在心理图像检索场景中显著优于现有的交互式方法。这项工作建立了一项新任务，并提供了数据集和一种有效的生成式检索方法，为未来该方向的研究奠定了基础。"
    },
    {
        "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at\n  Test Time",
        "url": "http://arxiv.org/abs/2506.06254v1",
        "pub_date": "2025-06-06",
        "summary": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.",
        "translated": "大型语言模型（LLM）驱动的智能体近期作为先进范式涌现，在广泛的领域和任务中展现出卓越的能力。尽管其潜力巨大，当前的LLM智能体通常采用“一刀切”的方式，缺乏灵活性以响应用户多样化的需求和偏好。这一局限性促使我们开发了PersonaAgent，这是首个旨在处理多样化个性化任务的个性化LLM智能体框架。具体而言，PersonaAgent集成了两个互补的组件：一个包含情景记忆和语义记忆机制的个性化记忆模块；以及一个使智能体能够执行为用户量身定制的工具操作的个性化行动模块。其核心在于，人格（定义为每个用户的独特系统提示）充当中间层：它利用个性化记忆中的洞察来控制智能体行动，而这些行动的结果反过来又会反哺记忆。基于该框架，我们提出了一种测试时用户偏好对齐策略，该策略通过模拟最近的n次交互来优化人格提示，并利用模拟响应与真实响应之间的文本损失反馈，确保实时用户偏好对齐。实验评估表明，PersonaAgent显著优于其他基线方法，不仅有效地个性化了行动空间，而且在测试时真实世界应用中展现出良好的扩展能力。这些结果凸显了我们方法在提供量身定制的、动态的用户体验方面的可行性和潜力。"
    },
    {
        "title": "Reflect-then-Plan: Offline Model-Based Planning through a Doubly\n  Bayesian Lens",
        "url": "http://arxiv.org/abs/2506.06261v1",
        "pub_date": "2025-06-06",
        "summary": "Offline reinforcement learning (RL) is crucial when online exploration is costly or unsafe but often struggles with high epistemic uncertainty due to limited data. Existing methods rely on fixed conservative policies, restricting adaptivity and generalization. To address this, we propose Reflect-then-Plan (RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach. RefPlan unifies uncertainty modeling and MB planning by recasting planning as Bayesian posterior estimation. At deployment, it updates a belief over environment dynamics using real-time observations, incorporating uncertainty into MB planning via marginalization. Empirical results on standard benchmarks show that RefPlan significantly improves the performance of conservative offline RL policies. In particular, RefPlan maintains robust performance under high epistemic uncertainty and limited data, while demonstrating resilience to changing environment dynamics, improving the flexibility, generalizability, and robustness of offline-learned policies.",
        "translated": "离线强化学习 (RL) 在在线探索成本高昂或不安全时至关重要，但由于数据有限，它往往难以应对高认知不确定性。现有方法依赖固定的保守策略，这限制了其适应性和泛化能力。为解决此问题，我们提出了一种新颖的双重贝叶斯离线基于模型 (MB) 的规划方法：Reflect-then-Plan (RefPlan)。RefPlan 通过将规划重构为贝叶斯后验估计，统一了不确定性建模和MB规划。在部署时，它利用实时观测更新对环境动态的信念，并通过边缘化将不确定性融入MB规划。在标准基准上的实验结果表明，RefPlan 显著提升了保守离线强化学习策略的性能。具体而言，RefPlan 在高认知不确定性和数据有限的情况下仍能保持鲁棒性能，同时展现出对环境动态变化的韧性，从而提升了离线学习策略的灵活性、泛化能力和鲁棒性。"
    },
    {
        "title": "Bridging External and Parametric Knowledge: Mitigating Hallucination of\n  LLMs with Shared-Private Semantic Synergy in Dual-Stream Knowledge",
        "url": "http://arxiv.org/abs/2506.06240v1",
        "pub_date": "2025-06-06",
        "summary": "Retrieval-augmented generation (RAG) is a cost-effective approach to mitigate the hallucination of Large Language Models (LLMs) by incorporating the retrieved external knowledge into the generation process. However, external knowledge may conflict with the parametric knowledge of LLMs. Furthermore, current LLMs lack inherent mechanisms for resolving such knowledge conflicts, making traditional RAG methods suffer from degraded performance and stability. Thus, we propose a Dual-Stream Knowledge-Augmented Framework for Shared-Private Semantic Synergy (DSSP-RAG). Central to the framework is a novel approach that refines self-attention into a mixed-attention, distinguishing shared and private semantics for a controlled internal-external knowledge integration. To effectively facilitate DSSP in RAG, we further introduce an unsupervised hallucination detection method based on cognitive uncertainty, ensuring the necessity of introducing knowledge, and an Energy Quotient (EQ) based on attention difference matrices to reduce noise in the retrieved external knowledge. Extensive experiments on benchmark datasets show that DSSP-RAG can effectively resolve conflicts and enhance the complementarity of dual-stream knowledge, leading to superior performance over strong baselines.",
        "translated": "检索增强生成（RAG）是一种经济高效的方法，通过将检索到的外部知识融入生成过程，以缓解大型语言模型（LLM）的幻觉问题。然而，外部知识可能与LLM的参数知识发生冲突。此外，当前的LLM缺乏解决此类知识冲突的内在机制，导致传统RAG方法在性能和稳定性方面均有所下降。\n\n因此，我们提出了一种用于共享-私有语义协同的双流知识增强框架（DSSP-RAG）。该框架的核心是一种新颖的方法，它将自注意力机制细化为混合注意力机制，以区分共享和私有语义，从而实现对内外部知识的受控整合。为了在RAG中有效促进DSSP，我们进一步引入了一种基于认知不确定性的无监督幻觉检测方法，以确保引入知识的必要性；以及一种基于注意力差异矩阵的能量商（EQ），以减少检索到的外部知识中的噪声。在基准数据集上进行的大量实验表明，DSSP-RAG能够有效解决冲突并增强双流知识的互补性，从而实现超越强大基线的优异性能。"
    },
    {
        "title": "Building Models of Neurological Language",
        "url": "http://arxiv.org/abs/2506.06208v1",
        "pub_date": "2025-06-06",
        "summary": "This report documents the development and evaluation of domain-specific language models for neurology. Initially focused on building a bespoke model, the project adapted to rapid advances in open-source and commercial medical LLMs, shifting toward leveraging retrieval-augmented generation (RAG) and representational models for secure, local deployment. Key contributions include the creation of neurology-specific datasets (case reports, QA sets, textbook-derived data), tools for multi-word expression extraction, and graph-based analyses of medical terminology. The project also produced scripts and Docker containers for local hosting. Performance metrics and graph community results are reported, with future possible work open for multimodal models using open-source architectures like phi-4.",
        "translated": "本报告记录了神经病学领域专用语言模型的开发和评估。项目最初专注于构建定制模型，但随着开源和商业医疗大型语言模型（LLMs）的快速发展，项目策略也随之调整，转而侧重于利用检索增强生成（RAG）和表征模型，以实现安全、本地化部署。主要贡献包括：创建了神经病学专用数据集（涵盖病例报告、问答集和教材衍生数据），开发了多词表达提取工具，并进行了医学术语的基于图的分析。该项目还提供了用于本地部署的脚本和 Docker 容器。报告中提供了性能指标和图社区结果。未来的工作方向可能包括使用 phi-4 等开源架构的多模态模型。"
    },
    {
        "title": "PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels",
        "url": "http://arxiv.org/abs/2506.07606v1",
        "pub_date": "2025-06-09",
        "summary": "Stance detection identifies the viewpoint expressed in text toward a specific target, such as a political figure. While previous datasets have focused primarily on tweet-level stances from established platforms, user-level stance resources, especially on emerging platforms like Bluesky remain scarce. User-level stance detection provides a more holistic view by considering a user's complete posting history rather than isolated posts. We present the first stance detection dataset for the 2024 U.S. presidential election, collected from Bluesky and centered on Kamala Harris and Donald Trump. The dataset comprises 16,044 user-target stance pairs enriched with engagement metadata, interaction graphs, and user posting histories. PolitiSky24 was created using a carefully evaluated pipeline combining advanced information retrieval and large language models, which generates stance labels with supporting rationales and text spans for transparency. The labeling approach achieves 81\\% accuracy with scalable LLMs. This resource addresses gaps in political stance analysis through its timeliness, open-data nature, and user-level perspective. The dataset is available at https://doi.org/10.5281/zenodo.15616911",
        "translated": "立场检测旨在识别文本中针对特定目标（如政治人物）所表达的观点。尽管以往的数据集主要关注来自现有平台的推文级立场，但用户级立场资源，特别是在Bluesky等新兴平台上的此类资源，仍然稀缺。用户级立场检测通过考虑用户完整的发帖历史而非孤立的帖子，提供了一种更全面的视角。我们提出了首个针对2024年美国总统大选的立场检测数据集，该数据集从Bluesky平台收集，并以卡马拉·哈里斯和唐纳德·特朗普为中心。该数据集包含16,044个用户-目标立场对，并辅以互动元数据、互动图谱和用户发帖历史。PolitiSky24是通过结合先进信息检索（IR）技术和大型语言模型（LLM）的精心评估流程创建的，它能生成立场标签，并提供支持性理由和文本片段，以增强透明度。该标注方法在使用可扩展大型语言模型时，能达到81%的准确率。该资源通过其及时性、开放数据特性和用户级视角，弥补了政治立场分析中的不足。该数据集可在 https://doi.org/10.5281/zenodo.15616911 获取。"
    },
    {
        "title": "MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with\n  Expert Specialization",
        "url": "http://arxiv.org/abs/2506.07563v2",
        "pub_date": "2025-06-09",
        "summary": "Personalized recommendation systems must adapt to user interactions across different domains. Traditional approaches like MLoRA apply a single adaptation per domain but lack flexibility in handling diverse user behaviors. To address this, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is first trained independently to specialize in its domain before a gating network is trained to weight their contributions dynamically. We evaluate MoE-MLoRA across eight CTR models on Movielens and Taobao, showing that it improves performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20) but offers limited benefits in structured datasets with low domain diversity and sparsity. Further analysis of the number of experts per domain reveals that larger ensembles do not always improve performance, indicating the need for model-aware tuning. Our findings highlight the potential of expert-based architectures for multi-domain recommendation systems, demonstrating that task-aware specialization and adaptive gating can enhance predictive accuracy in complex environments. The implementation and code are available in our GitHub repository.",
        "translated": "个性化推荐系统必须适应跨域的用户交互。传统的MLoRA等方法在每个域采用单一的适配策略，但在处理多样化的用户行为时缺乏灵活性。为解决此问题，我们提出了MoE-MLoRA，这是一个混合专家（MoE）框架。在该框架中，每个专家首先被独立训练，以专注于其特定领域，然后训练一个门控网络来动态加权它们的贡献。\n\n我们在Movielens和淘宝数据集上的八种点击率（CTR）模型上评估了MoE-MLoRA，结果表明，它在大规模、动态数据集（在Taobao-20上加权AUC提升了1.45）中显著提升了性能，但在领域多样性低和稀疏的结构化数据集中收益有限。对每个域的专家数量的进一步分析表明，更大的集成模型并非总能提升性能，这预示着需要进行模型感知（model-aware）调优。\n\n我们的研究结果凸显了基于专家的架构在多域推荐系统中的潜力，证明了任务感知专业化和自适应门控能够在复杂环境中提高预测准确性。本研究的实现代码已在我们的GitHub仓库中开源。"
    },
    {
        "title": "Addressing Correlated Latent Exogenous Variables in Debiased Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2506.07517v1",
        "pub_date": "2025-06-09",
        "summary": "Recommendation systems (RS) aim to provide personalized content, but they face a challenge in unbiased learning due to selection bias, where users only interact with items they prefer. This bias leads to a distorted representation of user preferences, which hinders the accuracy and fairness of recommendations. To address the issue, various methods such as error imputation based, inverse propensity scoring, and doubly robust techniques have been developed. Despite the progress, from the structural causal model perspective, previous debiasing methods in RS assume the independence of the exogenous variables. In this paper, we release this assumption and propose a learning algorithm based on likelihood maximization to learn a prediction model. We first discuss the correlation and difference between unmeasured confounding and our scenario, then we propose a unified method that effectively handles latent exogenous variables. Specifically, our method models the data generation process with latent exogenous variables under mild normality assumptions. We then develop a Monte Carlo algorithm to numerically estimate the likelihood function. Extensive experiments on synthetic datasets and three real-world datasets demonstrate the effectiveness of our proposed method. The code is at https://github.com/WallaceSUI/kdd25-background-variable.",
        "translated": "推荐系统（RS）旨在提供个性化内容，但由于选择偏差（即用户只与他们偏好的项目进行交互），它们在无偏学习方面面临挑战。这种偏差导致用户偏好表示失真，进而阻碍了推荐的准确性和公平性。为了解决这个问题，研究人员已经开发了各种方法，例如基于误差插补、逆倾向加权和双重鲁棒技术。\n\n尽管取得了进展，但从结构因果模型的角度来看，以往推荐系统中的去偏方法都假设外生变量是独立的。在本文中，我们放宽了这一假设，并提出了一种基于似然最大化的学习算法来学习一个预测模型。我们首先讨论了未测量混杂变量与我们场景之间的关联与区别，然后提出了一种统一的方法，能够有效处理潜在外生变量。具体来说，我们的方法在温和的正态性假设下，对带有潜在外生变量的数据生成过程进行了建模。随后，我们开发了一种蒙特卡洛算法来数值估计似然函数。在合成数据集和三个真实世界数据集上进行的大量实验证明了我们所提出方法的有效性。\n\n代码位于：https://github.com/WallaceSUI/kdd25-background-variable。"
    },
    {
        "title": "Leveraging Historical and Current Interests for Continual Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.07466v1",
        "pub_date": "2025-06-09",
        "summary": "Sequential recommendation models based on the Transformer architecture show superior performance in harnessing long-range dependencies within user behavior via self-attention. However, naively updating them on continuously arriving non-stationary data streams incurs prohibitive computation costs or leads to catastrophic forgetting. To address this, we propose Continual Sequential Transformer for Recommendation (CSTRec) that effectively leverages well-preserved historical user interests while capturing current interests. At its core is Continual Sequential Attention (CSA), a linear attention mechanism that retains past knowledge without direct access to old data. CSA integrates two key components: (1) Cauchy-Schwarz Normalization that stabilizes training under uneven interaction frequencies, and (2) Collaborative Interest Enrichment that mitigates forgetting through shared, learnable interest pools. We further introduce a technique that facilitates learning for cold-start users by transferring historical knowledge from behaviorally similar existing users. Extensive experiments on three real-world datasets indicate that CSTRec outperforms state-of-the-art baselines in both knowledge retention and acquisition.",
        "translated": "基于Transformer架构的序列推荐模型通过自注意力机制，在捕获用户行为序列中的长程依赖方面表现出卓越的性能。然而，若对其在持续到达的非平稳数据流上进行朴素更新，则会产生高昂的计算成本或导致灾难性遗忘。为解决此问题，我们提出了持续序列Transformer推荐模型（CSTRec），该模型能够有效利用保存完好的历史用户兴趣，同时捕获当前兴趣。\n\n其核心是持续序列注意力机制（CSA），这是一种线性注意力机制，能够在无需直接访问旧数据的情况下保留过往知识。CSA集成了两个关键组件：(1) 柯西-施瓦茨归一化（Cauchy-Schwarz Normalization），旨在稳定不均匀交互频率下的训练；以及(2) 协作兴趣丰富（Collaborative Interest Enrichment），通过共享的可学习兴趣池缓解遗忘。我们进一步引入了一种技术，通过从行为相似的现有用户中迁移历史知识，从而促进冷启动用户的学习。在三个真实世界数据集上进行的大量实验表明，CSTRec在知识保留和获取两方面均超越了最先进的基线模型。"
    },
    {
        "title": "LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework\n  for LLM-Based Ranking",
        "url": "http://arxiv.org/abs/2506.07449v1",
        "pub_date": "2025-06-09",
        "summary": "Recent advances in Large Language Models (LLMs) have driven their adoption in recommender systems through Retrieval-Augmented Generation (RAG) frameworks. However, existing RAG approaches predominantly rely on flat, similarity-based retrieval that fails to leverage the rich relational structure inherent in user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass, end-to-end trainable framework that integrates personalized knowledge graph context into LLM-based recommendation ranking. Our approach extends the LlamaRec architecture by incorporating a lightweight user preference module that dynamically identifies salient relation paths within a heterogeneous knowledge graph constructed from user behavior and item metadata. These personalized subgraphs are seamlessly integrated into prompts for a fine-tuned Llama-2 model, enabling efficient and interpretable recommendations through a unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty datasets demonstrate consistent and significant improvements over LlamaRec across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates the critical value of structured reasoning in LLM-based recommendations and establishes a foundation for scalable, knowledge-aware personalization in next-generation recommender systems. Code is available at~\\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.",
        "translated": "大型语言模型（LLM）的最新进展，推动了它们通过检索增强生成（RAG）框架在推荐系统中的应用。然而，现有的RAG方法主要依赖于扁平的、基于相似度的检索，未能充分利用用户-物品交互中固有的丰富关系结构。\n\n我们引入了LlamaRec-LKG-RAG，这是一种新颖的单次、端到端可训练的框架，它将个性化知识图谱上下文集成到基于LLM的推荐排名中。我们的方法扩展了LlamaRec架构，通过引入一个轻量级的用户偏好模块，该模块能够动态识别从用户行为和物品元数据构建的异构知识图谱中的显著关系路径。这些个性化子图被无缝集成到微调后的Llama-2模型的提示中，通过统一的推理步骤实现高效且可解释的推荐。\n\n在ML-100K和Amazon Beauty数据集上进行的全面实验表明，相对于LlamaRec，该方法在关键排名指标（MRR、NDCG、Recall）上取得了持续且显著的改进。LlamaRec-LKG-RAG证明了结构化推理在基于LLM的推荐中的关键价值，并为下一代推荐系统中可扩展、知识感知的个性化奠定了基础。代码已开源于[仓库](https://github.com/VahidAz/LlamaRec-LKG-RAG)。"
    },
    {
        "title": "HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language\n  Models for Efficient Multimodal Hotel Retrieval",
        "url": "http://arxiv.org/abs/2506.07296v1",
        "pub_date": "2025-06-08",
        "summary": "We present HotelMatch-LLM, a multimodal dense retrieval model for the travel domain that enables natural language property search, addressing the limitations of traditional travel search engines which require users to start with a destination and editing search parameters. HotelMatch-LLM features three key innovations: (1) Domain-specific multi-task optimization with three novel retrieval, visual, and language modeling objectives; (2) Asymmetrical dense retrieval architecture combining a small language model (SLM) for efficient online query processing and a large language model (LLM) for embedding hotel data; and (3) Extensive image processing to handle all property image galleries. Experiments on four diverse test sets show HotelMatch-LLM significantly outperforms state-of-the-art models, including VISTA and MARVEL. Specifically, on the test set -- main query type -- we achieve 0.681 for HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our analysis highlights the impact of our multi-task optimization, the generalizability of HotelMatch-LLM across LLM architectures, and its scalability for processing large image galleries.",
        "translated": "我们提出了 HotelMatch-LLM，这是一种专为旅游领域设计的多模态稠密检索模型，它支持自然语言酒店信息搜索，解决了传统旅游搜索引擎要求用户必须先选择目的地并手动调整搜索参数的局限性。HotelMatch-LLM 具有三项关键创新：(1) 领域特定多任务优化，包含三个新颖的检索、视觉和语言建模目标；(2) 非对称稠密检索架构，结合了用于高效在线查询处理的小型语言模型（SLM）和用于嵌入酒店数据的大型语言模型（LLM）；以及 (3) 大量的图像处理能力，以处理所有酒店的图片库。在四个多样化的测试集上进行的实验表明，HotelMatch-LLM 显著优于包括 VISTA 和 MARVEL 在内的最先进模型。具体而言，在测试集（主要查询类型）上，HotelMatch-LLM 的性能指标达到了 0.681，而最有效的基线模型 MARVEL 仅为 0.603。我们的分析强调了多任务优化的影响、HotelMatch-LLM 在不同 LLM 架构下的泛化能力，以及其处理大型图片库的可扩展性。"
    },
    {
        "title": "RADAR: Recall Augmentation through Deferred Asynchronous Retrieval",
        "url": "http://arxiv.org/abs/2506.07261v1",
        "pub_date": "2025-06-08",
        "summary": "Modern large-scale recommender systems employ multi-stage ranking funnel (Retrieval, Pre-ranking, Ranking) to balance engagement and computational constraints (latency, CPU). However, the initial retrieval stage, often relying on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles to effectively surface the most engaging items from billion-scale catalogs, particularly distinguishing highly relevant and engaging candidates from merely relevant ones. We introduce Recall Augmentation through Deferred Asynchronous Retrieval (RADAR), a novel framework that leverages asynchronous, offline computation to pre-rank a significantly larger candidate set for users using the full complexity ranking model. These top-ranked items are stored and utilized as a high-quality retrieval source during online inference, bypassing online retrieval and pre-ranking stages for these candidates. We demonstrate through offline experiments that RADAR significantly boosts recall (2X Recall@200 vs DNN retrieval baseline) by effectively combining a larger retrieved candidate set with a more powerful ranking model. Online A/B tests confirm a +0.8% lift in topline engagement metrics, validating RADAR as a practical and effective method to improve recommendation quality under strict online serving constraints.",
        "translated": "现代大规模推荐系统采用多阶段排序漏斗（召回、初排、精排）以平衡用户参与度与计算资源限制（如延迟、CPU）。然而，初始召回阶段通常依赖于K近邻（KNN）等高效但不那么精确的方法，这使得它难以在十亿级别的商品目录中有效发现最具吸引力的物品，尤其难以区分高度相关且能吸引用户参与的候选物品与仅仅相关的物品。\n\n我们提出了通过延迟异步召回进行召回增强（RADAR）这一新颖框架，它利用异步离线计算，使用全复杂度排序模型为用户预先排序一个显著更大的候选集。这些排名靠前的物品被存储起来，并在在线推理期间用作高质量的召回源，从而使这些候选物品绕过在线召回和初排阶段。我们通过离线实验证明，RADAR通过有效结合更大的召回候选集和更强大的排序模型，显著提升了召回率（相较于DNN召回基线，Recall@200提升2倍）。在线A/B测试证实，核心用户参与度指标提升了0.8%，验证了RADAR是在严格在线服务约束下提升推荐质量的实用且有效方法。"
    },
    {
        "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
        "url": "http://arxiv.org/abs/2506.07976v2",
        "pub_date": "2025-06-09",
        "summary": "The current paradigm of test-time scaling relies on generating long reasoning traces (\"thinking\" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.",
        "translated": "当前测试时规模化的范式依赖于在生成响应之前生成长的推理轨迹（即“思考”更多）。在需要交互的智能体问题中，这可以通过在环境中行动之前生成思考轨迹来实现。然而，这种方法不允许智能体从环境中获取新信息，也无法随时间推移调整其行为。\n\n在这项工作中，我们提出对测试时交互进行规模化，这是测试时规模化一个尚未开发的维度，它增加了智能体的交互视野，从而能够在单次推演（rollout）中运行丰富的行为，例如探索、回溯和动态重新规划。为了证明这一规模化维度的潜力，我们研究了网络智能体领域。我们首先展示，即使是基于提示的交互规模化，在没有任何训练的情况下，也能显著提高网络基准上的任务成功率。在此基础上，我们引入了TTI（Test-Time Interaction，测试时交互），这是一种基于课程的在线强化学习（RL）方法，通过自适应调整智能体的推演长度来训练它们。\n\n使用Gemma 3 12B模型，TTI在WebVoyager和WebArena基准上生成了最先进的开源、开放数据网络智能体。我们进一步表明，TTI使智能体能够自适应地平衡探索与利用。我们的结果确立了交互规模化作为扩展每步计算量（per-step compute）的强大且互补的维度，为训练自适应智能体提供了新途径。"
    },
    {
        "title": "Discrete Scale-invariant Metric Learning for Efficient Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2506.09898v1",
        "pub_date": "2025-06-11",
        "summary": "Metric learning has attracted extensive interest for its ability to provide personalized recommendations based on the importance of observed user-item interactions. Current metric learning methods aim to push negative items away from the corresponding users and positive items by an absolute geometrical distance margin. However, items may come from imbalanced categories with different intra-class variations. Thus, the absolute distance margin may not be ideal for estimating the difference between user preferences over imbalanced items. To this end, we propose a new method, named discrete scale-invariant metric learning (DSIML), by adding binary constraints to users and items, which maps users and items into binary codes of a shared Hamming subspace to speed up the online recommendation. Specifically, we firstly propose a scale-invariant margin based on angles at the negative item points in the shared Hamming subspace. Then, we derive a scale-invariant triple hinge loss based on the margin. To capture more preference difference information, we integrate a pairwise ranking loss into the scale-invariant loss in the proposed model. Due to the difficulty of directly optimizing the mixed integer optimization problem formulated with \\textit{log-sum-exp} functions, we seek to optimize its variational quadratic upper bound and learn hash codes with an alternating optimization strategy. Experiments on benchmark datasets clearly show that our proposed method is superior to competitive metric learning and hashing-based baselines for recommender systems. The implementation code is available at https://github.com/AnonyFeb/dsml.",
        "translated": "度量学习因其能够基于观测到的用户-物品交互来提供个性化推荐而受到了广泛关注。当前的度量学习方法旨在通过一个绝对的几何距离间隔将负样本物品推离相应的用户和正样本物品。然而，物品可能来自不平衡的类别，并具有不同的类内变异性。因此，绝对距离间隔可能不适合估计用户对不平衡物品的偏好差异。为此，我们提出了一种新方法，名为离散尺度不变度量学习（DSIML），该方法通过对用户和物品添加二值约束，将它们映射到共享汉明子空间的二值编码中，以加速在线推荐。具体而言，我们首先在共享汉明子空间中，基于负样本物品点处的角度，提出了一个尺度不变的间隔。然后，我们基于该间隔推导出了一个尺度不变的三元合页损失。为了捕获更多的偏好差异信息，我们在所提出的模型中，将一个成对排序损失整合到尺度不变损失中。由于直接优化使用 \\textit{log-sum-exp} 函数表述的混合整数优化问题存在困难，我们寻求优化其变分二次上界，并采用交替优化策略来学习哈希码。在基准数据集上的实验清楚地表明，我们提出的方法在推荐系统方面优于具有竞争力的度量学习和基于哈希的基线方法。实施代码已在 https://github.com/AnonyFeb/dsml 发布。"
    },
    {
        "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data\n  Augmentation Strategies for Knowledge Graph Question Answering",
        "url": "http://arxiv.org/abs/2506.09414v1",
        "pub_date": "2025-06-11",
        "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural language processing that requires reasoning over knowledge graphs (KGs) to answer natural language questions. Recent methods utilizing large language models (LLMs) have shown remarkable semantic parsing capabilities but are limited by the scarcity of diverse annotated data and multi-hop reasoning samples. Traditional data augmentation approaches are focus mainly on single-hop questions and prone to semantic distortion, while LLM-based methods primarily address semantic distortion but usually neglect multi-hop reasoning, thus limiting data diversity. The scarcity of multi-hop samples further weakens models' generalization. To address these issues, we propose PGDA-KGQA, a prompt-guided generative framework with multiple data augmentation strategies for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by crafting meticulously engineered prompts that integrate the provided textual content, it leverages LLMs to generate large-scale (question, logical form) pairs for model training. Specifically, PGDA-KGQA enriches its training set by: (1) generating single-hop pseudo questions to improve the alignment of question semantics with KG relations; (2) applying semantic-preserving question rewriting to improve robustness against linguistic variations; (3) employing answer-guided reverse path exploration to create realistic multi-hop questions. By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA utilizes the augmented data to enhance the accuracy of logical form generation and thus improve answer retrieval performance. Experiments demonstrate that outperforms state-of-the-art methods on standard KGQA datasets, achieving improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by 1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.",
        "translated": "知识图谱问答（KGQA）是自然语言处理中的一项关键任务，它需要对知识图谱（KGs）进行推理以回答自然语言问题。近期利用大语言模型（LLMs）的方法已展现出卓越的语义解析能力，但受限于多样化标注数据和多跳推理样本的稀缺性。传统数据增强方法主要侧重于单跳问题且易产生语义失真，而基于LLM的方法主要解决语义失真但通常忽略多跳推理，从而限制了数据多样性。多跳样本的稀缺性进一步削弱了模型的泛化能力。\n\n为了解决这些问题，我们提出了PGDA-KGQA，这是一个提示引导的生成框架，其中包含多种KGQA数据增强策略。其核心是，PGDA-KGQA采用统一的提示设计范式：通过精心设计整合了所提供文本内容的提示，它利用LLMs生成大规模的（问题，逻辑形式）对以用于模型训练。具体而言，PGDA-KGQA通过以下方式丰富其训练集：(1) 生成单跳伪问题，以提高问题语义与知识图谱关系的对齐度；(2) 应用语义保持的问题重写，以提高对语言变体的鲁棒性；(3) 采用答案引导的逆向路径探索，以创建真实的多跳问题。通过采用增强-生成-检索的语义解析流水线，PGDA-KGQA利用增强后的数据提高逻辑形式生成的准确性，从而提升答案检索性能。实验表明，PGDA-KGQA在标准KGQA数据集上优于最先进的方法，在WebQSP数据集上F1、Hits@1和Accuracy分别提升了2.8%、1.2%和3.1%，在ComplexWebQuestions数据集上则分别提升了1.8%、1.1%和2.4%。"
    },
    {
        "title": "MAGMaR Shared Task System Description: Video Retrieval with OmniEmbed",
        "url": "http://arxiv.org/abs/2506.09409v1",
        "pub_date": "2025-06-11",
        "summary": "Effective video retrieval remains challenging due to the complexity of integrating visual, auditory, and textual modalities. In this paper, we explore unified retrieval methods using OmniEmbed, a powerful multimodal embedding model from the Tevatron 2.0 toolkit, in the context of the MAGMaR shared task. Evaluated on the comprehensive MultiVENT 2.0 dataset, OmniEmbed generates unified embeddings for text, images, audio, and video, enabling robust multimodal retrieval. By finetuning OmniEmbed with the combined multimodal data--visual frames, audio tracks, and textual descriptions provided in MultiVENT 2.0, we achieve substantial improvements in complex, multilingual video retrieval tasks. Our submission achieved the highest score on the MAGMaR shared task leaderboard among public submissions as of May 20th, 2025, highlighting the practical effectiveness of our unified multimodal retrieval approach. Model checkpoint in this work is opensourced.",
        "translated": "由于视觉、听觉和文本模态整合的复杂性，有效的视频检索仍然面临挑战。本文探讨了在 MAGMaR 共享任务背景下，使用 Tevatron 2.0 工具包中强大的多模态嵌入模型 OmniEmbed 来实现统一检索的方法。\n\nOmniEmbed 在综合性的 MultiVENT 2.0 数据集上进行评估，能够为文本、图像、音频和视频生成统一的嵌入表示，从而实现鲁棒的多模态检索。通过利用 MultiVENT 2.0 中提供的视觉帧、音频轨和文本描述等组合多模态数据对 OmniEmbed 进行微调，我们在复杂的、多语言视频检索任务中取得了显著提升。\n\n截至 2025 年 5 月 20 日，我们的提交方案在 MAGMaR 共享任务排行榜的公开提交中获得了最高分，这凸显了我们统一多模态检索方法的实用有效性。本文使用的模型检查点已开源。"
    },
    {
        "title": "ThinkQE: Query Expansion via an Evolving Thinking Process",
        "url": "http://arxiv.org/abs/2506.09260v1",
        "pub_date": "2025-06-10",
        "summary": "Effective query expansion for web search benefits from promoting both exploration and result diversity to capture multiple interpretations and facets of a query. While recent LLM-based methods have improved retrieval performance and demonstrate strong domain generalization without additional training, they often generate narrowly focused expansions that overlook these desiderata. We propose ThinkQE, a test-time query expansion framework addressing this limitation through two key components: a thinking-based expansion process that encourages deeper and comprehensive semantic exploration, and a corpus-interaction strategy that iteratively refines expansions using retrieval feedback from the corpus. Experiments on diverse web search benchmarks (DL19, DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches, including training-intensive dense retrievers and rerankers.",
        "translated": "网络搜索中有效的查询扩展，得益于促进探索性和结果多样性，以便捕获查询的多重释义和多重维度。尽管近期基于大语言模型（LLM）的方法在无需额外训练的情况下，提升了检索性能并展现出强大的领域泛化能力，但它们通常会生成范围过于狭窄的扩展，从而忽略了上述理想特性。\n\n为此，我们提出了ThinkQE，一个测试时（test-time）查询扩展框架，旨在解决这一局限性。该框架通过两个关键组件来实现：一是基于思考的扩展过程，旨在促进更深层次、更全面的语义探索；二是语料库交互策略，该策略利用来自语料库的检索反馈迭代地完善查询扩展。在多样化的网络搜索基准测试集（DL19、DL20和BRIGHT）上的实验表明，ThinkQE持续优于现有方法，包括那些需要大量训练的稠密检索器和重排序器。"
    },
    {
        "title": "In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation",
        "url": "http://arxiv.org/abs/2506.09221v1",
        "pub_date": "2025-06-10",
        "summary": "The spread of online misinformation poses serious threats to democratic societies. Traditionally, expert fact-checkers verify the truthfulness of information through investigative processes. However, the volume and immediacy of online content present major scalability challenges. Crowdsourcing offers a promising alternative by leveraging non-expert judgments, but it introduces concerns about bias, accuracy, and interpretability. This thesis investigates how human intelligence can be harnessed to assess the truthfulness of online information, focusing on three areas: misinformation assessment, cognitive biases, and automated fact-checking systems. Through large-scale crowdsourcing experiments and statistical modeling, it identifies key factors influencing human judgments and introduces a model for the joint prediction and explanation of truthfulness. The findings show that non-expert judgments often align with expert assessments, particularly when factors such as timing and experience are considered. By deepening our understanding of human judgment and bias in truthfulness assessment, this thesis contributes to the development of more transparent, trustworthy, and interpretable systems for combating misinformation.",
        "translated": "网络虚假信息的传播对民主社会构成严重威胁。传统上，专家事实核查员通过调查流程核实信息的真实性。然而，网络内容的海量体量和即时性带来了重大的可扩展性挑战。众包通过利用非专业人士的判断，提供了一种有前景的替代方案，但它也引发了对偏见、准确性和可解释性的担忧。\n\n本论文研究了如何利用人类智能来评估网络信息的真实性，侧重于三个领域：虚假信息评估、认知偏差和自动化事实核查系统。通过大规模众包实验和统计建模，本论文识别出影响人类判断的关键因素，并引入了一个用于真实性联合预测和解释的模型。研究结果表明，非专业人士的判断往往与专家评估一致，尤其是在考虑时效性和经验等因素时。通过深化我们对真实性评估中人类判断和偏见的理解，本论文为开发更透明、更值得信赖、更可解释的打击虚假信息系统做出了贡献。"
    },
    {
        "title": "Revisiting Graph Projections for Effective Complementary Product\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.09209v1",
        "pub_date": "2025-06-10",
        "summary": "Complementary product recommendation is a powerful strategy to improve customer experience and retail sales. However, recommending the right product is not a simple task because of the noisy and sparse nature of user-item interactions. In this work, we propose a simple yet effective method to predict a list of complementary products given a query item, based on the structure of a directed weighted graph projected from the user-item bipartite graph. We revisit bipartite graph projections for recommender systems and propose a novel approach for inferring complementarity relationships from historical user-item interactions. We compare our model with recent methods from the literature and show, despite the simplicity of our approach, an average improvement of +43% and +38% over sequential and graph-based recommenders, respectively, over different benchmarks.",
        "translated": "互补商品推荐是提升客户体验和零售额的强有力策略。然而，由于用户-商品交互中存在的噪声和稀疏性，推荐合适的产品并非易事。在本文中，我们提出了一种简单而有效的方法，用于在给定一个查询商品的情况下，基于从用户-商品二分图（user-item bipartite graph）投影得到的有向加权图（directed weighted graph）结构来预测互补商品列表。我们重新审视了用于推荐系统的二分图投影，并提出了一种从历史用户-商品交互中推断互补关系的新颖方法。我们将我们的模型与文献中最新的方法进行比较，结果表明，尽管我们的方法很简单，但在不同的基准数据集上，相较于序列推荐器（sequential recommenders）和基于图的推荐器（graph-based recommenders），我们的方法平均分别取得了43%和38%的提升。"
    },
    {
        "title": "Multimodal Representation Alignment for Cross-modal Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.08774v1",
        "pub_date": "2025-06-10",
        "summary": "Different machine learning models can represent the same underlying concept in different ways. This variability is particularly valuable for in-the-wild multimodal retrieval, where the objective is to identify the corresponding representation in one modality given another modality as input. This challenge can be effectively framed as a feature alignment problem. For example, given a sentence encoded by a language model, retrieve the most semantically aligned image based on features produced by an image encoder, or vice versa. In this work, we first investigate the geometric relationships between visual and textual embeddings derived from both vision-language models and combined unimodal models. We then align these representations using four standard similarity metrics as well as two learned ones, implemented via neural networks. Our findings indicate that the Wasserstein distance can serve as an informative measure of the modality gap, while cosine similarity consistently outperforms alternative metrics in feature alignment tasks. Furthermore, we observe that conventional architectures such as multilayer perceptrons are insufficient for capturing the complex interactions between image and text representations. Our study offers novel insights and practical considerations for researchers working in multimodal information retrieval, particularly in real-world, cross-modal applications.",
        "translated": "不同的机器学习模型能够以不同的方式表征相同的底层概念。这种多样性对于实际应用场景下的多模态检索尤为重要，其目标是在给定一种模态作为输入时，识别出另一种模态中对应的表征。这一挑战可以被有效地建模为一个特征对齐问题。例如，给定一个由语言模型编码的句子，基于图像编码器生成的特征检索语义上最对齐的图像，反之亦然。\n\n在这项工作中，我们首先探究了分别从视觉-语言模型和组合式单模态模型中获得的视觉嵌入和文本嵌入之间的几何关系。随后，我们使用四种标准相似性度量以及两种通过神经网络实现的学习型度量对这些表征进行对齐。我们的研究结果表明，Wasserstein距离可以作为模态间隙的有效衡量标准，而余弦相似度在特征对齐任务中始终优于其他备选度量。此外，我们观察到多层感知机（MLP）等传统架构不足以捕获图像和文本表征之间复杂的交互作用。我们的研究为多模态信息检索领域的研究人员提供了新颖的见解和实用性考量，尤其是在真实世界的跨模态应用中。"
    },
    {
        "title": "Paths to Causality: Finding Informative Subgraphs Within Knowledge\n  Graphs for Knowledge-Based Causal Discovery",
        "url": "http://arxiv.org/abs/2506.08771v1",
        "pub_date": "2025-06-10",
        "summary": "Inferring causal relationships between variable pairs is crucial for understanding multivariate interactions in complex systems. Knowledge-based causal discovery -- which involves inferring causal relationships by reasoning over the metadata of variables (e.g., names or textual context) -- offers a compelling alternative to traditional methods that rely on observational data. However, existing methods using Large Language Models (LLMs) often produce unstable and inconsistent results, compromising their reliability for causal inference. To address this, we introduce a novel approach that integrates Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery. Our approach identifies informative metapath-based subgraphs within KGs and further refines the selection of these subgraphs using Learning-to-Rank-based models. The top-ranked subgraphs are then incorporated into zero-shot prompts, improving the effectiveness of LLMs in inferring the causal relationship. Extensive experiments on biomedical and open-domain datasets demonstrate that our method outperforms most baselines by up to 44.4 points in F1 scores, evaluated across diverse LLMs and KGs. Our code and datasets are available on GitHub: https://github.com/susantiyuni/path-to-causality",
        "translated": "推断变量对之间的因果关系对于理解复杂系统中的多变量交互至关重要。基于知识的因果发现——通过对变量的元数据（例如，名称或文本上下文）进行推理来推断因果关系——为依赖观测数据的传统方法提供了一种引人注目的替代方法。然而，现有使用大型语言模型（LLM）的方法经常产生不稳定和不一致的结果，损害了它们进行因果推断的可靠性。\n\n为此，我们提出了一种新颖的方法，将知识图谱（KG）与LLM相结合，以增强基于知识的因果发现。我们的方法识别KG中信息丰富的元路径子图，并使用基于学习排序（Learning-to-Rank）的模型进一步优化这些子图的选择。排名靠前的子图随后被纳入零样本提示中，从而提高了LLM在推断因果关系方面的有效性。在生物医学和开放域数据集上进行的大量实验表明，我们的方法在F1分数上最高可优于大多数基线44.4点，并在多种LLM和KG上进行了评估。我们的代码和数据集可在GitHub上获取：https://github.com/susantiyuni/path-to-causality"
    },
    {
        "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and\n  Re-ranking",
        "url": "http://arxiv.org/abs/2506.09944v1",
        "pub_date": "2025-06-11",
        "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. We identify QRHEAD by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). We further introduce QR- RETRIEVER, an efficient and effective retriever that uses the accumulated attention mass of QRHEAD as retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. We also evaluate QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the querycontext attention scoring and task selection are crucial for identifying QRHEAD with strong downstream utility. Overall, our work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs.",
        "translated": "近期研究（Wu 等人，2025b）已识别出“检索头”（retrieval heads），它们是注意力头的一个子集，负责在长上下文语言模型（LM）中检索关键信息，并通过其在“大海捞针”任务中的复制粘贴行为进行衡量。在本文中，我们引入了 QRHEAD（查询聚焦检索头），这是一组改进的注意力头，能够增强从长上下文中的信息检索能力。我们通过结合输入查询聚合注意力分数来识别 QRHEAD，并利用少量真实世界任务（例如长上下文问答）的示例。我们进一步引入了 QR-RETRIEVER，这是一种高效且有效的检索器，它使用 QRHEAD 的累积注意力权重作为检索分数。我们将 QR-RETRIEVER 用于长上下文推理，通过选择检索分数最高的、最相关的部分。在多跳推理任务 LongMemEval 和 CLIPPER 上，这使得性能相较于完整上下文提升了 10% 以上，并优于强大的稠密检索器。我们还将 QR-RETRIEVER 作为重排器在 BEIR 基准测试上进行了评估，发现它实现了强大的零样本性能，优于其他基于大型语言模型（LLM）的重排器，例如 RankGPT。进一步分析表明，查询-上下文注意力评分和任务选择对于识别具有强大下游效用的 QRHEAD 都至关重要。总而言之，我们的工作贡献了一个通用检索器，并为语言模型的长上下文能力提供了可解释性见解。"
    },
    {
        "title": "Aspect-Based Opinion Summarization with Argumentation Schemes",
        "url": "http://arxiv.org/abs/2506.09917v1",
        "pub_date": "2025-06-11",
        "summary": "Reviews are valuable resources for customers making purchase decisions in online shopping. However, it is impractical for customers to go over the vast number of reviews and manually conclude the prominent opinions, which prompts the need for automated opinion summarization systems. Previous approaches, either extractive or abstractive, face challenges in automatically producing grounded aspect-centric summaries. In this paper, we propose a novel summarization system that not only captures predominant opinions from an aspect perspective with supporting evidence, but also adapts to varying domains without relying on a pre-defined set of aspects. Our proposed framework, ASESUM, summarizes viewpoints relevant to the critical aspects of a product by extracting aspect-centric arguments and measuring their salience and validity. We conduct experiments on a real-world dataset to demonstrate the superiority of our approach in capturing diverse perspectives of the original reviews compared to new and existing methods.",
        "translated": "在线购物中，评论是顾客做出购买决策的宝贵资源。然而，顾客逐一查阅海量评论并从中人工归纳出主要观点是不切实际的，这促使了对自动化观点摘要系统的需求。现有的抽取式或生成式方法在自动生成有事实依据的方面中心摘要方面面临挑战。在本文中，我们提出了一种新颖的摘要系统，它不仅能够从方面角度捕获主要观点并附带支持证据，而且无需依赖预定义的方面集合即可适应不同领域。我们提出的框架ASESUM通过提取方面中心论据并衡量其显著性和有效性，从而总结出与产品关键方面相关的观点。我们在真实世界数据集上进行了实验，结果表明，与新方法和现有方法相比，我们的方法在捕获原始评论多样化视角方面表现出优越性。"
    },
    {
        "title": "Precise Zero-Shot Pointwise Ranking with LLMs through Post-Aggregated\n  Global Context Information",
        "url": "http://arxiv.org/abs/2506.10859v1",
        "pub_date": "2025-06-12",
        "summary": "Recent advancements have successfully harnessed the power of Large Language Models (LLMs) for zero-shot document ranking, exploring a variety of prompting strategies. Comparative approaches like pairwise and listwise achieve high effectiveness but are computationally intensive and thus less practical for larger-scale applications. Scoring-based pointwise approaches exhibit superior efficiency by independently and simultaneously generating the relevance scores for each candidate document. However, this independence ignores critical comparative insights between documents, resulting in inconsistent scoring and suboptimal performance. In this paper, we aim to improve the effectiveness of pointwise methods while preserving their efficiency through two key innovations: (1) We propose a novel Global-Consistent Comparative Pointwise Ranking (GCCP) strategy that incorporates global reference comparisons between each candidate and an anchor document to generate contrastive relevance scores. We strategically design the anchor document as a query-focused summary of pseudo-relevant candidates, which serves as an effective reference point by capturing the global context for document comparison. (2) These contrastive relevance scores can be efficiently Post-Aggregated with existing pointwise methods, seamlessly integrating essential Global Context information in a training-free manner (PAGC). Extensive experiments on the TREC DL and BEIR benchmark demonstrate that our approach significantly outperforms previous pointwise methods while maintaining comparable efficiency. Our method also achieves competitive performance against comparative methods that require substantially more computational resources. More analyses further validate the efficacy of our anchor construction strategy.",
        "translated": "近期进展已成功驾驭大语言模型（LLM）进行零样本文档排序，并探索了多种提示策略。成对比较和列表比较等对比方法虽然能取得高有效性，但其计算开销大，因此不适用于大规模应用。基于评分的逐点方法通过独立且并行地生成每个候选文档的相关性分数，展现出卓越的效率。然而，这种独立性忽略了文档间关键的比较信息，从而导致评分不一致和次优性能。\n\n在本文中，我们旨在通过两项关键创新，在保持效率的同时提升逐点方法的有效性：(1) 我们提出了一种新颖的全局一致比较逐点排序（Global-Consistent Comparative Pointwise Ranking, GCCP）策略，该策略引入了每个候选文档与一个锚点文档之间的全局参考比较，以生成对比相关性分数。我们策略性地将锚点文档设计为伪相关候选文档的查询中心摘要，通过捕获文档比较的全局上下文，它可作为一个有效的参考点。(2) 这些对比相关性分数可以与现有逐点方法进行高效的后聚合（Post-Aggregated with Global Context, PAGC），以免训练的方式无缝集成关键的全局上下文信息。\n\n在TREC DL和BEIR基准数据集上的大量实验表明，我们的方法显著超越了以往的逐点方法，同时保持了可比的效率。此外，我们的方法与需要大量计算资源的对比方法相比也达到了具有竞争力的性能。进一步的分析也验证了我们锚点构建策略的有效性。"
    },
    {
        "title": "CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation\n  through Self-Training",
        "url": "http://arxiv.org/abs/2506.10844v1",
        "pub_date": "2025-06-12",
        "summary": "This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG) framework composed of specialized agents for subtasks such as planning, searching, reasoning, and coordination. Our system uses a self-training paradigm with reward-guided trajectory sampling to optimize inter-agent collaboration and enhance response generation. Evaluated on DataMorgana-derived datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms conventional RAG baselines. We further analyze competition outcomes and showcase the framework's strengths with case studies, demonstrating its efficacy for complex, real-world RAG tasks.",
        "translated": "本文提出了mRAG，这是一个多智能体检索增强生成（RAG）框架，它由专门用于规划、搜索、推理和协调等子任务的智能体组成。我们的系统采用自训练范式，通过奖励引导的轨迹采样来优化智能体间的协作并提升响应生成能力。在SIGIR 2025 LiveRAG 竞赛期间，mRAG 在源自DataMorgana的数据集上进行评估，其性能超越了传统的RAG基线。我们进一步分析了竞赛结果，并通过案例研究展示了该框架的优势，证明了其在复杂、真实世界的RAG任务中的有效性。"
    },
    {
        "title": "Constructing and Evaluating Declarative RAG Pipelines in PyTerrier",
        "url": "http://arxiv.org/abs/2506.10802v1",
        "pub_date": "2025-06-12",
        "summary": "Search engines often follow a pipeline architecture, where complex but effective reranking components are used to refine the results of an initial retrieval. Retrieval augmented generation (RAG) is an exciting application of the pipeline architecture, where the final component generates a coherent answer for the users from the retrieved documents. In this demo paper, we describe how such RAG pipelines can be formulated in the declarative PyTerrier architecture, and the advantages of doing so. Our PyTerrier-RAG extension for PyTerrier provides easy access to standard RAG datasets and evaluation measures, state-of-the-art LLM readers, and using PyTerrier's unique operator notation, easy-to-build pipelines. We demonstrate the succinctness of indexing and RAG pipelines on standard datasets (including Natural Questions) and how to build on the larger PyTerrier ecosystem with state-of-the-art sparse, learned-sparse, and dense retrievers, and other neural rankers.",
        "translated": "搜索引擎通常遵循流水线架构，其中使用复杂但有效的重排序组件来优化初始检索的结果。检索增强生成（RAG）是流水线架构的一种令人兴奋的应用，其最终组件从检索到的文档中为用户生成连贯的答案。在这篇演示论文中，我们描述了如何在声明式 PyTerrier 架构中构建此类 RAG 流水线，并阐述了这样做的好处。\n\n我们的 PyTerrier-RAG 扩展为 PyTerrier 提供了便捷的访问方式，可用于标准 RAG 数据集和评估指标、最先进的 LLM 阅读器，并且利用 PyTerrier 独特的运算符表示法，可以轻松构建流水线。我们展示了在标准数据集（包括 Natural Questions）上构建索引和 RAG 流水线的简洁性，以及如何利用更大的 PyTerrier 生态系统，结合最先进的稀疏、学习型稀疏和稠密检索器以及其他神经网络排序器。"
    },
    {
        "title": "TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to\n  Evolving Research Corpora",
        "url": "http://arxiv.org/abs/2506.10737v1",
        "pub_date": "2025-06-12",
        "summary": "The rapid evolution of scientific fields introduces challenges in organizing and retrieving scientific literature. While expert-curated taxonomies have traditionally addressed this need, the process is time-consuming and expensive. Furthermore, recent automatic taxonomy construction methods either (1) over-rely on a specific corpus, sacrificing generalizability, or (2) depend heavily on the general knowledge of large language models (LLMs) contained within their pre-training datasets, often overlooking the dynamic nature of evolving scientific domains. Additionally, these approaches fail to account for the multi-faceted nature of scientific literature, where a single research paper may contribute to multiple dimensions (e.g., methodology, new tasks, evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a framework that dynamically adapts an LLM-generated taxonomy to a given corpus across multiple dimensions. TaxoAdapt performs iterative hierarchical classification, expanding both the taxonomy width and depth based on corpus' topical distribution. We demonstrate its state-of-the-art performance across a diverse set of computer science conferences over the years to showcase its ability to structure and capture the evolution of scientific fields. As a multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more granularity-preserving and 50.41% more coherent than the most competitive baselines judged by LLMs.",
        "translated": "科学领域的快速演进给科学文献的组织和检索带来了挑战。尽管专家人工构建的分类体系（taxonomies）传统上能满足这一需求，但其过程耗时且昂贵。此外，近期自动分类体系构建方法存在两类问题：(1) 过度依赖特定语料库，牺牲了泛化能力；(2) 严重依赖大型语言模型（LLM）预训练数据中包含的通用知识，常常忽视不断演进的科学领域的动态特性。此外，这些方法未能考虑到科学文献的多维度特性，即一篇研究论文可能在多个维度上有所贡献（例如，方法论、新任务、评估指标、基准）。\n\n为了弥补这些不足，我们提出了TaxoAdapt，一个能将LLM生成的分类体系动态适应到给定语料库的跨维度框架。TaxoAdapt执行迭代分层分类，基于语料库的主题分布扩展分类体系的广度和深度。我们通过在多年间各种计算机科学会议上的实验，展示了其最先进的性能，以彰显其构建结构和捕捉科学领域演进的能力。作为一种多维度方法，TaxoAdapt生成的分类体系在LLM评判下，比最具竞争力的基线方法在粒度保持性上高出26.51%，在连贯性上高出50.41%。"
    },
    {
        "title": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of\n  Nuanced Claims",
        "url": "http://arxiv.org/abs/2506.10728v1",
        "pub_date": "2025-06-12",
        "summary": "Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely \"true\" or \"false\" -- as is frequently the case with scientific and political claims. However, a claim (e.g., \"vaccine A is better than vaccine B\") can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., \"how many biomedical papers believe vaccine A is more transportable than B?\"). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines.",
        "translated": "个人或实体提出的主张往往细致入微，无法被明确地标记为完全“真”或“假”——科学和政治领域的主张尤其如此。然而，一项主张（例如，“疫苗A优于疫苗B”）可以被剖析为其组成方面和子方面（例如，功效、安全性、分发），这些细分方面更容易单独验证。这种方法能够促成更全面、结构化的回应，为特定问题提供多维度的视角，同时允许读者优先关注主张中感兴趣的特定角度（例如，对儿童的安全性）。\n\n为此，我们提出ClaimSpect，一个基于检索增强生成（RAG）的框架，旨在自动构建处理主张时通常会考虑的方面层级结构，并利用特定语料库的视角来丰富这些方面。该结构能够对输入语料库进行分层分区，以检索相关片段，从而有助于发现新的子方面。此外，这些片段还能揭示对主张某个方面的不同观点（例如，支持、中立或反对），以及这些观点的普遍程度（例如，“有多少生物医学论文认为疫苗A比疫苗B更易于运输？”）。我们将ClaimSpect应用于我们所构建数据集中涵盖的各类真实世界科学和政治主张，展示了其在解构细致入微的主张和表示语料库内观点方面的鲁棒性和准确性。通过真实世界案例研究和人工评估，我们验证了其相对于多个基线的有效性。"
    },
    {
        "title": "Contrastive Matrix Completion with Denoising and Augmented Graph Views\n  for Robust Recommendation",
        "url": "http://arxiv.org/abs/2506.10658v1",
        "pub_date": "2025-06-12",
        "summary": "Matrix completion is a widely adopted framework in recommender systems, as predicting the missing entries in the user-item rating matrix enables a comprehensive understanding of user preferences. However, current graph neural network (GNN)-based approaches are highly sensitive to noisy or irrelevant edges--due to their inherent message-passing mechanisms--and are prone to overfitting, which limits their generalizability. To overcome these challenges, we propose a novel method called Matrix Completion using Contrastive Learning (MCCL). Our approach begins by extracting local neighborhood subgraphs for each interaction and subsequently generates two distinct graph representations. The first representation emphasizes denoising by integrating GNN layers with an attention mechanism, while the second is obtained via a graph variational autoencoder that aligns the feature distribution with a standard prior. A mutual learning loss function is employed during training to gradually harmonize these representations, enabling the model to capture common patterns and significantly enhance its generalizability. Extensive experiments on several real-world datasets demonstrate that our approach not only improves the numerical accuracy of the predicted scores--achieving up to a 0.8% improvement in RMSE--but also produces superior rankings with improvements of up to 36% in ranking metrics.",
        "translated": "矩阵补全作为推荐系统中广泛采用的框架，通过预测用户-物品评分矩阵中的缺失项，能够全面理解用户偏好。然而，当前基于图神经网络（GNN）的方法由于其固有的消息传递机制，对噪声或不相关边高度敏感，且容易过拟合，这限制了它们的泛化能力。为了克服这些挑战，我们提出了一种名为“使用对比学习的矩阵补全”（MCCL）的新颖方法。我们的方法首先为每个交互提取局部邻域子图，随后生成两种不同的图表示。第一个表示通过集成GNN层和注意力机制来侧重于去噪，而第二个则通过图变分自编码器获得，该编码器将特征分布与标准先验对齐。在训练过程中，我们采用互学习损失函数来逐步协调这些表示，使模型能够捕获共同模式并显著增强其泛化能力。在多个真实世界数据集上进行的广泛实验表明，我们的方法不仅提高了预测分数的数值精度——在RMSE上实现了高达0.8%的改进——而且在排名指标上提升高达36%，产生了更优越的排名。"
    },
    {
        "title": "Conversational Search: From Fundamentals to Frontiers in the LLM Era",
        "url": "http://arxiv.org/abs/2506.10635v1",
        "pub_date": "2025-06-12",
        "summary": "Conversational search enables multi-turn interactions between users and systems to fulfill users' complex information needs. During this interaction, the system should understand the users' search intent within the conversational context and then return the relevant information through a flexible, dialogue-based interface. The recent powerful large language models (LLMs) with capacities of instruction following, content generation, and reasoning, attract significant attention and advancements, providing new opportunities and challenges for building up intelligent conversational search systems. This tutorial aims to introduce the connection between fundamentals and the emerging topics revolutionized by LLMs in the context of conversational search. It is designed for students, researchers, and practitioners from both academia and industry. Participants will gain a comprehensive understanding of both the core principles and cutting-edge developments driven by LLMs in conversational search, equipping them with the knowledge needed to contribute to the development of next-generation conversational search systems.",
        "translated": "对话式搜索使得用户与系统之间能够进行多轮交互，以满足用户复杂的信息需求。在此交互过程中，系统应理解用户在对话上下文中的搜索意图，然后通过灵活的、基于对话的界面返回相关信息。\n\n近期强大的大型语言模型（LLMs），具备指令遵循、内容生成和推理的能力，吸引了广泛关注并取得了显著进展，为构建智能对话式搜索系统提供了新的机遇和挑战。本教程旨在介绍在对话式搜索领域中，基础知识与由LLMs带来变革的新兴主题之间的联系。\n\n本教程面向来自学术界和工业界的学生、研究人员和实践者。参与者将获得对对话式搜索中由LLMs驱动的核心原则和前沿发展的全面理解，从而掌握所需的知识，以助力下一代对话式搜索系统的发展。"
    },
    {
        "title": "Macro Graph of Experts for Billion-Scale Multi-Task Recommendation",
        "url": "http://arxiv.org/abs/2506.10520v1",
        "pub_date": "2025-06-12",
        "summary": "Graph-based multi-task learning at billion-scale presents a significant challenge, as different tasks correspond to distinct billion-scale graphs. Traditional multi-task learning methods often neglect these graph structures, relying solely on individual user and item embeddings. However, disregarding graph structures overlooks substantial potential for improving performance. In this paper, we introduce the Macro Graph of Expert (MGOE) framework, the first approach capable of leveraging macro graph embeddings to capture task-specific macro features while modeling the correlations between task-specific experts. Specifically, we propose the concept of a Macro Graph Bottom, which, for the first time, enables multi-task learning models to incorporate graph information effectively. We design the Macro Prediction Tower to dynamically integrate macro knowledge across tasks. MGOE has been deployed at scale, powering multi-task learning for the homepage of a leading billion-scale recommender system. Extensive offline experiments conducted on three public benchmark datasets demonstrate its superiority over state-of-the-art multi-task learning methods, establishing MGOE as a breakthrough in multi-task graph-based recommendation. Furthermore, online A/B tests confirm the superiority of MGOE in billion-scale recommender systems.",
        "translated": "在十亿级规模下进行基于图的多任务学习面临巨大挑战，因为不同的任务对应着各自独立的十亿级图。传统的的多任务学习方法常忽略这些图结构，仅依赖于独立的用户和物品嵌入。然而，忽视图结构会错失巨大的性能提升潜力。本文提出宏观专家图（Macro Graph of Expert, MGOE）框架，这是首个能够利用宏观图嵌入捕获任务特定宏观特征，并同时建模任务特定专家之间关联性的方法。具体而言，我们提出了宏观图底部（Macro Graph Bottom）的概念，这首次使多任务学习模型能够有效地融入图信息。我们设计了宏观预测塔（Macro Prediction Tower）来动态整合跨任务的宏观知识。MGOE已实现大规模部署，驱动着某领先十亿级推荐系统首页的多任务学习。在三个公开基准数据集上进行的广泛离线实验证明了MGOE优于最先进的多任务学习方法，使其成为基于图的多任务推荐领域的突破性进展。此外，在线A/B测试进一步证实了MGOE在十亿级推荐系统中的优越性。"
    },
    {
        "title": "Generative Representational Learning of Foundation Models for\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.11999v1",
        "pub_date": "2025-06-13",
        "summary": "Developing a single foundation model with the capability to excel across diverse tasks has been a long-standing objective in the field of artificial intelligence. As the wave of general-purpose foundation models sweeps across various domains, their influence has significantly extended to the field of recommendation systems. While recent efforts have explored recommendation foundation models for various generative tasks, they often overlook crucial embedding tasks and struggle with the complexities of multi-task learning, including knowledge sharing &amp; conflict resolution, and convergence speed inconsistencies. To address these limitations, we introduce RecFound, a generative representational learning framework for recommendation foundation models. We construct the first comprehensive dataset for recommendation foundation models covering both generative and embedding tasks across diverse scenarios. Based on this dataset, we propose a novel multi-task training scheme featuring a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge sharing &amp; conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched) to address inconsistent convergence, and a Model Merge module to balance the performance across tasks. Experiments demonstrate that RecFound achieves state-of-the-art performance across various recommendation tasks, outperforming existing baselines.",
        "translated": "在人工智能领域，开发一个能够胜任多样化任务的统一基础模型，一直是一个长期以来的目标。随着通用基础模型浪潮席卷各个领域，其影响力已深刻延伸至推荐系统领域。尽管近期研究已探索推荐基础模型在多种生成任务上的应用，但它们却往往忽视了关键的嵌入任务，并且在多任务学习的复杂性方面表现不足，例如知识共享与冲突消解、以及收敛速度不一致性等问题。\n\n为解决这些局限，我们提出了RecFound，一种用于推荐基础模型的生成式表征学习框架。我们构建了首个面向推荐基础模型的综合性数据集，该数据集同时涵盖了多种场景下的生成任务和嵌入任务。基于此数据集，我们提出了一种新颖的多任务训练方案，该方案包含一个任务级低秩专家混合（Task-wise Mixture of Low-rank Experts, TMoLE）模块以处理知识共享与冲突，一个分步式收敛导向采样调度器（Step-wise Convergence-oriented Sample Scheduler, S2Sched）以解决收敛不一致的问题，以及一个模型合并模块（Model Merge）以平衡跨任务的性能。实验结果表明，RecFound在各种推荐任务中均达到了最先进的水平，并优于现有基线。"
    },
    {
        "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
        "url": "http://arxiv.org/abs/2506.11763v1",
        "pub_date": "2025-06-13",
        "summary": "Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.",
        "translated": "深度研究智能体是基于大型语言模型（LLM）的智能体中的一个突出类别。通过自主编排多步骤网络探索、定向检索和高阶综合，它们能够将海量在线信息转化为分析师级别的、富含引用的报告——将数小时的人工桌面研究压缩至数分钟。然而，目前仍然缺乏一个能够系统性评估这些智能体能力的综合基准。\n\n为了弥补这一空白，我们提出了DeepResearch Bench，这是一个包含100个博士级研究任务的基准，每个任务均由22个不同领域的领域专家精心设计。评估深度研究智能体（DRA）本质上是复杂且劳动密集型的。因此，我们提出了两种新颖的方法，它们与人类判断具有高度一致性。第一种是基于参考的评估方法，采用自适应标准来评估生成研究报告的质量。另一个框架旨在通过评估其有效引用数量和整体引用准确性来评估DRA的信息检索和收集能力。我们已将DeepResearch Bench和这些框架的关键组件在https://github.com/Ayanami0730/deep_research_bench上开源，以加速实用的大型语言模型智能体的开发。"
    },
    {
        "title": "Forgetful by Design? A Critical Audit of YouTube's Search API for\n  Academic Research",
        "url": "http://arxiv.org/abs/2506.11727v1",
        "pub_date": "2025-06-13",
        "summary": "This paper critically audits the search endpoint of YouTube's Data API (v3), a common tool for academic research. Through systematic weekly searches over six months using eleven queries, we identify major limitations regarding completeness, representativeness, consistency, and bias. Our findings reveal substantial differences between ranking parameters like relevance and date in terms of video recall and precision, with relevance often retrieving numerous off-topic videos. We also find severe temporal decay, as the number of findable videos for a specific period dramatically decreases after just 20-60 days from the publication date, potentially hampering many different research designs. Furthermore, search results lack consistency, with identical queries yielding different video sets over time, compromising replicability. A case study on the European Parliament elections highlights how these issues impact research outcomes. While the paper offers several mitigation strategies, it concludes that the API's search function, potentially prioritizing \"freshness\" over comprehensive retrieval, is not adequate for robust academic research, especially concerning Digital Services Act requirements.",
        "translated": "本文批判性地审查了YouTube Data API (v3) 的搜索端点，该API是学术研究中常用的工具。通过系统地进行为期六个月的每周搜索，并使用十一个查询，我们发现了其在完整性、代表性、一致性和偏差方面的主要局限性。我们的研究结果揭示了相关性（relevance）和日期（date）等排序参数在视频召回率和精确率方面存在显著差异，其中相关性排序经常检索到大量偏离主题的视频。我们还发现严重的时效性衰减，即特定时期内可被找到的视频数量在发布日期后仅仅20-60天就急剧减少，这可能会阻碍许多不同的研究设计。此外，搜索结果缺乏一致性，相同的查询在不同时间会产生不同的视频集合，从而损害了研究的可复现性。一项关于欧洲议会选举的案例研究强调了这些问题如何影响研究成果。尽管本文提出了一些缓解策略，但它总结认为，该API的搜索功能（可能优先考虑“新鲜度”而非全面的检索）不足以支持严谨的学术研究，尤其是在满足《数字服务法案》要求方面。"
    },
    {
        "title": "TongSearch-QR: Reinforced Query Reasoning for Retrieval",
        "url": "http://arxiv.org/abs/2506.11603v1",
        "pub_date": "2025-06-13",
        "summary": "Traditional information retrieval (IR) methods excel at textual and semantic matching but struggle in reasoning-intensive retrieval tasks that require multi-hop inference or complex semantic understanding between queries and documents. One promising solution is to explicitly rewrite or augment queries using large language models (LLMs) to elicit reasoning-relevant content prior to retrieval. However, the widespread use of large-scale language models like GPT-4 or LLaMA3-70B remains impractical due to their high inference cost and limited deployability in real-world systems. In this work, we introduce TongSearch QR (Previously Known as \"TongSearch Reasoner\"), a family of small-scale language models for query reasoning and rewriting in reasoning-intensive retrieval. With a novel semi-rule-based reward function, we employ reinforcement learning approaches enabling smaller language models, e,g, Qwen2.5-7B-Instruct and Qwen2.5-1.5B-Instruct, to achieve query reasoning performance rivaling large-scale language models without their prohibitive inference costs. Experiment results on BRIGHT benchmark show that with BM25 as retrievers, both TongSearch QR-7B and TongSearch QR-1.5B models significantly outperform existing baselines, including prompt-based query reasoners and some latest dense retrievers trained for reasoning-intensive retrieval tasks, offering superior adaptability for real-world deployment.",
        "translated": "传统信息检索 (IR) 方法擅长文本和语义匹配，但在需要多跳推理或查询与文档之间复杂语义理解的推理密集型检索任务中表现不佳。一个有前景的解决方案是，在检索之前使用大型语言模型 (LLM) 显式地重写或增强查询，以引出与推理相关的内容。然而，由于像 GPT-4 或 LLaMA3-70B 这样的大型语言模型具有高昂的推理成本和在实际系统中有限的部署能力，它们的广泛应用仍然不切实际。\n\n在这项工作中，我们引入了 TongSearch QR（原名“TongSearch Reasoner”），这是一系列用于推理密集型检索中查询推理和重写的小型语言模型。借助一种新颖的半基于规则的奖励函数，我们采用了强化学习方法，使得像 Qwen2.5-7B-Instruct 和 Qwen2.5-1.5B-Instruct 这样的小型语言模型能够实现与大型语言模型相媲美的查询推理性能，而无需承担其高昂的推理成本。BRIGHT 基准测试上的实验结果表明，以 BM25 作为检索器时，TongSearch QR-7B 和 TongSearch QR-1.5B 模型均显著优于现有基线，包括基于提示的查询推理器以及一些为推理密集型检索任务训练的最新密集检索器，从而为实际部署提供了卓越的适应性。"
    },
    {
        "title": "GraphRAG-Causal: A novel graph-augmented framework for causal reasoning\n  and annotation in news",
        "url": "http://arxiv.org/abs/2506.11600v1",
        "pub_date": "2025-06-13",
        "summary": "GraphRAG-Causal introduces an innovative framework that combines graph-based retrieval with large language models to enhance causal reasoning in news analysis. Traditional NLP approaches often struggle with identifying complex, implicit causal links, especially in low-data scenarios. Our approach addresses these challenges by transforming annotated news headlines into structured causal knowledge graphs. It then employs a hybrid retrieval system that merges semantic embeddings with graph-based structural cues leveraging Neo4j to accurately match and retrieve relevant events. The framework is built on a three-stage pipeline: First, during Data Preparation, news sentences are meticulously annotated and converted into causal graphs capturing cause, effect, and trigger relationships. Next, the Graph Retrieval stage stores these graphs along with their embeddings in a Neo4j database and utilizes hybrid Cypher queries to efficiently identify events that share both semantic and structural similarities with a given query. Finally, the LLM Inference stage utilizes these retrieved causal graphs in a few-shot learning setup with XML-based prompting, enabling robust classification and tagging of causal relationships. Experimental evaluations demonstrate that GraphRAG-Causal achieves an impressive F1-score of 82.1% on causal classification using just 20 few-shot examples. This approach significantly boosts accuracy and consistency, making it highly suitable for real-time applications in news reliability assessment, misinformation detection, and policy analysis.",
        "translated": "GraphRAG-Causal 引入了一个创新的框架，该框架结合了图基检索与大型语言模型，旨在增强新闻分析中的因果推理能力。传统的自然语言处理（NLP）方法在识别复杂、隐式的因果关系方面常常面临挑战，特别是在数据稀缺的场景下。我们的方法通过将标注的新闻标题转化为结构化的因果知识图谱，解决了这些挑战。它随后采用了一种混合检索系统，该系统将语义嵌入与图基结构线索相结合，并利用 Neo4j 精确匹配和检索相关事件。\n\n该框架基于一个三阶段的流水线构建：\n\n1.  **数据准备**：新闻语句被精心标注并转换为因果图，捕获原因、结果和触发关系。\n2.  **图检索**：将这些图及其嵌入存储在 Neo4j 数据库中，并利用混合 Cypher 查询高效地识别与给定查询同时具有语义和结构相似性的事件。\n3.  **LLM 推理**：在基于 XML 提示的少样本学习设置中利用这些检索到的因果图，从而实现因果关系的鲁棒分类和标记。\n\n实验评估表明，GraphRAG-Causal 在仅使用 20 个少样本示例的情况下，在因果分类上取得了高达 82.1% 的 F1 分数。这种方法显著提升了准确性和一致性，使其非常适用于新闻可靠性评估、虚假信息检测和政策分析等实时应用。"
    },
    {
        "title": "Dual-View Disentangled Multi-Intent Learning for Enhanced Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2506.11538v1",
        "pub_date": "2025-06-13",
        "summary": "Disentangling user intentions from implicit feedback has become a promising strategy to enhance recommendation accuracy and interpretability. Prior methods often model intentions independently and lack explicit supervision, thus failing to capture the joint semantics that drive user-item interactions. To address these limitations, we propose DMICF, a unified framework that explicitly models interaction-level intent alignment while leveraging structural signals from both user and item perspectives. DMICF adopts a dual-view architecture that jointly encodes user-item interaction graphs from both sides, enabling bidirectional information fusion. This design enhances robustness under data sparsity by allowing the structural redundancy of one view to compensate for the limitations of the other. To model fine-grained user-item compatibility, DMICF introduces an intent interaction encoder that performs sub-intent alignment within each view, uncovering shared semantic structures that underlie user decisions. This localized alignment enables adaptive refinement of intent embeddings based on interaction context, thus improving the model's generalization and expressiveness, particularly in long-tail scenarios. Furthermore, DMICF integrates an intent-aware scoring mechanism that aggregates compatibility signals from matched intent pairs across user and item subspaces, enabling personalized prediction grounded in semantic congruence rather than entangled representations. To facilitate semantic disentanglement, we design a discriminative training signal via multi-negative sampling and softmax normalization, which pulls together semantically aligned intent pairs while pushing apart irrelevant or noisy ones. Extensive experiments demonstrate that DMICF consistently delivers robust performance across datasets with diverse interaction distributions.",
        "translated": "将用户意图从隐式反馈中解耦，已成为提升推荐准确性和可解释性的一种有前景的策略。以往的方法通常独立建模意图，且缺乏显式监督，因此未能捕捉驱动用户-物品交互的联合语义。为解决这些局限性，我们提出了DMICF，一个统一框架，它显式建模交互层面的意图对齐，同时利用用户和物品两个视角的结构信号。\n\nDMICF采用双视图架构，从用户和物品两侧共同编码用户-物品交互图，实现双向信息融合。这种设计通过允许一个视图的结构冗余来弥补另一个视图的局限性，从而增强了数据稀疏性下的鲁棒性。为了建模细粒度的用户-物品兼容性，DMICF引入了一个意图交互编码器，它在每个视图内部执行子意图对齐，从而揭示用户决策背后的共享语义结构。这种局部对齐能够基于交互上下文对意图嵌入进行自适应细化，进而提升模型的泛化能力和表达性，尤其在长尾场景中表现突出。\n\n此外，DMICF集成了意图感知评分机制，它聚合了用户和物品子空间中匹配意图对的兼容性信号，从而实现了基于语义一致性而非纠缠表示的个性化预测。为了促进语义解耦，我们通过多负采样和softmax归一化设计了一种判别性训练信号，该信号能拉近语义对齐的意图对，同时推开不相关或带有噪声的意图对。大量实验表明，DMICF在具有多样交互分布的数据集上始终展现出强大的鲁棒性能。"
    },
    {
        "title": "Leveraging Reference Documents for Zero-Shot Ranking via Large Language\n  Models",
        "url": "http://arxiv.org/abs/2506.11452v1",
        "pub_date": "2025-06-13",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in the task of text ranking for information retrieval. While Pointwise ranking approaches offer computational efficiency by scoring documents independently, they often yield biased relevance estimates due to the lack of inter-document comparisons. In contrast, Pairwise methods improve ranking accuracy by explicitly comparing document pairs, but suffer from substantial computational overhead with quadratic complexity ($O(n^2)$). To address this tradeoff, we propose \\textbf{RefRank}, a simple and effective comparative ranking method based on a fixed reference document. Instead of comparing all document pairs, RefRank prompts the LLM to evaluate each candidate relative to a shared reference anchor. By selecting the reference anchor that encapsulates the core query intent, RefRank implicitly captures relevance cues, enabling indirect comparison between documents via this common anchor. This reduces computational cost to linear time ($O(n)$) while importantly, preserving the advantages of comparative evaluation. To further enhance robustness, we aggregate multiple RefRank outputs using a weighted averaging scheme across different reference choices. Experiments on several benchmark datasets and with various LLMs show that RefRank significantly outperforms Pointwise baselines and could achieve performance at least on par with Pairwise approaches with a significantly lower computational cost.",
        "translated": "大型语言模型（LLM）在信息检索的文本排序任务中展现出卓越的性能。尽管逐点排序（Pointwise）方法通过独立地为文档评分来提供计算效率，但由于缺乏文档间比较，它们通常会产生有偏的相关性估计。相比之下，成对（Pairwise）方法通过显式比较文档对来提高排序准确性，但会带来巨大的计算开销，具有二次复杂度（$O(n^2)$）。为了解决这种权衡，我们提出了 \\textbf{RefRank}，一种基于固定参考文档的简单有效比较排序方法。RefRank 不再比较所有文档对，而是提示LLM评估每个候选文档相对于一个共享的参考锚点。通过选择能封装核心查询意图的参考锚点，RefRank 隐式地捕获相关性线索，从而通过这个共同锚点实现文档间的间接比较。这使得计算成本降低到线性时间（$O(n)$），同时重要的是，保留了比较评估的优势。为了进一步增强鲁棒性，我们采用加权平均方案，聚合了在不同参考选择下的多个 RefRank 输出。在多个基准数据集和各种LLM上的实验表明，RefRank 显著优于逐点基线方法，并且在计算成本显著降低的情况下，其性能至少与成对方法持平。"
    },
    {
        "title": "Deep Learning Model Acceleration and Optimization Strategies for\n  Real-Time Recommendation Systems",
        "url": "http://arxiv.org/abs/2506.11421v1",
        "pub_date": "2025-06-13",
        "summary": "With the rapid growth of Internet services, recommendation systems play a central role in delivering personalized content. Faced with massive user requests and complex model architectures, the key challenge for real-time recommendation systems is how to reduce inference latency and increase system throughput without sacrificing recommendation quality. This paper addresses the high computational cost and resource bottlenecks of deep learning models in real-time settings by proposing a combined set of modeling- and system-level acceleration and optimization strategies. At the model level, we dramatically reduce parameter counts and compute requirements through lightweight network design, structured pruning, and weight quantization. At the system level, we integrate multiple heterogeneous compute platforms and high-performance inference libraries, and we design elastic inference scheduling and load-balancing mechanisms based on real-time load characteristics. Experiments show that, while maintaining the original recommendation accuracy, our methods cut latency to less than 30% of the baseline and more than double system throughput, offering a practical solution for deploying large-scale online recommendation services.",
        "translated": "随着互联网服务的快速发展，推荐系统在提供个性化内容方面扮演着核心角色。面对海量用户请求和复杂的模型架构，实时推荐系统的关键挑战在于如何在不牺牲推荐质量的前提下，降低推理延迟并提高系统吞吐量。本文针对深度学习模型在实时场景中的高计算开销和资源瓶颈问题，提出了一套结合模型级和系统级的加速与优化策略。在模型层面，我们通过轻量级网络设计、结构化剪枝和权重量化等技术，显著减少了模型参数量和计算需求。在系统层面，我们整合了多个异构计算平台和高性能推理库，并基于实时负载特性设计了弹性推理调度和负载均衡机制。实验结果表明，在保持原有推荐精度的同时，我们的方法将延迟降低到基线的30%以下，并将系统吞吐量提升了一倍以上，为部署大规模在线推荐服务提供了一种实用的解决方案。"
    },
    {
        "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
        "url": "http://arxiv.org/abs/2506.12015v1",
        "pub_date": "2025-06-13",
        "summary": "Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.",
        "translated": "开源基础模型已得到快速采用和发展，在多样化领域实现了强大的通用能力。然而，由于微调大型基础模型所需的内存开销远超推理，对于大多数用户而言，将其微调用于领域特定或个性化任务仍然成本过高，令人望而却步。我们引入EMLoC（带有LoRA校正的基于模拟器的内存高效微调框架），它使得模型微调能够在与推理所需的相同内存预算内进行。\n\nEMLoC通过在小型下游校准集上应用激活感知奇异值分解（SVD），构建了一个任务特定的轻量级模拟器。随后，通过LoRA在该轻量级模拟器上执行微调。为解决原始模型与压缩模拟器之间的失配问题，我们提出了一种新颖的补偿算法来校正微调后的LoRA模块，从而可以将该模块合并到原始模型中进行推理。EMLoC支持灵活的压缩比和标准训练流程，使其能够适应广泛的应用。\n\n大量实验表明，EMLoC在多个数据集和模态上均优于其他基线方法。此外，在无需量化的前提下，EMLoC实现了在单一24GB消费级GPU上微调380亿参数模型，为个人用户带来了高效实用的模型适应。"
    },
    {
        "title": "Generative Representational Learning of Foundation Models for\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.11999v1",
        "pub_date": "2025-06-13",
        "summary": "Developing a single foundation model with the capability to excel across diverse tasks has been a long-standing objective in the field of artificial intelligence. As the wave of general-purpose foundation models sweeps across various domains, their influence has significantly extended to the field of recommendation systems. While recent efforts have explored recommendation foundation models for various generative tasks, they often overlook crucial embedding tasks and struggle with the complexities of multi-task learning, including knowledge sharing &amp; conflict resolution, and convergence speed inconsistencies. To address these limitations, we introduce RecFound, a generative representational learning framework for recommendation foundation models. We construct the first comprehensive dataset for recommendation foundation models covering both generative and embedding tasks across diverse scenarios. Based on this dataset, we propose a novel multi-task training scheme featuring a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge sharing &amp; conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched) to address inconsistent convergence, and a Model Merge module to balance the performance across tasks. Experiments demonstrate that RecFound achieves state-of-the-art performance across various recommendation tasks, outperforming existing baselines.",
        "translated": "人工智能领域的一个长期目标是开发能够胜任各种任务的单一基础模型。随着通用型基础模型浪潮席卷各个领域，它们的影响力已显著扩展到推荐系统领域。尽管最近的研究探索了用于各种生成任务的推荐基础模型，但它们常常忽视关键的嵌入任务，并且难以应对多任务学习的复杂性，包括知识共享与冲突解决，以及收敛速度不一致性。\n\n为了解决这些局限性，我们引入了RecFound，一个专为推荐基础模型设计的生成式表示学习框架。我们构建了首个综合性数据集，专用于推荐基础模型，涵盖了跨越不同场景的生成任务和嵌入任务。基于该数据集，我们提出了一种新颖的多任务训练方案，其特点包括：任务级低秩专家混合（TMoLE）以处理知识共享与冲突、逐步收敛导向型样本调度器（S2Sched）以解决不一致的收敛问题，以及一个模型合并模块以平衡各项任务的性能。实验表明，RecFound在各种推荐任务中均达到了最先进的性能，超越了现有基线。"
    },
    {
        "title": "LTRR: Learning To Rank Retrievers for LLMs",
        "url": "http://arxiv.org/abs/2506.13743v1",
        "pub_date": "2025-06-16",
        "summary": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed retriever, despite growing evidence that no single retriever performs optimally across all query types. In this paper, we explore a query routing approach that dynamically selects from a pool of retrievers based on the query, using both train-free heuristics and learned routing models. We frame routing as a learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to rank retrievers by their expected utility gain to downstream LLM performance. Our experiments, conducted on synthetic QA data with controlled query type variations, show that routing-based RAG systems can outperform the best single-retriever-based systems. Performance gains are especially pronounced in models trained with the Answer Correctness (AC) metric and with pairwise learning approaches, especially with XGBoost. We also observe improvements in generalization to out-of-distribution queries. As part of the SIGIR 2025 LiveRAG challenge, our submitted system demonstrated the practical viability of our approach, achieving competitive performance in both answer correctness and faithfulness. These findings highlight the importance of both training methodology and metric selection in query routing for RAG systems.",
        "translated": "检索增强生成（RAG）系统通常依赖于单个固定的检索器，尽管越来越多的证据表明没有哪个单一检索器能在所有查询类型上都达到最佳性能。在本文中，我们探索了一种查询路由方法，该方法能够根据查询，结合使用免训练启发式方法和学习型路由模型，动态地从检索器池中选择检索器。我们将路由问题构建为一个学习排序（LTR）问题，并引入了LTRR，这是一个学习根据检索器对下游大型语言模型（LLM）性能的预期效用增益对其进行排序的框架。\n\n我们的实验在具有受控查询类型变化的合成问答数据上进行，结果表明基于路由的RAG系统能够超越性能最佳的单一检索器系统。性能提升在使用答案正确性（AC）指标训练的模型以及采用成对学习方法时尤其显著，特别是使用XGBoost时。我们还观察到在对分布外查询的泛化能力方面有所改进。作为SIGIR 2025 LiveRAG挑战赛的一部分，我们提交的系统展示了我们方法的实际可行性，在答案正确性和忠实性方面均取得了有竞争力的性能。这些发现强调了训练方法和度量指标选择在RAG系统查询路由中的重要性。"
    },
    {
        "title": "OneRec Technical Report",
        "url": "http://arxiv.org/abs/2506.13695v1",
        "pub_date": "2025-06-16",
        "summary": "Recommender systems have been widely used in various large-scale user-oriented platforms for many years. However, compared to the rapid developments in the AI community, recommendation systems have not achieved a breakthrough in recent years. For instance, they still rely on a multi-stage cascaded architecture rather than an end-to-end approach, leading to computational fragmentation and optimization inconsistencies, and hindering the effective application of key breakthrough technologies from the AI community in recommendation scenarios.   To address these issues, we propose OneRec, which reshapes the recommendation system through an end-to-end generative approach and achieves promising results. Firstly, we have enhanced the computational FLOPs of the current recommendation model by 10 $\\times$ and have identified the scaling laws for recommendations within certain boundaries. Secondly, reinforcement learning techniques, previously difficult to apply for optimizing recommendations, show significant potential in this framework. Lastly, through infrastructure optimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU) on flagship GPUs during training and inference, respectively, aligning closely with the LLM community. This architecture significantly reduces communication and storage overhead, resulting in operating expense that is only 10.6% of traditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP, it handles 25% of total queries per second, enhancing overall App Stay Time by 0.54% and 1.24%, respectively. Additionally, we have observed significant increases in metrics such as 7-day Lifetime, which is a crucial indicator of recommendation experience. We also provide practical lessons and insights derived from developing, optimizing, and maintaining a production-scale recommendation system with significant real-world impact.",
        "translated": "推荐系统多年来已广泛应用于各种大规模面向用户的平台。然而，与人工智能领域的快速发展相比，推荐系统近年来并未取得突破性进展。例如，它们仍依赖于多阶段级联架构而非端到端方法，这导致了计算碎片化和优化不一致性，阻碍了人工智能领域关键突破性技术在推荐场景中的有效应用。\n\n为解决这些问题，我们提出了OneRec，它通过端到端生成式方法重塑了推荐系统，并取得了喜人的成果。首先，我们将当前推荐模型的计算浮点运算次数（FLOPs）提升了10倍，并在特定范围内发现了推荐的缩放定律。其次，此前难以应用于推荐优化的强化学习技术，在此框架中展现出巨大潜力。最后，通过基础设施优化，我们在旗舰级GPU上实现了训练和推理过程中23.7%和28.8%的模型浮点运算利用率（MFU），这与大语言模型（LLM）领域高度一致。该架构显著降低了通信和存储开销，使得运营支出（OpEx）仅为传统推荐流水线的10.6%。它已部署于快手/快手极速版APP，处理了总查询量（QPS）的25%，分别将总App停留时长提升了0.54%和1.24%。此外，我们观察到7日留存等指标也显著增长，这是衡量推荐体验的关键指标。我们还分享了在开发、优化和维护具有重大实际影响的生产规模推荐系统过程中获得的实践经验和见解。"
    },
    {
        "title": "Tree-Based Text Retrieval via Hierarchical Clustering in RAGFrameworks:\n  Application on Taiwanese Regulations",
        "url": "http://arxiv.org/abs/2506.13607v1",
        "pub_date": "2025-06-16",
        "summary": "Traditional Retrieval-Augmented Generation (RAG) systems employ brute-force inner product search to retrieve the top-k most similar documents, then combined with the user query and passed to a language model. This allows the model to access external knowledge and reduce hallucinations. However, selecting an appropriate k value remains a significant challenge in practical applications: a small k may fail to retrieve sufficient information, while a large k can introduce excessive and irrelevant content. To address this, we propose a hierarchical clustering-based retrieval method that eliminates the need to predefine k. Our approach maintains the accuracy and relevance of system responses while adaptively selecting semantically relevant content. In the experiment stage, we applied our method to a Taiwanese legal dataset with expert-graded queries. The results show that our approach achieves superior performance in expert evaluations and maintains high precision while eliminating the need to predefine k, demonstrating improved accuracy and interpretability in legal text retrieval tasks. Our framework is simple to implement and easily integrates with existing RAG pipelines, making it a practical solution for real-world applications under limited resources.",
        "translated": "传统的检索增强生成（RAG）系统采用暴力内积搜索来检索最相似的top-k个文档，随后将这些文档与用户查询结合并输入给语言模型。这使得模型能够访问外部知识并减少幻觉（hallucinations）。然而，在实际应用中，选择一个合适的k值仍然是一个重大挑战：k值过小可能无法检索到足够的信息，而k值过大则可能引入过多无关内容。为解决此问题，我们提出了一种基于层次聚类的检索方法，该方法无需预定义k值。我们的方法在保持系统响应的准确性和相关性的同时，自适应地选择语义相关内容。\n\n在实验阶段，我们将该方法应用于一个包含专家评级查询的台湾法律数据集。结果表明，我们的方法在专家评估中表现出卓越的性能，并在无需预定义k值的情况下保持了高精度，从而证明了其在法律文本检索任务中提高了准确性和可解释性。我们的框架易于实现，并且可以轻松集成到现有的RAG管道中，使其成为在有限资源下实际应用的实用解决方案。"
    },
    {
        "title": "Hierarchical Multi-Positive Contrastive Learning for Patent Image\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.13496v1",
        "pub_date": "2025-06-16",
        "summary": "Patent images are technical drawings that convey information about a patent's innovation. Patent image retrieval systems aim to search in vast collections and retrieve the most relevant images. Despite recent advances in information retrieval, patent images still pose significant challenges due to their technical intricacies and complex semantic information, requiring efficient fine-tuning for domain adaptation. Current methods neglect patents' hierarchical relationships, such as those defined by the Locarno International Classification (LIC) system, which groups broad categories (e.g., \"furnishing\") into subclasses (e.g., \"seats\" and \"beds\") and further into specific patent designs. In this work, we introduce a hierarchical multi-positive contrastive loss that leverages the LIC's taxonomy to induce such relations in the retrieval process. Our approach assigns multiple positive pairs to each patent image within a batch, with varying similarity scores based on the hierarchical taxonomy. Our experimental analysis with various vision and multimodal models on the DeepPatent2 dataset shows that the proposed method enhances the retrieval results. Notably, our method is effective with low-parameter models, which require fewer computational resources and can be deployed on environments with limited hardware.",
        "translated": "专利图像是技术图纸，用于传达专利创新的信息。专利图像检索系统旨在海量图像集合中搜索并检索出最相关的图像。尽管信息检索领域取得了最新进展，但专利图像因其技术复杂性和复杂的语义信息，仍然带来了巨大挑战，需要高效的领域适应性微调。\n\n现有方法忽略了专利的层级关系，例如由洛迦诺国际分类（LIC）系统定义的层级关系，该系统将大类（例如“家具”）分为子类（例如“座椅”和“床”），并进一步细分为具体的专利设计。在这项工作中，我们引入了一种层级多正例对比损失（hierarchical multi-positive contrastive loss），它利用LIC的分类体系在检索过程中引入此类关系。我们的方法在一个批次内为每个专利图像分配多个正例对，并根据层级分类体系赋予不同的相似度分数。\n\n我们在DeepPatent2数据集上使用各种视觉和多模态模型进行的实验分析表明，所提出的方法显著提升了检索结果。值得注意的是，我们的方法对低参数模型同样有效，这使得它所需的计算资源更少，并能够部署在硬件资源有限的环境中。"
    },
    {
        "title": "Beyond One-Size-Fits-All: A Study of Neural and Behavioural Variability\n  Across Different Recommendation Categories",
        "url": "http://arxiv.org/abs/2506.13409v1",
        "pub_date": "2025-06-16",
        "summary": "Traditionally, Recommender Systems (RS) have primarily measured performance based on the accuracy and relevance of their recommendations. However, this algorithmic-centric approach overlooks how different types of recommendations impact user engagement and shape the overall quality of experience. In this paper, we shift the focus to the user and address for the first time the challenge of decoding the neural and behavioural variability across distinct recommendation categories, considering more than just relevance. Specifically, we conducted a controlled study using a comprehensive e-commerce dataset containing various recommendation types, and collected Electroencephalography and behavioural data. We analysed both neural and behavioural responses to recommendations that were categorised as Exact, Substitute, Complement, or Irrelevant products within search query results. Our findings offer novel insights into user preferences and decision-making processes, revealing meaningful relationships between behavioural and neural patterns for each category, but also indicate inter-subject variability.",
        "translated": "传统上，推荐系统（RS）主要根据推荐的准确性和相关性来衡量其性能。然而，这种以算法为中心的方法忽视了不同类型的推荐如何影响用户参与度并塑造整体用户体验质量。在本文中，我们将重点转向用户，并首次解决了在考虑相关性之外，解码不同推荐类别中神经和行为变异性的挑战。具体而言，我们利用一个包含各种推荐类型的综合电商数据集，进行了一项对照研究，并收集了脑电图（Electroencephalography）和行为数据。我们分析了在搜索查询结果中被归类为精确（Exact）、替代（Substitute）、互补（Complement）或不相关（Irrelevant）产品的推荐所引起的神经和行为反应。我们的研究结果为用户偏好和决策过程提供了新颖的见解，揭示了每种类别中行为和神经模式之间有意义的关系，但也表明了主体间的变异性。"
    },
    {
        "title": "Decompositional Reasoning for Graph Retrieval with Large Language Models",
        "url": "http://arxiv.org/abs/2506.13380v1",
        "pub_date": "2025-06-16",
        "summary": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with multi-hop reasoning and factual consistency, limiting their effectiveness on knowledge-intensive tasks like complex question answering (QA). Linking Knowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally lack the ability to reason efficiently over graph-structured information. To tackle this problem, we propose a novel retrieval approach that integrates textual knowledge graphs into the LLM reasoning process via query decomposition. Our method decomposes complex questions into sub-questions, retrieves relevant textual subgraphs, and composes a question-specific knowledge graph to guide answer generation. For that, we use a weighted similarity function that focuses on both the complex question and the generated subquestions to extract a relevant subgraph, which allows efficient and precise retrieval for complex questions and improves the performance of LLMs on multi-hop QA tasks. This structured reasoning pipeline enhances factual grounding and interpretability while leveraging the generative strengths of LLMs. We evaluate our method on standard multi-hop QA benchmarks and show that it achieves comparable or superior performance to competitive existing methods, using smaller models and fewer LLM calls.",
        "translated": "**大型语言模型（LLMs）** 在众多自然语言处理（NLP）任务中表现出色，但在**多跳推理**和**事实一致性**方面存在困难，从而限制了它们在复杂问答（QA）等**知识密集型任务**上的有效性。将**知识图谱（KG）** 与LLMs结合已展现出有前景的结果，但LLMs通常缺乏对图结构信息进行高效推理的能力。\n\n为解决此问题，我们提出了一种新颖的**检索方法**，通过**查询分解**将文本知识图谱整合到LLM的推理过程中。我们的方法将复杂问题分解为**子问题**，检索相关的**文本子图**，并构建一个与问题相关的知识图谱以指导**答案生成**。为此，我们使用一个**加权相似性函数**，该函数同时关注复杂问题和生成的子问题，以提取相关子图，从而实现了复杂问题的高效精确检索，并提高了LLMs在多跳问答（QA）任务上的性能。这种**结构化推理流程**增强了**事实基础**和**可解释性**，同时充分利用了LLMs的生成能力。我们在标准多跳问答（QA）基准上评估了所提出的方法，结果表明，与现有有竞争力的SOTA方法相比，该方法实现了可比或更优的性能，且使用的模型更小，LLM调用次数更少。"
    },
    {
        "title": "Gated Rotary-Enhanced Linear Attention for Long-term Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.13315v1",
        "pub_date": "2025-06-16",
        "summary": "In Sequential Recommendation Systems (SRSs), Transformer models show remarkable performance but face computation cost challenges when modeling long-term user behavior sequences due to the quadratic complexity of the dot-product attention mechanism. By approximating the dot-product attention, linear attention provides an efficient option with linear complexity. However, existing linear attention methods face two limitations: 1) they often use learnable position encodings, which incur extra computational costs in long-term sequence scenarios, and 2) they may not consider the user's fine-grained local preferences and confuse these with the actual change of long-term interests. To remedy these drawbacks, we propose a long-term sequential Recommendation model with Gated Rotary Enhanced Linear Attention (RecGRELA). Specifically, we first propose a Rotary-Enhanced Linear Attention (RELA) module to model long-range dependency within the user's historical information using rotary position encodings. We then introduce a local short operation to incorporate local preferences and demonstrate the theoretical insight. We further introduce a SiLU-based Gated mechanism for RELA (GRELA) to help the model determine whether a user's behavior indicates local interest or a genuine shift in long-term preferences. Experimental results on four public datasets demonstrate that our RecGRELA achieves state-of-the-art performance compared to existing SRSs while maintaining low memory overhead.",
        "translated": "在序列推荐系统（SRSs）中，Transformer模型展现出卓越性能，但在建模长期用户行为序列时面临计算成本挑战，这源于点积注意力机制的二次复杂度。通过近似点积注意力，线性注意力提供了一种具有线性复杂度的高效选择。然而，现有的线性注意力方法面临两个局限性：1) 它们通常使用可学习的位置编码，这在长期序列场景中会带来额外的计算成本；2) 它们可能未考虑用户的细粒度局部偏好，并将其与长期兴趣的实际变化混淆。\n\n为了弥补这些不足，我们提出了一种带有门控旋转增强线性注意力的长期序列推荐模型（RecGRELA）。具体而言，我们首先提出了一个旋转增强线性注意力（RELA）模块，该模块使用旋转位置编码来建模用户历史信息中的长距离依赖关系。接着，我们引入了一个局部短操作来纳入局部偏好，并阐明了其理论洞察。此外，我们还为RELA引入了一个基于SiLU的门控机制（GRELA），以帮助模型判断用户的行为是表明局部兴趣还是长期偏好的真正转变。在四个公共数据集上的实验结果表明，与现有SRSs相比，我们的RecGRELA实现了最先进的性能，同时保持了低内存开销。"
    },
    {
        "title": "Vector Ontologies as an LLM world view extraction method",
        "url": "http://arxiv.org/abs/2506.13252v1",
        "pub_date": "2025-06-16",
        "summary": "Large Language Models (LLMs) possess intricate internal representations of the world, yet these latent structures are notoriously difficult to interpret or repurpose beyond the original prediction task. Building on our earlier work (Rothenfusser, 2025), which introduced the concept of vector ontologies as a framework for translating high-dimensional neural representations into interpretable geometric structures, this paper provides the first empirical validation of that approach. A vector ontology defines a domain-specific vector space spanned by ontologically meaningful dimensions, allowing geometric analysis of concepts and relationships within a domain. We construct an 8-dimensional vector ontology of musical genres based on Spotify audio features and test whether an LLM's internal world model of music can be consistently and accurately projected into this space. Using GPT-4o-mini, we extract genre representations through multiple natural language prompts and analyze the consistency of these projections across linguistic variations and their alignment with ground-truth data. Our results show (1) high spatial consistency of genre projections across 47 query formulations, (2) strong alignment between LLM-inferred genre locations and real-world audio feature distributions, and (3) evidence of a direct relationship between prompt phrasing and spatial shifts in the LLM's inferred vector ontology. These findings demonstrate that LLMs internalize structured, repurposable knowledge and that vector ontologies offer a promising method for extracting and analyzing this knowledge in a transparent and verifiable way.",
        "translated": "大语言模型（LLM）拥有复杂精密的内部世界表征，然而，这些潜在结构在原始预测任务之外却难以解释或复用。本文基于我们之前的研究（Rothenfusser, 2025），该研究引入了向量本体论（vector ontologies）的概念，将其作为一种将高维神经表征转换为可解释几何结构的框架。在此基础上，本文首次提供了对该方法的实证验证。向量本体论定义了一个领域特定的向量空间，该空间由本体论意义上的维度所张成，从而能够对领域内的概念和关系进行几何分析。\n\n我们基于Spotify音频特征构建了一个8维的音乐流派向量本体论，并测试了大语言模型的内部音乐世界模型是否能被一致且准确地投影到这个空间中。我们使用GPT-4o-mini，通过多个自然语言提示提取流派表征，并分析这些投影在语言变体之间的一致性及其与真实数据的对齐情况。我们的结果表明：（1）流派投影在47种查询表述中表现出高度的空间一致性；（2）大语言模型推断的流派位置与真实世界的音频特征分布之间存在很强的对齐；以及（3）提示措辞与大语言模型推断的向量本体论中空间偏移之间存在直接关系的证据。这些发现表明，大语言模型内化了结构化、可复用的知识，并且向量本体论为以透明和可验证的方式提取和分析这些知识提供了一种有前景的方法。"
    },
    {
        "title": "PB$^2$: Preference Space Exploration via Population-Based Methods in\n  Preference-Based Reinforcement Learning",
        "url": "http://arxiv.org/abs/2506.13741v1",
        "pub_date": "2025-06-16",
        "summary": "Preference-based reinforcement learning (PbRL) has emerged as a promising approach for learning behaviors from human feedback without predefined reward functions. However, current PbRL methods face a critical challenge in effectively exploring the preference space, often converging prematurely to suboptimal policies that satisfy only a narrow subset of human preferences. In this work, we identify and address this preference exploration problem through population-based methods. We demonstrate that maintaining a diverse population of agents enables more comprehensive exploration of the preference landscape compared to single-agent approaches. Crucially, this diversity improves reward model learning by generating preference queries with clearly distinguishable behaviors, a key factor in real-world scenarios where humans must easily differentiate between options to provide meaningful feedback. Our experiments reveal that current methods may fail by getting stuck in local optima, requiring excessive feedback, or degrading significantly when human evaluators make errors on similar trajectories, a realistic scenario often overlooked by methods relying on perfect oracle teachers. Our population-based approach demonstrates robust performance when teachers mislabel similar trajectory segments and shows significantly enhanced preference exploration capabilities,particularly in environments with complex reward landscapes.",
        "translated": "Preference-based reinforcement learning (PbRL) has emerged as a promising approach for learning behaviors from human feedback without predefined reward functions. However, current PbRL methods face a critical challenge in effectively exploring the preference space, often converging prematurely to suboptimal policies that satisfy only a narrow subset of human preferences. In this work, we identify and address this preference exploration problem through population-based methods. We demonstrate that maintaining a diverse population of agents enables more comprehensive exploration of the preference landscape compared to single-agent approaches. Crucially, this diversity improves reward model learning by generating preference queries with clearly distinguishable behaviors, a key factor in real-world scenarios where humans must easily differentiate between options to provide meaningful feedback. Our experiments reveal that current methods may fail by getting stuck in local optima, requiring excessive feedback, or degrading significantly when human evaluators make errors on similar trajectories, a realistic scenario often overlooked by methods relying on perfect oracle teachers. Our population-based approach demonstrates robust performance when teachers mislabel similar trajectory segments and shows significantly enhanced preference exploration capabilities, particularly in environments with complex reward landscapes.\n\n---\n\n**中文翻译：**\n\n基于偏好的强化学习（PbRL）已成为一种有前景的方法，用于在没有预定义奖励函数的情况下从人类反馈中学习行为。然而，当前的PbRL方法在有效探索偏好空间方面面临一个严峻挑战，它们经常过早地收敛到次优策略，这些策略仅能满足人类偏好的狭隘子集。在本工作中，我们通过基于种群的方法识别并解决了这一偏好探索问题。我们证明，与单智能体方法相比，维护多样化的智能体种群能够实现对偏好景观更全面的探索。至关重要的是，这种多样性通过生成具有易于区分行为的偏好查询来改善奖励模型学习，这在人类必须轻松区分不同选项才能提供有意义反馈的现实场景中是一个关键因素。我们的实验表明，当前方法可能会陷入局部最优、需要过多反馈，或者当人类评估者对相似轨迹进行误判时性能显著下降。这种情况是一个现实场景，但往往被那些依赖完美预言机教师的方法所忽视。我们的基于种群的方法在教师误标记相似轨迹片段时表现出鲁棒的性能，并显示出显著增强的偏好探索能力，尤其是在奖励景观复杂的环境中。"
    },
    {
        "title": "BanditWare: A Contextual Bandit-based Framework for Hardware Prediction",
        "url": "http://arxiv.org/abs/2506.13730v1",
        "pub_date": "2025-06-16",
        "summary": "Distributed computing systems are essential for meeting the demands of modern applications, yet transitioning from single-system to distributed environments presents significant challenges. Misallocating resources in shared systems can lead to resource contention, system instability, degraded performance, priority inversion, inefficient utilization, increased latency, and environmental impact.   We present BanditWare, an online recommendation system that dynamically selects the most suitable hardware for applications using a contextual multi-armed bandit algorithm. BanditWare balances exploration and exploitation, gradually refining its hardware recommendations based on observed application performance while continuing to explore potentially better options. Unlike traditional statistical and machine learning approaches that rely heavily on large historical datasets, BanditWare operates online, learning and adapting in real-time as new workloads arrive.   We evaluated BanditWare on three workflow applications: Cycles (an agricultural science scientific workflow) BurnPro3D (a web-based platform for fire science) and a matrix multiplication application. Designed for seamless integration with the National Data Platform (NDP), BanditWare enables users of all experience levels to optimize resource allocation efficiently.",
        "translated": "分布式计算系统对于满足现代应用的需求至关重要，然而，从单系统环境向分布式环境的转变带来了巨大的挑战。在共享系统中资源分配不当可能导致资源争用、系统不稳定、性能下降、优先级反转、资源利用效率低下、延迟增加以及环境影响。\n\n我们提出了 BanditWare，一个在线推荐系统，它采用上下文多臂老虎机算法，为应用动态选择最合适的硬件。BanditWare 平衡了探索与利用，基于观察到的应用性能逐步优化其硬件推荐，同时继续探索潜在更优的选择。与严重依赖大量历史数据集的传统统计和机器学习方法不同，BanditWare 在线运行，随着新工作负载的到来，实时学习和适应。\n\n我们在三个工作流应用上评估了 BanditWare：Cycles（一个农业科学科研工作流）、BurnPro3D（一个基于网络的火灾科学平台）以及一个矩阵乘法应用。BanditWare 旨在与国家数据平台（NDP）无缝集成，使各种经验水平的用户能够高效优化资源分配。"
    },
    {
        "title": "LTRR: Learning To Rank Retrievers for LLMs",
        "url": "http://arxiv.org/abs/2506.13743v1",
        "pub_date": "2025-06-16",
        "summary": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed retriever, despite growing evidence that no single retriever performs optimally across all query types. In this paper, we explore a query routing approach that dynamically selects from a pool of retrievers based on the query, using both train-free heuristics and learned routing models. We frame routing as a learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to rank retrievers by their expected utility gain to downstream LLM performance. Our experiments, conducted on synthetic QA data with controlled query type variations, show that routing-based RAG systems can outperform the best single-retriever-based systems. Performance gains are especially pronounced in models trained with the Answer Correctness (AC) metric and with pairwise learning approaches, especially with XGBoost. We also observe improvements in generalization to out-of-distribution queries. As part of the SIGIR 2025 LiveRAG challenge, our submitted system demonstrated the practical viability of our approach, achieving competitive performance in both answer correctness and faithfulness. These findings highlight the importance of both training methodology and metric selection in query routing for RAG systems.",
        "translated": "检索增强生成 (RAG) 系统通常依赖于单个固定检索器，尽管越来越多的证据表明，没有一个检索器能够在所有查询类型上都达到最佳性能。在本文中，我们探索了一种查询路由方法，该方法利用免训练启发式方法和学习型路由模型，根据查询动态地从检索器池中选择检索器。我们将路由框定为一个排序学习 (LTR) 问题，并引入了 LTRR 框架，该框架学习根据检索器对下游大型语言模型 (LLM) 性能的预期效用增益来对其进行排序。\n\n我们的实验在具有受控查询类型变体的合成问答数据上进行，结果表明基于路由的 RAG 系统能够超越表现最佳的单检索器系统。性能提升在使用答案正确性 (AC) 指标和成对学习方法（特别是 XGBoost）训练的模型中尤为显著。我们还观察到系统对分布外 (OOD) 查询的泛化能力有所提高。作为 SIGIR 2025 LiveRAG 挑战的一部分，我们提交的系统展示了所提方法的实际可行性，在答案正确性和忠实性方面均取得了有竞争力的性能。这些发现强调了训练方法和指标选择在 RAG 系统查询路由中的重要性。"
    },
    {
        "title": "A Systematic Replicability and Comparative Study of BSARec and SASRec\n  for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2506.14692v1",
        "pub_date": "2025-06-17",
        "summary": "This study aims at comparing two sequential recommender systems: Self-Attention based Sequential Recommendation (SASRec), and Beyond Self-Attention based Sequential Recommendation (BSARec) in order to check the improvement frequency enhancement - the added element in BSARec - has on recommendations. The models in the study, have been re-implemented with a common base-structure from EasyRec, with the aim of obtaining a fair and reproducible comparison. The results obtained displayed how BSARec, by including bias terms for frequency enhancement, does indeed outperform SASRec, although the increases in performance obtained, are not as high as those presented by the authors. This work aims at offering an overview on existing methods, and most importantly at underlying the importance of implementation details for performance comparison.",
        "translated": "本研究旨在比较两种序列推荐系统：基于自注意力机制的序列推荐（SASRec）和超越自注意力机制的序列推荐（BSARec），以验证频率增强（BSARec中新增的元素）对推荐效果的提升作用。研究中的模型基于EasyRec的通用基础架构进行了重新实现，旨在实现公平且可复现的比较。所获得的结果表明，BSARec通过引入用于频率增强的偏置项，性能确实优于SASRec，尽管所实现的性能提升并未达到原作者所报告的水平。本研究旨在对现有方法进行概述，更重要的是强调实现细节对于性能比较的重要性。"
    },
    {
        "title": "Refining music sample identification with a self-supervised graph neural\n  network",
        "url": "http://arxiv.org/abs/2506.14684v1",
        "pub_date": "2025-06-17",
        "summary": "Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.   In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.   To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.",
        "translated": "自动采样识别（ASID），即检测并识别在新音乐作品中被重用的音频片段，是音频查询检索领域一项重要但充满挑战的任务。尽管一项相关任务——音频指纹识别，已在“真实世界”（例如噪声、混响）条件下准确检索音乐内容方面取得了显著进展，但ASID系统在识别经过音乐修改的采样时仍面临困难。因此，开发一个能够抵抗常见音乐制作变换（如时间拉伸、音高转换、效果处理以及底层或叠加音乐）的鲁棒系统，是一个重要的开放性挑战。\n\n在这项工作中，我们提出了一种轻量级且可扩展的编码架构，它在对比学习框架内采用了图神经网络（GNN）。与当前最先进的系统相比，我们的模型仅使用 9% 的可训练参数，同时实现了可比的性能，达到了 44.2% 的平均精度（mAP）。\n\n为了提高检索质量，我们引入了一种两阶段方法：首先进行初步的粗粒度相似性搜索以进行候选选择，随后是一个交叉注意力分类器，用于拒绝不相关的匹配并优化检索到候选的排序——这是现有模型所缺乏的关键能力。此外，由于现实世界应用中的查询通常持续时间较短，我们使用为 Sample100 数据集新增的细粒度标注，对系统进行了短查询的基准测试，这些标注也作为我们工作的一部分公开发布。"
    },
    {
        "title": "RMIT-ADM+S at the SIGIR 2025 LiveRAG Challenge",
        "url": "http://arxiv.org/abs/2506.14516v1",
        "pub_date": "2025-06-17",
        "summary": "This paper presents the RMIT--ADM+S participation in the SIGIR 2025 LiveRAG Challenge. Our Generation-Retrieval-Augmented Generation (GRAG) approach relies on generating a hypothetical answer that is used in the retrieval phase, alongside the original question. GRAG also incorporates a pointwise large language model (LLM)-based re-ranking step prior to final answer generation. We describe the system architecture and the rationale behind our design choices. In particular, a systematic evaluation using the Grid of Points (GoP) framework and N-way ANOVA enabled comparison across multiple configurations, including query variant generation, question decomposition, rank fusion strategies, and prompting techniques for answer generation. Our system achieved a Relevance score of 1.199 and a Faithfulness score of 0.477 on the private leaderboard, placing among the top four finalists in the LiveRAG 2025 Challenge.",
        "translated": "以下是论文摘要的中文翻译：\n\n本文介绍了 RMIT-ADM+S 团队在 SIGIR 2025 LiveRAG 挑战赛中的参与情况。我们提出的生成-检索增强生成（GRAG）方法，基于生成一个假设性答案，该答案与原始问题一同用于检索阶段。GRAG 还结合了一个基于逐点式大语言模型（LLM）的重排序步骤，用于最终答案生成之前。我们描述了系统架构以及我们设计选择背后的原理。特别是，一项利用网格点（GoP）框架和 N 向方差分析的系统性评估，实现了多种配置间的比较，包括查询变体生成、问题分解、排序融合策略以及答案生成的提示技术。我们的系统在私人排行榜上取得了 1.199 的相关性得分和 0.477 的忠实性得分，在 LiveRAG 2025 挑战赛中跻身前四名决赛队伍。"
    },
    {
        "title": "Vela: Scalable Embeddings with Voice Large Language Models for\n  Multimodal Retrieval",
        "url": "http://arxiv.org/abs/2506.14445v1",
        "pub_date": "2025-06-17",
        "summary": "Multimodal large language models (MLLMs) have seen substantial progress in recent years. However, their ability to represent multimodal information in the acoustic domain remains underexplored. In this work, we introduce Vela, a novel framework designed to adapt MLLMs for the generation of universal multimodal embeddings. By leveraging MLLMs with specially crafted prompts and selected in-context learning examples, Vela effectively bridges the modality gap across various modalities. We then propose a single-modality training approach, where the model is trained exclusively on text pairs. Our experiments show that Vela outperforms traditional CLAP models in standard text-audio retrieval tasks. Furthermore, we introduce new benchmarks that expose CLAP models' limitations in handling long texts and complex retrieval tasks. In contrast, Vela, by harnessing the capabilities of MLLMs, demonstrates robust performance in these scenarios. Our code will soon be available.",
        "translated": "多模态大型语言模型（MLLMs）近年来取得了显著进展。然而，它们在音频领域表示多模态信息的能力仍未得到充分探索。本工作引入了Vela，一个新颖的框架，旨在通过适应多模态大型语言模型（MLLMs）来生成通用多模态嵌入。Vela通过利用MLLM，并结合精心设计的提示（prompts）和选定的上下文学习（in-context learning）示例，有效地弥合了不同模态间的模态鸿沟。接着，我们提出了一种单模态训练方法，其中模型仅通过文本对进行训练。我们的实验表明，在标准文本-音频检索任务中，Vela的表现优于传统的CLAP模型。此外，我们还引入了新的基准，揭示了CLAP模型在处理长文本和复杂检索任务时的局限性。相比之下，Vela凭借MLLM的强大能力，在这些场景中展现出稳健的性能。我们的代码即将开源。"
    },
    {
        "title": "Similarity = Value? Consultation Value Assessment and Alignment for\n  Personalized Search",
        "url": "http://arxiv.org/abs/2506.14437v1",
        "pub_date": "2025-06-17",
        "summary": "Personalized search systems in e-commerce platforms increasingly involve user interactions with AI assistants, where users consult about products, usage scenarios, and more. Leveraging consultation to personalize search services is trending. Existing methods typically rely on semantic similarity to align historical consultations with current queries due to the absence of 'value' labels, but we observe that semantic similarity alone often fails to capture the true value of consultation for personalization. To address this, we propose a consultation value assessment framework that evaluates historical consultations from three novel perspectives: (1) Scenario Scope Value, (2) Posterior Action Value, and (3) Time Decay Value. Based on this, we introduce VAPS, a value-aware personalized search model that selectively incorporates high-value consultations through a consultation-user action interaction module and an explicit objective that aligns consultations with user actions. Experiments on both public and commercial datasets show that VAPS consistently outperforms baselines in both retrieval and ranking tasks.",
        "translated": "电商平台中的个性化搜索系统越来越多地涉及用户与AI助手的交互，用户在其中咨询产品、使用场景等。利用咨询内容来个性化搜索服务正呈现上升趋势。现有方法由于缺乏“价值”标签，通常依赖语义相似性将历史咨询与当前查询对齐。但我们观察到，仅凭语义相似性往往无法捕获咨询内容对于个性化搜索的真正价值。为解决这一问题，我们提出了一个咨询价值评估框架，从三个新颖的视角评估历史咨询内容：(1) 场景范围价值，(2) 后续行动价值，和 (3) 时间衰减价值。在此基础上，我们引入了VAPS，一个价值感知个性化搜索模型。VAPS通过一个咨询-用户行为交互模块以及一个将咨询内容与用户行为对齐的明确目标，选择性地整合高价值咨询。在公开和商业数据集上的实验表明，VAPS在检索和排序任务中均始终优于基线模型。"
    },
    {
        "title": "RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG\n  Systems for the SIGIR LiveRAG Competition",
        "url": "http://arxiv.org/abs/2506.14412v1",
        "pub_date": "2025-06-17",
        "summary": "Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR 2025 LiveRAG Challenge.",
        "translated": "检索增强生成 (RAG) 通过结合大型语言模型 (LLMs) 内部的参数化知识与外部的非参数化来源，旨在提高事实准确性并最大程度地减少幻觉。LiveRAG 2025 挑战赛旨在探索 RAG 解决方案，以最大限度地提高在 DataMorgana 问答对上的准确性，这些问答对由单跳和多跳问题组成。该挑战赛提供了 Fineweb 10BT 数据集的稀疏 OpenSearch 索引和稠密 Pinecone 索引的访问权限。它限制模型使用参数量不超过 100 亿的 LLM，并且最终答案的生成必须使用 Falcon-3-10B 模型。提交的答案由一个判官 LLM 和人类评估者共同评估。通过在挑战赛条件下探索不同的检索器组合和 RAG 解决方案，我们最终的解决方案是采用了 InstructRAG，并结合了 Pinecone 检索器和 BGE 重排序器。我们的解决方案取得了 1.13 的正确性分数和 0.55 的忠实度分数，最终在 SIGIR 2025 LiveRAG 挑战赛中位列第四。"
    },
    {
        "title": "hyperFA*IR: A hypergeometric approach to fair rankings with finite\n  candidate pool",
        "url": "http://arxiv.org/abs/2506.14349v1",
        "pub_date": "2025-06-17",
        "summary": "Ranking algorithms play a pivotal role in decision-making processes across diverse domains, from search engines to job applications. When rankings directly impact individuals, ensuring fairness becomes essential, particularly for groups that are marginalised or misrepresented in the data. Most of the existing group fairness frameworks often rely on ensuring proportional representation of protected groups. However, these approaches face limitations in accounting for the stochastic nature of ranking processes or the finite size of candidate pools. To this end, we present hyperFA*IR, a framework for assessing and enforcing fairness in rankings drawn from a finite set of candidates. It relies on a generative process based on the hypergeometric distribution, which models real-world scenarios by sampling without replacement from fixed group sizes. This approach improves fairness assessment when top-$k$ selections are large relative to the pool or when protected groups are small. We compare our approach to the widely used binomial model, which treats each draw as independent with fixed probability, and demonstrate$-$both analytically and empirically$-$that our method more accurately reproduces the statistical properties of sampling from a finite population. To operationalise this framework, we propose a Monte Carlo-based algorithm that efficiently detects unfair rankings by avoiding computationally expensive parameter tuning. Finally, we adapt our generative approach to define affirmative action policies by introducing weights into the sampling process.",
        "translated": "排名算法在从搜索引擎到求职申请等各种决策过程中发挥着举足轻重的作用。当排名直接影响个人时，确保公平性至关重要，特别是对于那些在数据中被边缘化或代表性不足的群体。大多数现有的群体公平性框架通常依赖于确保受保护群体的比例代表性。然而，这些方法在考虑排名过程的随机性或候选池的有限大小时面临局限性。\n\n为此，我们提出了 hyperFA*IR，一个用于评估和保证从有限候选集中生成的排名公平性的框架。它依赖于一个基于超几何分布的生成过程，该过程通过从固定群体规模中进行不放回抽样来模拟现实世界场景。这种方法在top-k选择相对于整个候选池较大或受保护群体规模较小时，改进了公平性评估。我们将我们的方法与广泛使用的二项式模型进行比较，后者将每次抽取视为具有固定概率的独立事件，并从理论和经验上证明，我们的方法更准确地再现了从有限总体中抽样的统计特性。为了将该框架付诸实践，我们提出了一种基于蒙特卡洛的算法，该算法通过避免计算成本高昂的参数调优来有效地检测不公平排名。最后，我们调整了我们的生成方法，通过在抽样过程中引入权重来定义平权行动政策。"
    },
    {
        "title": "A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive,\n  Transparent, and Reproducible Geo-Temporal Information Synthesis",
        "url": "http://arxiv.org/abs/2506.14345v1",
        "pub_date": "2025-06-17",
        "summary": "The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.",
        "translated": "大型语言模型（LLM）的兴起已彻底改变了信息获取方式。当前，LLM还驱动着深度研究系统，这些系统能够通过规划的迭代搜索、检索和推理，生成全面、报告式的答案。然而，当前的深度研究系统仍缺乏时空能力，而这种能力对于回答涉及地理和/或时间约束的上下文丰富问题至关重要，这些问题常见于公共卫生、环境科学或社会经济分析等领域。\n\n本文提出了我们对下一代系统的愿景，明确了将时空推理整合到深度研究流程中的重要技术、基础设施和评估挑战。我们主张，应通过开放和可复现的基础设施以及严格的评估协议，增强检索和合成过程处理时空约束的能力。我们的愿景为构建更先进、更具时空感知能力的深度研究系统指明了一条路径，有望对人工智能驱动的信息获取的未来产生深远影响。"
    },
    {
        "title": "ImpReSS: Implicit Recommender System for Support Conversations",
        "url": "http://arxiv.org/abs/2506.14231v1",
        "pub_date": "2025-06-17",
        "summary": "Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.",
        "translated": "随着大语言模型（LLM）的最新进展，基于LLM的聊天机器人通过自动化交互并提供一致、可扩展的服务，彻底改变了客户支持。虽然基于LLM的对话推荐系统（CRS）因其提升推荐质量的能力而备受关注，但针对推荐在客户支持交互中隐式集成的研究却十分有限。在本文中，我们提出了ImpReSS，一个专为客户支持对话设计的隐式推荐系统。ImpReSS与现有支持聊天机器人并行运行，在用户报告问题、聊天机器人提供解决方案的场景中发挥作用。基于客户支持对话，ImpReSS识别出推荐相关解决方案产品类别（SPC）的机会，这些类别有助于解决问题或预防问题再次发生，从而也支持业务增长。与传统CRS不同，ImpReSS完全隐式地运作，不依赖于用户购买意图的任何假设。我们对ImpReSS在支持对话中推荐有助于解决所提出问题的相关SPC的能力进行了实证评估，结果显示出可喜的成果，包括：在通用问题解决方面，MRR@1（以及recall@3）分别为0.72（0.89）；在信息安全支持方面为0.82（0.83）；在网络安全故障排除方面为0.85（0.67）。为了支持未来的研究，我们的数据和代码将应要求共享。"
    },
    {
        "title": "InsertRank: LLMs can reason over BM25 scores to Improve Listwise\n  Reranking",
        "url": "http://arxiv.org/abs/2506.14086v1",
        "pub_date": "2025-06-17",
        "summary": "Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.",
        "translated": "大语言模型（LLMs）在各类信息检索任务中展现出显著进展，尤其是在作为重排序器（reranker）方面，这得益于它们通过大规模预训练获得的强大泛化能力和知识迁移能力。与此同时，基于LLM的聊天界面的兴起提升了用户预期，促使用户提出更复杂的查询，这些查询需要通过对文档进行“推理”来检索，而非通过简单的关键词匹配或语义相似性。尽管一些近期的研究工作已利用LLM的推理能力来对这类查询进行重排序，但仍有相当大的改进空间。\n\n为此，我们引入了InsertRank，这是一种基于LLM的重排序器，它在重排序过程中利用了BM25分数等词法信号，以进一步提升检索性能。InsertRank在BRIGHT（一个涵盖12个不同领域的推理基准）和R2MED（一个涵盖8个不同任务的专用医学推理检索基准）上均展现出更高的检索有效性。我们进行了详尽的评估和多项消融研究，并证明InsertRank在包括GPT、Gemini和Deepseek模型在内的多种LLM家族中持续提升了检索有效性。此外，我们还对通过改变BM25分数尺度进行的归一化（normalization）以及通过打乱文档顺序引起的位置偏差（positional bias）进行了消融研究。使用Deepseek-R1模型，InsertRank在BRIGHT基准上获得了37.5分，在R2MED基准上获得了51.1分，超越了以往的方法。"
    },
    {
        "title": "Refining music sample identification with a self-supervised graph neural\n  network",
        "url": "http://arxiv.org/abs/2506.14684v1",
        "pub_date": "2025-06-17",
        "summary": "Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.   In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.   To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.",
        "translated": "自动样本识别（ASID）旨在检测和识别在新音乐作品中被重用的音频片段，是音频查询检索领域中一项重要但具有挑战性的任务。尽管相关任务“音频指纹”在“真实世界”（例如，有噪声、混响）条件下准确检索音乐内容方面取得了显著进展，但ASID系统在识别经过音乐修改的样本时面临困难。因此，开发一个对时间拉伸、音高偏移、效果处理以及伴奏或叠加音乐等常见音乐制作变换具有鲁棒性的系统，是一个重要的开放性挑战。\n\n在这项工作中，我们提出了一种轻量级且可扩展的编码架构，该架构在对比学习框架内采用图神经网络。与当前最先进的系统相比，我们的模型仅使用了9%的可训练参数，同时达到了可比的性能，平均精度（mAP）达到44.2%。\n\n为了提升检索质量，我们引入了一种两阶段方法：首先进行用于候选选择的初步粗粒度相似性搜索，然后是一个交叉注意力分类器，用于拒绝不相关的匹配并细化检索到的候选排名——这是以往模型所缺乏的关键能力。此外，由于实际应用中的查询通常持续时间较短，我们利用为Sample100数据集制作的新的细粒度标注，对系统在短查询场景下的性能进行了基准测试，这些标注也作为我们工作的一部分公开发布。"
    },
    {
        "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets",
        "url": "http://arxiv.org/abs/2506.14761v1",
        "pub_date": "2025-06-17",
        "summary": "Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.",
        "translated": "分词（Tokenization）为输入文本强制设定了固定的粒度，这限制了语言模型对数据的操作方式以及其未来预测的范围。字节对编码（Byte Pair Encoding, BPE）及类似方案对文本进行一次性分割，构建静态词表，这使得模型受限于该选择。我们通过引入一个自回归 U-Net 来打破了这种僵化，该网络在训练过程中学习嵌入自身的标记。该网络读取原始字节，将其汇聚成单词，然后是单词对，再到最多4个单词，从而为其提供了序列的多尺度视图。在更深的阶段，模型必须预测更远的未来——即预测接下来的几个单词而非下一个字节——因此更深的阶段侧重于更广泛的语义模式，而较早的阶段则处理细粒度细节。在仔细调整并控制预训练计算量的情况下，浅层层次结构的模型能与强大的 BPE 基线模型性能持平，而更深层次结构则展现出有前景的趋势。由于分词现在内嵌于模型中，相同的系统能够处理字符级任务，并能在低资源语言之间传递知识。"
    },
    {
        "title": "DiscRec: Disentangled Semantic-Collaborative Modeling for Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.15576v1",
        "pub_date": "2025-06-18",
        "summary": "Generative recommendation is emerging as a powerful paradigm that directly generates item predictions, moving beyond traditional matching-based approaches. However, current methods face two key challenges: token-item misalignment, where uniform token-level modeling ignores item-level granularity that is critical for collaborative signal learning, and semantic-collaborative signal entanglement, where collaborative and semantic signals exhibit distinct distributions yet are fused in a unified embedding space, leading to conflicting optimization objectives that limit the recommendation performance.   To address these issues, we propose DiscRec, a novel framework that enables Disentangled Semantic-Collaborative signal modeling with flexible fusion for generative Recommendation.First, DiscRec introduces item-level position embeddings, assigned based on indices within each semantic ID, enabling explicit modeling of item structure in input token sequences.Second, DiscRec employs a dual-branch module to disentangle the two signals at the embedding layer: a semantic branch encodes semantic signals using original token embeddings, while a collaborative branch applies localized attention restricted to tokens within the same item to effectively capture collaborative signals. A gating mechanism subsequently fuses both branches while preserving the model's ability to model sequential dependencies. Extensive experiments on four real-world datasets demonstrate that DiscRec effectively decouples these signals and consistently outperforms state-of-the-art baselines. Our codes are available on https://github.com/Ten-Mao/DiscRec.",
        "translated": "生成式推荐正成为一种强大的范式，它直接生成物品预测，超越了传统的基于匹配的方法。然而，当前方法面临两个关键挑战：一是词元-物品错位，即统一的词元级建模忽略了物品级粒度，而这对于协同信号学习至关重要；二是语义-协同信号纠缠，即协同信号和语义信号表现出不同的分布，却在一个统一的嵌入空间中融合，这导致了冲突的优化目标，从而限制了推荐性能。\n\n为解决这些问题，我们提出了DiscRec，这是一个新颖的框架，旨在实现生成式推荐中语义-协同信号的解耦建模与灵活融合。首先，DiscRec引入了物品级位置嵌入，这些嵌入根据每个语义ID内的索引进行分配，从而能够在输入词元序列中显式地建模物品结构。其次，DiscRec采用一个双分支模块，在嵌入层解耦这两种信号：一个语义分支使用原始词元嵌入来编码语义信号，而一个协同分支则对同一物品内的词元应用局部注意力，以有效捕获协同信号。一个门控机制随后融合了两个分支，同时保留了模型建模序列依赖性的能力。在四个真实世界数据集上进行的大量实验表明，DiscRec有效地解耦了这些信号，并且持续优于最先进的基线方法。我们的代码可在 https://github.com/Ten-Mao/DiscRec 获取。"
    },
    {
        "title": "Multi-Interest Recommendation: A Survey",
        "url": "http://arxiv.org/abs/2506.15284v1",
        "pub_date": "2025-06-18",
        "summary": "Existing recommendation methods often struggle to model users' multifaceted preferences due to the diversity and volatility of user behavior, as well as the inherent uncertainty and ambiguity of item attributes in practical scenarios. Multi-interest recommendation addresses this challenge by extracting multiple interest representations from users' historical interactions, enabling fine-grained preference modeling and more accurate recommendations. It has drawn broad interest in recommendation research. However, current recommendation surveys have either specialized in frontier recommendation methods or delved into specific tasks and downstream applications. In this work, we systematically review the progress, solutions, challenges, and future directions of multi-interest recommendation by answering the following three questions: (1) Why is multi-interest modeling significantly important for recommendation? (2) What aspects are focused on by multi-interest modeling in recommendation? and (3) How can multi-interest modeling be applied, along with the technical details of the representative modules? We hope that this survey establishes a fundamental framework and delivers a preliminary overview for researchers interested in this field and committed to further exploration. The implementation of multi-interest recommendation summarized in this survey is maintained at https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey.",
        "translated": "现有推荐方法在实际应用中，由于用户行为的多样性和易变性，以及物品属性固有的不确定性和模糊性，往往难以建模用户多兴趣偏好。多兴趣推荐（Multi-interest recommendation）通过从用户历史交互中提取多个兴趣表示，解决了这一挑战，从而实现细粒度偏好建模和更准确的推荐。它在推荐研究领域引起了广泛关注。然而，当前的推荐综述要么专注于前沿推荐方法，要么深入探讨特定任务和下游应用。\n\n在这项工作中，我们通过回答以下三个问题，系统地综述了多兴趣推荐的进展、解决方案、挑战和未来方向：\n(1) 多兴趣建模对推荐为何如此重要？\n(2) 推荐中的多兴趣建模关注哪些方面？\n(3) 多兴趣建模如何应用，以及代表性模块的技术细节？\n\n我们希望这篇综述能为对该领域感兴趣并致力于深入探索的研究人员建立一个基本框架，并提供初步概述。本综述所总结的多兴趣推荐相关实现维护在：https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey。"
    },
    {
        "title": "Next-User Retrieval: Enhancing Cold-Start Recommendations via Generative\n  Next-User Modeling",
        "url": "http://arxiv.org/abs/2506.15267v1",
        "pub_date": "2025-06-18",
        "summary": "The item cold-start problem is critical for online recommendation systems, as the success of this phase determines whether high-quality new items can transition to popular ones, receive essential feedback to inspire creators, and thus lead to the long-term retention of creators. However, modern recommendation systems still struggle to address item cold-start challenges due to the heavy reliance on item and historical interactions, which are non-trivial for cold-start items lacking sufficient exposure and feedback. Lookalike algorithms provide a promising solution by extending feedback for new items based on lookalike users. Traditional lookalike algorithms face such limitations: (1) failing to effectively model the lookalike users and further improve recommendations with the existing rule- or model-based methods; and (2) struggling to utilize the interaction signals and incorporate diverse features in modern recommendation systems.   Inspired by lookalike algorithms, we propose Next-User Retrieval, a novel framework for enhancing cold-start recommendations via generative next-user modeling. Specifically, we employ a transformer-based model to capture the unidirectional relationships among recently interacted users and utilize these sequences to generate the next potential user who is most likely to interact with the item. The additional item features are also integrated as prefix prompt embeddings to assist the next-user generation. The effectiveness of Next-User Retrieval is evaluated through both offline experiments and online A/B tests. Our method achieves significant improvements with increases of 0.0142% in daily active users and +0.1144% in publications in Douyin, showcasing its practical applicability and scalability.",
        "translated": "物品冷启动问题对在线推荐系统至关重要，因为这一阶段的成功决定了高质量的新物品能否发展为热门物品，能否获得关键反馈以激励创作者，从而实现创作者的长期留存。然而，现代推荐系统仍难以有效解决物品冷启动挑战，因为它们严重依赖物品和历史交互信息，而对于缺乏足够曝光和反馈的冷启动物品而言，这些信息是难以获取的。相似用户算法（Lookalike algorithms）通过基于相似用户为新物品扩展反馈，提供了一种有前景的解决方案。传统的相似用户算法面临以下局限性：(1) 无法有效建模相似用户，也无法利用现有基于规则或基于模型的方法进一步改进推荐；(2) 难以在现代推荐系统中有效利用交互信号并整合多样化特征。\n\n受相似用户算法启发，我们提出了“下一位用户检索”（Next-User Retrieval），这是一个新颖的框架，旨在通过生成式下一位用户建模来增强冷启动推荐。具体而言，我们采用基于Transformer的模型来捕捉最近交互用户之间的单向关系，并利用这些序列生成最有可能与该物品交互的下一位潜在用户。额外的物品特征也被整合为前缀提示嵌入（prefix prompt embeddings），以辅助下一位用户的生成。“下一位用户检索”的有效性通过离线实验和在线A/B测试进行了评估。我们的方法取得了显著改进，使抖音的日活跃用户（DAU）增加了0.0142%，发布量增加了0.1144%，充分展示了其在实际应用中的可行性和可扩展性。"
    },
    {
        "title": "Advancing Loss Functions in Recommender Systems: A Comparative Study\n  with a Rényi Divergence-Based Solution",
        "url": "http://arxiv.org/abs/2506.15120v1",
        "pub_date": "2025-06-18",
        "summary": "Loss functions play a pivotal role in optimizing recommendation models. Among various loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are particularly effective. Their theoretical connections and differences warrant in-depth exploration. This work conducts comprehensive analyses of these losses, yielding significant insights: 1) Common strengths -- both can be viewed as augmentations of traditional losses with Distributional Robust Optimization (DRO), enhancing robustness to distributional shifts; 2) Respective limitations -- stemming from their use of different distribution distance metrics in DRO optimization, SL exhibits high sensitivity to false negative instances, whereas CCL suffers from low data utilization. To address these limitations, this work proposes a new loss function, DrRL, which generalizes SL and CCL by leveraging R\\'enyi-divergence in DRO optimization. DrRL incorporates the advantageous structures of both SL and CCL, and can be demonstrated to effectively mitigate their limitations. Extensive experiments have been conducted to validate the superiority of DrRL on both recommendation accuracy and robustness.",
        "translated": "损失函数在优化推荐模型中扮演着关键角色。在各种损失函数中，Softmax损失 (SL) 和余弦对比损失 (CCL) 尤为有效。它们的理论联系和区别值得深入探讨。本文对这些损失函数进行了全面分析，获得了重要的见解：1) 共同优点——两者都可以被视为结合了分布鲁棒优化 (DRO) 的传统损失函数的增强，从而增强了对分布偏移的鲁棒性；2) 各自的局限性——源于它们在DRO优化中使用了不同的分布距离度量，SL对假阴性实例表现出高敏感性，而CCL则存在数据利用率低的问题。为解决这些局限性，本文提出了一种新的损失函数DrRL，它通过在DRO优化中利用Rényi散度，泛化了SL和CCL。DrRL融合了SL和CCL的优势结构，并且可以证明能够有效缓解它们的局限性。广泛的实验验证了DrRL在推荐准确性和鲁棒性两方面的优越性。"
    },
    {
        "title": "Dense SAE Latents Are Features, Not Bugs",
        "url": "http://arxiv.org/abs/2506.15679v1",
        "pub_date": "2025-06-18",
        "summary": "Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \\emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.",
        "translated": "稀疏自编码器（SAE）旨在通过施加稀疏性约束，从语言模型中提取可解释的特征。理想情况下，SAE的训练应生成既稀疏又语义有意义的隐变量。然而，许多SAE隐变量频繁激活（即是“密集”的），这引发了它们可能是训练过程中不良产物的担忧。\n\n本研究系统地调查了密集隐变量的几何、功能和起源，并表明它们不仅持久存在，而且通常反映了有意义的模型表示。我们首先证明，密集隐变量倾向于形成对立对，这些对立对能够重构残差流中的特定方向。并且，消融其子空间会抑制在重新训练的SAE中出现新的密集特征——这表明高密度特征是残差空间的一种内在属性。随后，我们引入了密集隐变量的分类法，识别出与位置跟踪、上下文绑定、熵调节、特定字母输出信号、词性以及主成分重构相关的类别。最后，我们分析了这些特征在不同层级间的演变，揭示了从早期层级的结构特征，到中间层级的语义特征，再到模型最后层级面向输出的信号的转变。\n\n我们的发现表明，密集隐变量在语言模型计算中扮演着功能性角色，不应被视为训练噪声而忽视。"
    },
    {
        "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards\n  Improve Preference Learning",
        "url": "http://arxiv.org/abs/2506.15651v1",
        "pub_date": "2025-06-18",
        "summary": "Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. We present AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, we employ language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. Our analysis confirms that the extracted rules exhibit good agreement with dataset preference. We find that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, our case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix, and the code is open-sourced at https://github.com/cxcscmu/AutoRule.",
        "translated": "基于规则的奖励为改进人类反馈强化学习（RLHF）提供了一种有前景的策略，但当前方法通常依赖手动规则工程。我们提出了 AutoRule，一种从偏好反馈中提取规则并将其转化为基于规则的奖励的全自动方法。AutoRule 的提取过程分为三个阶段：它利用推理模型解释用户偏好，从这些解释的推理链中识别候选规则，并将它们综合成一个统一的规则集。利用最终确定的规则集，我们采用语言模型验证器来计算每个输出满足规则的比例，并在策略优化期间将此指标作为辅助奖励，与学习到的奖励模型并行使用。与使用相同学习奖励模型但没有基于规则辅助奖励进行训练的 GRPO 基线模型相比，使用 AutoRule 训练 Llama-3-8B 模型，在 AlpacaEval2.0 上实现了长度受控胜率 28.6% 的相对提升，并在一个保留的 MT-Bench 子集上实现了第二轮性能 6.1% 的相对提升。我们的分析证实，提取的规则与数据集偏好表现出良好的一致性。我们发现，在两轮运行中，AutoRule 相比于学习到的奖励模型，展现出更低的奖励作弊（reward hacking）现象。最后，我们的案例研究表明，提取的规则捕捉到了不同数据集中所重视的独特质量。提取的规则已在附录中提供，代码已在 https://github.com/cxcscmu/AutoRule 开源。"
    },
    {
        "title": "Towards AI Search Paradigm",
        "url": "http://arxiv.org/abs/2506.17188v1",
        "pub_date": "2025-06-20",
        "summary": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.",
        "translated": "本论文引入了AI搜索范式，这是一个全面的蓝图，旨在构建能够模拟人类信息处理和决策的下一代搜索系统。该范式采用了一种由四个LLM（大型语言模型）驱动的智能体（Master、Planner、Executor和Writer）组成的模块化架构，这些智能体能够动态适应从简单事实查询到复杂多阶段推理任务的各种信息需求。这些智能体通过协调的工作流动态协作，以评估查询复杂性、将问题分解为可执行计划，并协调工具使用、任务执行和内容合成。我们系统地介绍了实现这一范式的关键方法，包括任务规划和工具集成、执行策略、对齐且鲁棒的检索增强生成，以及高效的LLM推理，涵盖了算法技术和基础设施层面的优化。通过为这些基础组件提供深入指导，本工作旨在为开发可信赖、自适应和可扩展的AI搜索系统提供参考。"
    },
    {
        "title": "PersonalAI: Towards digital twins in the graph form",
        "url": "http://arxiv.org/abs/2506.17001v1",
        "pub_date": "2025-06-20",
        "summary": "The challenge of personalizing language models, specifically the ability to account for a user's history during interactions, is of significant interest. Despite recent advancements in large language models (LLMs) and Retrieval Augmented Generation that have enhanced the factual base of LLMs, the task of retaining extensive personal information and using it to generate personalized responses remains pertinent. To address this, we propose utilizing external memory in the form of knowledge graphs, which are constructed and updated by the LLM itself. We have expanded upon ideas of AriGraph architecture and for the first time introduced a combined graph featuring both standard edges and two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and DiaASQ benchmarks indicates that this approach aids in making the process of graph construction and knowledge extraction unified and robust. Furthermore, we augmented the DiaASQ benchmark by incorporating parameters such as time into dialogues and introducing contradictory statements made by the same speaker at different times. Despite these modifications, the performance of the question-answering system remained robust, demonstrating the proposed architecture's ability to maintain and utilize temporal dependencies.",
        "translated": "个性化语言模型，特别是如何在交互过程中考虑用户历史信息的能力，备受关注。尽管大语言模型（LLMs）和检索增强生成（Retrieval Augmented Generation，RAG）的最新进展已显著提升了LLMs的事实基础，但保留大量个人信息并利用其生成个性化回复的任务仍然具有重要意义。\n\n为解决这一问题，我们提出利用外部记忆，具体形式为由LLM自身构建和更新的知识图谱。我们在AriGraph架构思想的基础上进行了扩展，并首次引入了一种结合了标准边和两种类型超边的组合图。在TriviaQA、HotpotQA和DiaASQ基准测试集上进行的实验表明，该方法有助于使图谱构建和知识提取过程更加统一和鲁棒。此外，我们通过在对话中引入时间等参数，并加入同一说话者在不同时间做出的矛盾性陈述，对DiaASQ基准测试集进行了增强。尽管进行了这些修改，问答系统的性能依然保持鲁棒，这证明了所提出的架构能够维护和利用时间依赖性。"
    },
    {
        "title": "RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed\n  Question Answering",
        "url": "http://arxiv.org/abs/2506.16988v1",
        "pub_date": "2025-06-20",
        "summary": "We present RAGentA, a multi-agent retrieval-augmented generation (RAG) framework for attributed question answering (QA). With the goal of trustworthy answer generation, RAGentA focuses on optimizing answer correctness, defined by coverage and relevance to the question and faithfulness, which measures the extent to which answers are grounded in retrieved documents. RAGentA uses a multi-agent architecture that iteratively filters retrieved documents, generates attributed answers with in-line citations, and verifies completeness through dynamic refinement. Central to the framework is a hybrid retrieval strategy that combines sparse and dense methods, improving Recall@20 by 12.5% compared to the best single retrieval model, resulting in more correct and well-supported answers. Evaluated on a synthetic QA dataset derived from the FineWeb index, RAGentA outperforms standard RAG baselines, achieving gains of 1.09% in correctness and 10.72% in faithfulness. These results demonstrate the effectiveness of the multi-agent architecture and hybrid retrieval in advancing trustworthy QA.",
        "translated": "我们提出了 RAGentA，一个用于归因问答（QA）的多智能体检索增强生成（RAG）框架。RAGentA 以生成可信答案为目标，致力于优化答案的正确性（由覆盖度和与问题的相关性定义）和忠实性（衡量答案在多大程度上基于检索到的文档）。RAGentA 采用多智能体架构，该架构迭代地过滤检索到的文档，生成带有行内引用的归因答案，并通过动态优化验证其完整性。该框架的核心是一种混合检索策略，它结合了稀疏和密集方法，与最佳单一检索模型相比，将 Recall@20 提高了 12.5%，从而生成了更正确且有充分支撑的答案。在源自 FineWeb 索引的合成问答数据集上进行评估，RAGentA 优于标准 RAG 基线，在正确性方面提升了 1.09%，在忠实性方面提升了 10.72%。这些结果证明了多智能体架构和混合检索在推动可信问答方面的有效性。"
    },
    {
        "title": "Pyramid Mixer: Multi-dimensional Multi-period Interest Modeling for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2506.16942v1",
        "pub_date": "2025-06-20",
        "summary": "Sequential recommendation, a critical task in recommendation systems, predicts the next user action based on the understanding of the user's historical behaviors. Conventional studies mainly focus on cross-behavior modeling with self-attention based methods while neglecting comprehensive user interest modeling for more dimensions. In this study, we propose a novel sequential recommendation model, Pyramid Mixer, which leverages the MLP-Mixer architecture to achieve efficient and complete modeling of user interests. Our method learns comprehensive user interests via cross-behavior and cross-feature user sequence modeling. The mixer layers are stacked in a pyramid way for cross-period user temporal interest learning. Through extensive offline and online experiments, we demonstrate the effectiveness and efficiency of our method, and we obtain a +0.106% improvement in user stay duration and a +0.0113% increase in user active days in the online A/B test. The Pyramid Mixer has been successfully deployed on the industrial platform, demonstrating its scalability and impact in real-world applications.",
        "translated": "序列推荐是推荐系统中的一项关键任务，它基于对用户历史行为的理解来预测用户的下一个行为。传统研究主要关注使用基于自注意力的方法进行跨行为建模，却忽略了从更多维度进行综合用户兴趣建模。在这项研究中，我们提出了一个新颖的序列推荐模型——Pyramid Mixer，它利用MLP-Mixer架构来实现对用户兴趣的有效且完整的建模。我们的方法通过跨行为和跨特征的用户序列建模来学习综合用户兴趣。混合器层以金字塔方式堆叠，用于学习跨周期的用户时间兴趣。通过大量的离线和在线实验，我们证明了我们方法的有效性和效率，并且在在线A/B测试中，我们获得了用户停留时长0.106%的提升以及用户活跃天数0.0113%的增长。Pyramid Mixer已成功部署到工业平台，证明了其在实际应用中的可扩展性和影响力。"
    },
    {
        "title": "Multi-Objective Recommendation in the Era of Generative AI: A Survey of\n  Recent Progress and Future Prospects",
        "url": "http://arxiv.org/abs/2506.16893v1",
        "pub_date": "2025-06-20",
        "summary": "With the recent progress in generative artificial intelligence (Generative AI), particularly in the development of large language models, recommendation systems are evolving to become more versatile. Unlike traditional techniques, generative AI not only learns patterns and representations from complex data but also enables content generation, data synthesis, and personalized experiences. This generative capability plays a crucial role in the field of recommendation systems, helping to address the issue of data sparsity and improving the overall performance of recommendation systems. Numerous studies on generative AI have already emerged in the field of recommendation systems. Meanwhile, the current requirements for recommendation systems have surpassed the single utility of accuracy, leading to a proliferation of multi-objective research that considers various goals in recommendation systems. However, to the best of our knowledge, there remains a lack of comprehensive studies on multi-objective recommendation systems based on generative AI technologies, leaving a significant gap in the literature. Therefore, we investigate the existing research on multi-objective recommendation systems involving generative AI to bridge this gap. We compile current research on multi-objective recommendation systems based on generative techniques, categorizing them by objectives. Additionally, we summarize relevant evaluation metrics and commonly used datasets, concluding with an analysis of the challenges and future directions in this domain.",
        "translated": "随着生成式人工智能（Generative AI）的最新进展，特别是在大语言模型（LLMs）的开发方面，推荐系统正在演进得更加多功能。与传统技术不同，生成式AI不仅能从复杂数据中学习模式和表征，还能实现内容生成、数据合成和个性化体验。这种生成能力在推荐系统领域发挥着关键作用，有助于解决数据稀疏性问题，并提升推荐系统的整体性能。目前，推荐系统领域已经涌现出大量关于生成式AI的研究。同时，当前对推荐系统的要求已超越了单一的准确性效用，这导致了考虑推荐系统中多种目标的多目标研究激增。然而，据我们所知，目前仍缺乏对基于生成式AI技术的多目标推荐系统的全面研究，这在现有文献中留下了一个显著的空白。因此，我们旨在调研涉及生成式AI的多目标推荐系统领域的现有研究，以弥补这一空白。我们整理了基于生成式技术的多目标推荐系统的现有研究，并根据其目标进行分类。此外，我们总结了相关的评估指标和常用数据集，最后分析了该领域的挑战和未来方向。"
    },
    {
        "title": "eSapiens: A Real-World NLP Framework for Multimodal Document\n  Understanding and Enterprise Knowledge Processing",
        "url": "http://arxiv.org/abs/2506.16768v1",
        "pub_date": "2025-06-20",
        "summary": "We introduce eSapiens, a unified question-answering system designed for enterprise settings, which bridges structured databases and unstructured textual corpora via a dual-module architecture. The system combines a Text-to-SQL planner with a hybrid Retrieval-Augmented Generation (RAG) pipeline, enabling natural language access to both relational data and free-form documents. To enhance answer faithfulness, the RAG module integrates dense and sparse retrieval, commercial reranking, and a citation verification loop that ensures grounding consistency. We evaluate eSapiens on the RAGTruth benchmark across five leading large language models (LLMs), analyzing performance across key dimensions such as completeness, hallucination, and context utilization. Results demonstrate that eSapiens outperforms a FAISS baseline in contextual relevance and generation quality, with optional strict-grounding controls for high-stakes scenarios. This work provides a deployable framework for robust, citation-aware question answering in real-world enterprise applications.",
        "translated": "本文推出eSapiens，一个专为企业环境设计的统一问答系统，它通过双模块架构桥接了结构化数据库和非结构化文本语料库。该系统整合了Text-to-SQL规划器和混合式检索增强生成（RAG）流水线，从而实现了对关系数据和自由格式文档的自然语言访问。为提升回答忠实性，RAG模块融合了稠密和稀疏检索、商用重排序以及一个引用验证循环，旨在确保接地一致性。我们使用五个领先的大型语言模型（LLMs），在RAGTruth基准上对eSapiens进行了评估，并在完整性、幻觉和上下文利用率等关键维度上分析了其性能。结果表明，eSapiens在上下文相关性和生成质量方面超越了FAISS基线，并针对高风险场景提供了可选的严格接地控制。本研究为实际企业应用中鲁棒、引用感知的问答提供了一个可部署的框架。"
    },
    {
        "title": "A Simple Contrastive Framework Of Item Tokenization For Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.16683v1",
        "pub_date": "2025-06-20",
        "summary": "Generative retrieval-based recommendation has emerged as a promising paradigm aiming at directly generating the identifiers of the target candidates. However, in large-scale recommendation systems, this approach becomes increasingly cumbersome due to the redundancy and sheer scale of the token space. To overcome these limitations, recent research has explored the use of semantic tokens as an alternative to ID tokens, which typically leveraged reconstruction-based strategies, like RQ-VAE, to quantize content embeddings and significantly reduce the embedding size. However, reconstructive quantization aims for the precise reconstruction of each item embedding independently, which conflicts with the goal of generative retrieval tasks focusing more on differentiating among items. Moreover, multi-modal side information of items, such as descriptive text and images, geographical knowledge in location-based recommendation services, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Nevertheless, effectively integrating such complementary knowledge into existing generative recommendation frameworks remains challenging. To overcome these challenges, we propose a novel unsupervised deep quantization exclusively based on contrastive learning, named SimCIT (a Simple Contrastive Item Tokenization framework). Specifically, different from existing reconstruction-based strategies, SimCIT propose to use a learnable residual quantization module to align with the signals from different modalities of the items, which combines multi-modal knowledge alignment and semantic tokenization in a mutually beneficial contrastive learning framework. Extensive experiments across public datasets and a large-scale industrial dataset from various domains demonstrate SimCIT's effectiveness in LLM-based generative recommendation.",
        "translated": "基于生成式检索的推荐作为一种有前景的范式崭露头角，旨在直接生成目标候选项目的标识符。然而，在大规模推荐系统中，由于令牌空间的冗余和庞大规模，这种方法变得越来越繁琐。为了克服这些局限性，近期研究探索了使用语义令牌作为ID令牌的替代，这些方法通常利用基于重建的策略（如RQ-VAE）来量化内容嵌入并显著减小嵌入大小。然而，重建式量化旨在独立精确重建每个项目嵌入，这与生成式检索任务更侧重于区分项目的目标相冲突。此外，项目的多模态辅助信息，例如描述性文本和图像，以及基于位置的推荐服务中的地理知识，已被证明通过为交互提供更丰富的上下文，能有效改善推荐效果。尽管如此，将此类互补知识有效整合到现有生成式推荐框架中仍然具有挑战性。\n\n为了克服这些挑战，我们提出了一种完全基于对比学习的新颖无监督深度量化方法，命名为SimCIT（一个简单的对比项目令牌化框架）。具体来说，SimCIT不同于现有基于重建的策略，提出使用一个可学习的残差量化模块来对齐来自项目不同模态的信号，这在一个互利共赢的对比学习框架中结合了多模态知识对齐和语义令牌化。在来自不同领域的大量公开数据集和大规模工业数据集上的广泛实验表明，SimCIT在基于大语言模型的生成式推荐中表现出卓越的有效性。"
    },
    {
        "title": "Semantic Outlier Removal with Embedding Models and LLMs",
        "url": "http://arxiv.org/abs/2506.16644v1",
        "pub_date": "2025-06-19",
        "summary": "Modern text processing pipelines demand robust methods to remove extraneous content while preserving a document's core message. Traditional approaches such as HTML boilerplate extraction or keyword filters often fail in multilingual settings and struggle with context-sensitive nuances, whereas Large Language Models (LLMs) offer improved quality at high computational cost. We introduce SORE (Semantic Outlier Removal), a cost-effective, transparent method that leverages multilingual sentence embeddings and approximate nearest-neighbor search to identify and excise unwanted text segments. By first identifying core content via metadata embedding and then flagging segments that either closely match predefined outlier groups or deviate significantly from the core, SORE achieves near-LLM extraction precision at a fraction of the cost. Experiments on HTML datasets demonstrate that SORE outperforms structural methods and yield high precision in diverse scenarios. Our system is currently deployed in production, processing millions of documents daily across multiple languages while maintaining both efficiency and accuracy. To facilitate reproducibility and further research, we release our implementation and evaluation datasets.",
        "translated": "现代文本处理流水线需要鲁棒的方法来移除冗余内容，同时保留文档的核心信息。传统的如HTML样板内容提取或关键词过滤方法在多语言环境下往往失效，并且难以处理上下文敏感的细微差别；而大语言模型（LLM）虽然能提供更高的质量，但计算成本高昂。\n\n我们引入了SORE（语义离群点剔除），这是一种经济高效且透明的方法，它利用多语言句子嵌入和近似最近邻搜索来识别并剔除不需要的文本片段。SORE首先通过元数据嵌入识别核心内容，然后标记那些与预定义离群点组高度匹配或显著偏离核心内容的片段，从而以极低的成本实现了接近大语言模型的提取精度。在HTML数据集上的实验表明，SORE优于结构化方法，并在多样化场景中展现出高精度。\n\n我们的系统目前已投入生产使用，每天处理数百万份跨多种语言的文档，同时保持了高效性和准确性。为了促进可复现性和进一步研究，我们发布了我们的实现代码和评估数据集。"
    },
    {
        "title": "Revela: Dense Retriever Learning via Language Modeling",
        "url": "http://arxiv.org/abs/2506.16552v1",
        "pub_date": "2025-06-19",
        "summary": "Dense retrievers play a vital role in accessing external and specialized knowledge to augment language models (LMs). Training dense retrievers typically requires annotated query-document pairs, which are costly and hard to obtain in specialized domains such as code-motivating growing interest in self-supervised retriever learning. Since LMs are trained to capture token-level dependencies through a self-supervised learning objective (i.e., next-token prediction), we can analogously cast retrieval as learning dependencies among chunks of tokens. This analogy naturally leads to the question: How can we adapt self-supervised learning objectives in the spirit of language modeling to train retrievers?   To answer this question, we introduce Revela, a unified and scalable training framework for self-supervised retriever learning via language modeling. Revela models semantic dependencies among documents by conditioning next-token prediction on both local and cross-document context through an in-batch attention mechanism. This attention is weighted by retriever-computed similarity scores, enabling the retriever to be optimized as part of language modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific (CoIR) benchmarks across various retriever backbones. At a comparable parameter scale, Revela outperforms the previous best method with absolute improvements of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10, respectively, underscoring its effectiveness. Performance increases with model size, highlighting both the scalability of our approach and its promise for self-supervised retriever learning.",
        "translated": "稠密检索器在使语言模型（LMs）获取外部和专业知识以增强其能力方面发挥着至关重要的作用。训练稠密检索器通常需要标注的查询-文档对，这在代码等专业领域获取成本高昂且难度大，因此促使人们对自监督检索器学习的兴趣日益增长。鉴于语言模型通过自监督学习目标（即下词元预测）训练以捕获词元级依赖，我们可以类比地将检索视为学习词元块之间的依赖关系。这种类比自然引出了一个问题：我们如何将沿用语言建模思路的自监督学习目标应用于检索器训练？\n\n为了回答这个问题，我们引入了Revela，这是一个基于语言建模的自监督检索器学习的统一且可扩展的训练框架。Revela通过批内注意力机制，以局部和跨文档上下文为条件进行下词元预测，从而建模文档间的语义依赖。这种注意力由检索器计算的相似度分数加权，使得检索器能够作为语言建模的一部分进行优化。我们使用通用领域（BEIR）和领域特定（CoIR）基准，针对多种检索器骨干网络对Revela进行了评估。在可比较的参数规模下，Revela在NDCG@10上分别以5.2%（相对18.3%）和5.6%（相对14.4%）的绝对提升，超越了此前表现最佳的方法，强调了其有效性。性能随模型大小的增加而提升，突显了我们方法的可扩展性及其在自监督检索器学习方面的广阔前景。"
    },
    {
        "title": "Towards AI Search Paradigm",
        "url": "http://arxiv.org/abs/2506.17188v1",
        "pub_date": "2025-06-20",
        "summary": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.",
        "translated": "本文中，我们提出了**人工智能搜索范式**（AI Search Paradigm），这是一个全面的蓝图，旨在构建能够模拟人类信息处理和决策的下一代搜索系统。该范式采用模块化架构，由四个LLM驱动的智能体（Master、Planner、Executor和Writer）构成，这些智能体能够动态适应从简单事实查询到复杂多阶段推理任务的各类信息需求。这些智能体通过协调的工作流动态协作，以评估查询复杂度，将问题分解为可执行计划，并编排工具使用、任务执行和内容合成。\n\n我们系统性地介绍了实现这一范式的关键方法，包括任务规划与工具集成、执行策略、对齐且鲁棒的检索增强生成，以及高效的LLM推理，涵盖了算法技术和基础设施层面的优化。通过为这些基础组件提供深入指导，本工作旨在为开发可信赖、自适应和可扩展的AI搜索系统提供参考。"
    },
    {
        "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.18902v2",
        "pub_date": "2025-06-23",
        "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.",
        "translated": "我们推出了 jina-embeddings-v4，这是一个38亿参数的多模态嵌入模型，它通过一种新颖的架构，以后期交互（late interaction）风格统一了文本和图像表示，支持单向量和多向量嵌入。该模型集成了任务特定的低秩适应（LoRA）适配器，以优化在各种检索场景中的性能，包括查询-文档检索、语义文本相似度和代码搜索。综合评估结果显示，jina-embeddings-v4在单模态和跨模态检索任务上均达到了最先进的性能，尤其擅长处理表格、图表、示意图和混合媒体格式等视觉丰富内容。为便于评估其能力，我们还推出了Jina-VDR，这是一个专门为视觉丰富图像检索设计的新颖基准。"
    },
    {
        "title": "An Audio-centric Multi-task Learning Framework for Streaming Ads\n  Targeting on Spotify",
        "url": "http://arxiv.org/abs/2506.18735v1",
        "pub_date": "2025-06-23",
        "summary": "Spotify, a large-scale multimedia platform, attracts over 675 million monthly active users who collectively consume millions of hours of music, podcasts, audiobooks, and video content. This diverse content consumption pattern introduces unique challenges for computational advertising, which must effectively integrate a variety of ad modalities, including audio, video, and display, within a single user experience. Traditional ad recommendation models, primarily designed for foregrounded experiences, often struggle to reconcile the platform's inherent audio-centrality with the demands of optimizing ad performance across multiple formats and modalities. To overcome these challenges, we introduce Cross-modal Adaptive Mixture-of-Experts (CAMoE), a novel framework for optimizing click-through rate (CTR) prediction in both audio-centric and multi-modal settings. CAMoE enhances traditional mixture-of-experts models by incorporating modality-aware task grouping, adaptive loss masking, and deep-cross networks (DCN) to capture complex feature interactions within a multi-modal ad ecosystem. Through extensive ablation studies, we demonstrate that this approach achieves near Pareto-optimal performance across audio, video, and display ad formats, significantly improving AUC-PR compared to conventional single-task and content-based multi-task learning baselines. When deployed at scale on Spotify's ad serving platform, CAMoE delivered substantial gains, yielding a 14.5% increase in CTR for audio ads, a 1.3% increase for video ads, and a 4.8% reduction in expected cost-per-click (eCPC) for audio slots.",
        "translated": "Spotify作为一个大规模多媒体平台，吸引了超过6.75亿月活跃用户，他们共同消费了数百万小时的音乐、播客、有声读物和视频内容。这种多样化的内容消费模式为计算广告带来了独特的挑战，计算广告必须在单一用户体验中有效整合包括音频、视频和展示广告在内的多种广告模态。传统的广告推荐模型主要为前台体验设计，常常难以协调该平台固有的音频中心性与优化跨多种格式和模态的广告性能的需求。\n\n为了克服这些挑战，我们引入了跨模态自适应混合专家模型（CAMoE），这是一种旨在优化以音频为中心和多模态设置中点击率（CTR）预测的新颖框架。CAMoE通过整合模态感知任务分组、自适应损失掩码和深度交叉网络（DCN），增强了传统的混合专家模型，以捕获多模态广告生态系统中的复杂特征交互。通过广泛的消融研究，我们证明该方法在音频、视频和展示广告格式上实现了接近帕累托最优的性能，与传统的单任务和基于内容的多任务学习基线相比，显著提高了AUC-PR。当在Spotify的广告服务平台大规模部署时，CAMoE带来了显著的收益，使音频广告的CTR提升了14.5%，视频广告提升了1.3%，并使音频广告位的预期每点击成本（eCPC）降低了4.8%。"
    },
    {
        "title": "Harnessing the Power of Reinforcement Learning for Language-Model-Based\n  Information Retriever via Query-Document Co-Augmentation",
        "url": "http://arxiv.org/abs/2506.18670v1",
        "pub_date": "2025-06-23",
        "summary": "Recent studies have proposed leveraging Large Language Models (LLMs) as information retrievers through query rewriting. However, for challenging corpora, we argue that enhancing queries alone is insufficient for robust semantic matching; the LLM should also have sufficient understanding of the corpus by directly handling and augmenting the documents themselves. To this end, we present an LLM-based retriever empowered to augment both user queries and corpus documents, with its policy fully explored via reinforcement learning (RL) and minimal human inductive bias. Notably, we find that simply allowing the LLM to modify documents yields little benefit unless paired with our carefully designed bidirectional RL framework, which enables the LLM to simultaneously learn and collaborate on both query and document augmentation policies. A key technical challenge in realizing such a framework lies in jointly updating both policies during training, where the rewards for the two directions depend on each other, making their entangled reward intractable. Our approach addresses this by introducing a reward sampling strategy and a specifically designed RL algorithm that enables effective training with these sampled rewards. Experimental results demonstrate that our approach significantly enhances LLM-based retrieval performance in both sparse and dense settings, particularly in difficult retrieval domains, and achieves strong cross-benchmark generalization. Our code is released at https://github.com/liujm2001/CoAugRetriever.",
        "translated": "近期研究提出通过查询重写，将大语言模型（LLM）用作信息检索器。然而，我们认为，对于复杂的语料库，仅增强查询不足以实现鲁棒的语义匹配；LLM还应通过直接处理和增强文档本身，对语料库有充分的理解。为此，我们提出了一种基于LLM的检索器，它能够同时增强用户查询和语料库文档，其策略通过强化学习（RL）充分探索，且人工归纳偏置极小。值得注意的是，我们发现，仅允许LLM修改文档效果甚微，除非与我们精心设计的双向强化学习（RL）框架相结合，该框架使LLM能够同时学习并协同制定查询和文档的增强策略。实现这一框架的关键技术挑战在于训练期间如何联合更新两种策略，因为两个方向的奖励相互依赖，使得其纠缠奖励难以处理。我们的方法通过引入一种奖励采样策略和一种专门设计的强化学习算法来解决此问题，该算法能够利用这些采样奖励进行有效训练。实验结果表明，我们的方法显著提升了基于LLM的检索性能，无论是在稀疏还是密集设置下，尤其是在困难的检索领域，并展现出强大的跨基准泛化能力。我们的代码已在 https://github.com/liujm2001/CoAugRetriever 发布。"
    },
    {
        "title": "Rethinking Click Models in Light of Carousel Interfaces: Theory-Based\n  Categorization and Design of Click Models",
        "url": "http://arxiv.org/abs/2506.18548v1",
        "pub_date": "2025-06-23",
        "summary": "Click models are a well-established for modeling user interactions with web interfaces. Previous work has mainly focused on traditional single-list web search settings; this includes existing surveys that introduced categorizations based on the first generation of probabilistic graphical model (PGM) click models that have become standard. However, these categorizations have become outdated, as their conceptualizations are unable to meaningfully compare PGM with neural network (NN) click models nor generalize to newer interfaces, such as carousel interfaces. We argue that this outdated view fails to adequately explain the fundamentals of click model designs, thus hindering the development of novel click models.   This work reconsiders what should be the fundamental concepts in click model design, grounding them - unlike previous approaches - in their mathematical properties. We propose three fundamental key-design choices that explain what statistical patterns a click model can capture, and thus indirectly, what user behaviors they can capture. Based on these choices, we create a novel click model taxonomy that allows a meaningful comparison of all existing click models; this is the first taxonomy of single-list, grid and carousel click models that includes PGMs and NNs. Finally, we show how our conceptualization provides a foundation for future click model design by an example derivation of a novel design for carousel interfaces.",
        "translated": "点击模型是用于建模用户与网页界面交互的一种成熟方法。以往的工作主要集中在传统的单列表网页搜索设置中；这包括现有综述，它们基于已成为标准的第一代概率图模型（PGM）点击模型引入了分类方法。然而，这些分类方法已经过时，因为其概念框架无法有意义地比较 PGM 与神经网络（NN）点击模型，也无法泛化到轮播界面等新型界面。我们认为，这种过时的视角未能充分解释点击模型设计的基本原理，从而阻碍了新型点击模型的发展。\n\n本研究重新审视了点击模型设计中应有的基本概念，并与以往方法不同，将它们根植于其数学特性。我们提出了三个基本的关键设计选择，它们解释了点击模型能够捕捉哪些统计模式，从而间接解释了它们能够捕捉哪些用户行为。基于这些选择，我们创建了一种新颖的点击模型分类体系，它允许对所有现有点击模型进行有意义的比较；这是第一个涵盖 PGM 和 NN 的单列表、网格和轮播点击模型分类体系。最后，我们通过一个针对轮播界面的新颖设计示例推导，展示了我们的概念化如何为未来的点击模型设计提供了基础。"
    },
    {
        "title": "When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking",
        "url": "http://arxiv.org/abs/2506.18535v1",
        "pub_date": "2025-06-23",
        "summary": "This paper investigates the counterintuitive phenomenon where fine-tuning pre-trained transformer models degrades performance on the MS MARCO passage ranking task. Through comprehensive experiments involving five model variants-including full parameter fine-tuning and parameter efficient LoRA adaptations-we demonstrate that all fine-tuning approaches underperform the base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our analysis reveals that fine-tuning disrupts the optimal embedding space structure learned during the base model's extensive pre-training on 1 billion sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations show progressive embedding space flattening, while training dynamics analysis and computational efficiency metrics further support our findings. These results challenge conventional wisdom about transfer learning effectiveness on saturated benchmarks and suggest architectural innovations may be necessary for meaningful improvements.",
        "translated": "本文研究了一个反直觉现象：预训练Transformer模型在MS MARCO段落排序任务上进行微调后，性能反而下降。通过对五种模型变体（包括全参数微调和参数高效的LoRA适应）进行全面实验，我们证明所有微调方法均劣于基准的`sentence-transformers/all-MiniLM-L6-v2`模型（MRR@10: 0.3026）。我们的分析表明，微调破坏了基准模型在对10亿句子对（其中包括910万MS MARCO样本）进行广泛预训练期间学习到的最优嵌入空间结构。UMAP可视化显示嵌入空间逐渐扁平化，同时训练动态分析和计算效率指标进一步支持了我们的发现。这些结果挑战了关于迁移学习在饱和基准测试上有效性的传统认知，并表明为了实现有意义的改进，可能需要架构创新。"
    },
    {
        "title": "PERSCEN: Learning Personalized Interaction Pattern and Scenario\n  Preference for Multi-Scenario Matching",
        "url": "http://arxiv.org/abs/2506.18382v1",
        "pub_date": "2025-06-23",
        "summary": "With the expansion of business scales and scopes on online platforms, multi-scenario matching has become a mainstream solution to reduce maintenance costs and alleviate data sparsity. The key to effective multi-scenario recommendation lies in capturing both user preferences shared across all scenarios and scenario-aware preferences specific to each scenario. However, existing methods often overlook user-specific modeling, limiting the generation of personalized user representations. To address this, we propose PERSCEN, an innovative approach that incorporates user-specific modeling into multi-scenario matching. PERSCEN constructs a user-specific feature graph based on user characteristics and employs a lightweight graph neural network to capture higher-order interaction patterns, enabling personalized extraction of preferences shared across scenarios. Additionally, we leverage vector quantization techniques to distil scenario-aware preferences from users' behavior sequence within individual scenarios, facilitating user-specific and scenario-aware preference modeling. To enhance efficient and flexible information transfer, we introduce a progressive scenario-aware gated linear unit that allows fine-grained, low-latency fusion. Extensive experiments demonstrate that PERSCEN outperforms existing methods. Further efficiency analysis confirms that PERSCEN effectively balances performance with computational cost, ensuring its practicality for real-world industrial systems.",
        "translated": "随着在线平台业务规模和范围的不断扩大，多场景匹配已成为降低维护成本、缓解数据稀疏性的主流解决方案。有效多场景推荐的关键在于同时捕获跨场景共享的用户偏好和各场景特有的场景感知偏好。然而，现有方法常常忽视用户专属建模，这限制了个性化用户表示的生成。\n\n为解决此问题，我们提出了一种名为PERSCEN的创新方法，该方法将用户专属建模融入多场景匹配中。PERSCEN基于用户特征构建用户专属特征图，并采用轻量级图神经网络来捕获高阶交互模式，从而实现跨场景共享偏好的个性化提取。此外，我们利用矢量量化技术从用户在各个场景内的行为序列中提炼场景感知偏好，以促进用户专属且场景感知的偏好建模。为增强高效灵活的信息传输，我们引入了渐进式场景感知门控线性单元，该单元支持细粒度、低延迟的融合。大量实验表明，PERSCEN的性能优于现有方法。进一步的效率分析证实，PERSCEN在性能与计算成本之间取得了有效平衡，确保了其在真实工业系统中的实用性。"
    },
    {
        "title": "Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems",
        "url": "http://arxiv.org/abs/2506.18327v1",
        "pub_date": "2025-06-23",
        "summary": "Recommendation systems play a crucial role in our daily lives by impacting user experience across various domains, including e-commerce, job advertisements, entertainment, etc. Given the vital role of such systems in our lives, practitioners must ensure they do not produce unfair and imbalanced recommendations. Previous work addressing bias in recommendations overlooked bias in certain item categories, potentially leaving some biases unaddressed. Additionally, most previous work on fair re-ranking focused on binary-sensitive attributes. In this paper, we address these issues by proposing a fairness-aware re-ranking approach that helps mitigate bias in different categories of items. This re-ranking approach leverages existing biases to correct disparities in recommendations across various demographic groups. We show how our approach can mitigate bias on multiple sensitive attributes, including gender, age, and occupation. We experimented on three real-world datasets to evaluate the effectiveness of our re-ranking scheme in mitigating bias in recommendations. Our results show how this approach helps mitigate social bias with little to no degradation in performance.",
        "translated": "推荐系统通过影响电子商务、招聘广告、娱乐等各个领域的用户体验，在我们的日常生活中扮演着至关重要的角色。鉴于此类系统在我们生活中的重要作用，从业者必须确保它们不会产生不公平和不平衡的推荐。先前旨在解决推荐中偏见的工作忽略了某些物品类别中的偏见，可能导致一些偏见未能得到妥善处理。此外，大多数先前关于公平重排序的工作都侧重于二元敏感属性。\n\n在本文中，我们通过提出一种公平感知重排序方法来解决这些问题，该方法有助于减轻不同类别物品中的偏见。这种重排序方法利用现有偏见，以纠正在不同人口统计学群体之间推荐结果的差异。我们展示了我们的方法如何能够减轻在多个敏感属性（包括性别、年龄和职业）上的偏见。我们在三个真实世界数据集上进行了实验，以评估我们的重排序方案在减轻推荐偏见方面的有效性。我们的结果表明，该方法在性能几乎没有下降的情况下有助于减轻社会偏见。"
    },
    {
        "title": "Team LA at SCIDOCA shared task 2025: Citation Discovery via\n  relation-based zero-shot retrieval",
        "url": "http://arxiv.org/abs/2506.18316v1",
        "pub_date": "2025-06-23",
        "summary": "The Citation Discovery Shared Task focuses on predicting the correct citation from a given candidate pool for a given paragraph. The main challenges stem from the length of the abstract paragraphs and the high similarity among candidate abstracts, making it difficult to determine the exact paper to cite. To address this, we develop a system that first retrieves the top-k most similar abstracts based on extracted relational features from the given paragraph. From this subset, we leverage a Large Language Model (LLM) to accurately identify the most relevant citation. We evaluate our framework on the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its effectiveness in citation prediction.",
        "translated": "**引用发现共享任务**旨在针对给定段落，从给定的候选池中预测正确的引用文献。主要挑战源于摘要段落的长度以及候选摘要之间的高度相似性，这使得确定确切的引用论文变得困难。为此，我们开发了一个系统，该系统首先根据从给定段落中提取的关系特征，检索出top-k个最相似的摘要。从这个子集中，我们利用大型语言模型（LLM）来精确识别最相关的引用文献。我们在SCIDOCA 2025组织者提供的训练数据集上评估了我们的框架，证明了其在引用预测方面的有效性。"
    },
    {
        "title": "Enhancing Document Retrieval in COVID-19 Research: Leveraging Large\n  Language Models for Hidden Relation Extraction",
        "url": "http://arxiv.org/abs/2506.18311v1",
        "pub_date": "2025-06-23",
        "summary": "In recent years, with the appearance of the COVID-19 pandemic, numerous publications relevant to this disease have been issued. Because of the massive volume of publications, an efficient retrieval system is necessary to provide researchers with useful information if an unexpected pandemic happens so suddenly, like COVID-19. In this work, we present a method to help the retrieval system, the Covrelex-SE system, to provide more high-quality search results. We exploited the power of the large language models (LLMs) to extract the hidden relationships inside the unlabeled publication that cannot be found by the current parsing tools that the system is using. Since then, help the system to have more useful information during retrieval progress.",
        "translated": "近年来，随着COVID-19大流行的爆发，大量与该疾病相关的出版物被发布。鉴于出版物数量庞大，如果像COVID-19这样突如其来的大流行再次发生，一个高效的检索系统对于为研究人员提供有用信息至关重要。在这项工作中，我们提出了一种方法，旨在助力Covrelex-SE检索系统提供更高质量的搜索结果。我们利用大型语言模型（LLMs）的强大能力，从当前系统所用解析工具无法发现的未标注出版物中提取出隐藏关系。从而帮助系统在检索过程中获取更丰富的有用信息。"
    },
    {
        "title": "LettinGo: Explore User Profile Generation for Recommendation System",
        "url": "http://arxiv.org/abs/2506.18309v1",
        "pub_date": "2025-06-23",
        "summary": "User profiling is pivotal for recommendation systems, as it transforms raw user interaction data into concise and structured representations that drive personalized recommendations. While traditional embedding-based profiles lack interpretability and adaptability, recent advances with large language models (LLMs) enable text-based profiles that are semantically richer and more transparent. However, existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors. In this paper, we introduce LettinGo, a novel framework for generating diverse and adaptive user profiles. By leveraging the expressive power of LLMs and incorporating direct feedback from downstream recommendation tasks, our approach avoids the rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ Direct Preference Optimization (DPO) to align the profile generator with task-specific performance, ensuring that the profiles remain adaptive and effective. LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance. Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness. This work enhances profile generation as a key innovation for next-generation recommendation systems.",
        "translated": "用户画像对于推荐系统至关重要，因为它将原始用户交互数据转化为简洁且结构化的表示，从而驱动个性化推荐。传统的基于嵌入（embedding）的用户画像缺乏可解释性和适应性，而大型语言模型（LLMs）的最新进展使得基于文本的用户画像语义更丰富、更具透明度。然而，现有方法往往固守固定格式，这限制了它们捕捉用户行为全部多样性的能力。\n\n在本文中，我们提出LettinGo，一个用于生成多样化且自适应用户画像的新颖框架。通过利用LLM的强大表达能力并融合来自下游推荐任务的直接反馈，我们的方法避免了监督微调（SFT）所施加的僵化约束。相反，我们采用直接偏好优化（DPO）来使画像生成器与任务特定性能对齐，从而确保画像保持自适应性和有效性。LettinGo分为三个阶段运作：(1) 通过多个LLM探索多样化的用户画像；(2) 基于画像在推荐系统中的影响来评估其质量；(3) 通过源自任务性能的成对偏好数据来对齐画像生成。实验结果表明，我们的框架显著提升了推荐准确性、灵活性和上下文感知能力。本工作将用户画像生成提升为下一代推荐系统的一项关键创新。"
    },
    {
        "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.18902v2",
        "pub_date": "2025-06-23",
        "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.",
        "translated": "我们推出了jina-embeddings-v4，这是一个38亿参数的多模态嵌入模型。它通过一种支持晚期交互模式下单向量和多向量嵌入的新颖架构，统一了文本和图像的表示。该模型集成了任务专用的低秩适应（LoRA）适配器，以优化在各种检索场景中的性能，包括查询-文档检索、语义文本相似性以及代码搜索。全面评估表明，jina-embeddings-v4在单模态和跨模态检索任务上均实现了最先进的性能，尤其擅长处理表格、图表、示意图和混合媒体格式等视觉丰富的内容。为了促进对此能力的评估，我们还推出了Jina-VDR，这是一个专门为视觉丰富的图像检索设计的新颖基准。"
    },
    {
        "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.18902v2",
        "pub_date": "2025-06-23",
        "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.",
        "translated": "我们推出了 jina-embeddings-v4，这是一个拥有38亿参数的多模态嵌入模型。该模型通过一种新颖的架构，统一了文本和图像表示，并支持后期交互（late interaction）风格下的单向量和多向量嵌入。模型集成了任务专用的低秩适应（LoRA）适配器，旨在优化其在各种检索场景中的性能，包括查询-文档检索、语义文本相似度以及代码搜索。\n\n全面评估表明，jina-embeddings-v4 在单模态和跨模态检索任务上均实现了最先进的性能，尤其擅长处理视觉丰富的内容，例如表格、图表、示意图和混合媒体格式。为了便于评估这一能力，我们还推出了 Jina-VDR，这是一个专门为视觉丰富的图像检索设计的新颖基准。"
    },
    {
        "title": "Unidentified and Confounded? Understanding Two-Tower Models for Unbiased\n  Learning to Rank",
        "url": "http://arxiv.org/abs/2506.20501v1",
        "pub_date": "2025-06-25",
        "summary": "Additive two-tower models are popular learning-to-rank methods for handling biased user feedback in industry settings. Recent studies, however, report a concerning phenomenon: training two-tower models on clicks collected by well-performing production systems leads to decreased ranking performance. This paper investigates two recent explanations for this observation: confounding effects from logging policies and model identifiability issues. We theoretically analyze the identifiability conditions of two-tower models, showing that either document swaps across positions or overlapping feature distributions are required to recover model parameters from clicks. We also investigate the effect of logging policies on two-tower models, finding that they introduce no bias when models perfectly capture user behavior. However, logging policies can amplify biases when models imperfectly capture user behavior, particularly when prediction errors correlate with document placement across positions. We propose a sample weighting technique to mitigate these effects and provide actionable insights for researchers and practitioners using two-tower models.",
        "translated": "叠加式双塔模型（Additive two-tower models）是工业环境中用于处理有偏用户反馈的流行学习排序方法。然而，近期研究报告了一个令人担忧的现象：使用由性能良好的生产系统收集的点击数据来训练双塔模型，反而会导致排序性能下降。本文研究了对这一观察结果的两种近期解释：记录策略（logging policies）带来的混淆效应以及模型可识别性问题。\n\n我们从理论上分析了双塔模型的可识别性条件，表明需要文档在位置间的互换（document swaps across positions）或重叠的特征分布（overlapping feature distributions）才能从点击数据中恢复模型参数。我们还研究了记录策略对双塔模型的影响，发现当模型完美捕捉用户行为时，它们不会引入偏差。然而，当模型不完美捕捉用户行为时，记录策略会放大偏差，特别是当预测误差与文档在位置间的放置存在相关性时。我们提出了一种样本加权技术以减轻这些影响，并为使用双塔模型的研究人员和从业者提供可操作的见解。"
    },
    {
        "title": "Knowledge-Aware Diverse Reranking for Cross-Source Question Answering",
        "url": "http://arxiv.org/abs/2506.20476v1",
        "pub_date": "2025-06-25",
        "summary": "This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG competition. The competition's evaluation set, automatically generated by DataMorgana from internet corpora, encompassed a wide range of target topics, question types, question formulations, audience types, and knowledge organization methods. It offered a fair evaluation of retrieving question-relevant supporting documents from a 15M documents subset of the FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline achieved first place in the competition.",
        "translated": "本文介绍了Marikarp团队针对SIGIR 2025 LiveRAG竞赛提出的解决方案。该竞赛的评估数据集由DataMorgana从互联网语料库自动生成，涵盖了广泛的目标主题、问题类型、问题表述方式、受众类型和知识组织方法。它旨在公平地评估从FineWeb语料库的1500万文档子集中检索与问题相关的支持文档的能力。我们提出的知识感知多样化重排RAG管线在该竞赛中获得了第一名。"
    },
    {
        "title": "Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce\n  Search",
        "url": "http://arxiv.org/abs/2506.20330v1",
        "pub_date": "2025-06-25",
        "summary": "Semantic retrieval, which retrieves semantically matched items given a textual query, has been an essential component to enhance system effectiveness in e-commerce search. In this paper, we study the multimodal retrieval problem, where the visual information (e.g, image) of item is leveraged as supplementary of textual information to enrich item representation and further improve retrieval performance. Though learning from cross-modality data has been studied extensively in tasks such as visual question answering or media summarization, multimodal retrieval remains a non-trivial and unsolved problem especially in the asymmetric scenario where the query is unimodal while the item is multimodal. In this paper, we propose a novel model named SMAR, which stands for Semantic-enhanced Modality-Asymmetric Retrieval, to tackle the problem of modality fusion and alignment in this kind of asymmetric scenario. Extensive experimental results on an industrial dataset show that the proposed model outperforms baseline models significantly in retrieval accuracy. We have open sourced our industrial dataset for the sake of reproducibility and future research works.",
        "translated": "语义检索，即基于文本查询来检索语义匹配的物品，一直是提升电商搜索系统有效性的关键组件。本文研究了多模态检索问题，其中物品的视觉信息（例如图像）被用作文本信息的补充，以丰富物品表示并进一步提升检索性能。尽管跨模态数据学习已在视觉问答或媒体摘要等任务中得到了广泛研究，但多模态检索仍然是一个非平凡且尚未解决的问题，尤其是在查询是单模态而物品是多模态的非对称场景中。本文提出了一种名为 SMAR（意为“语义增强型模态非对称检索”）的新模型，旨在解决此类非对称场景中的模态融合与对齐问题。在一个工业数据集上进行的大量实验结果表明，所提出的模型在检索精度方面显著优于基线模型。为了可复现性和未来的研究工作，我们已开源了我们的工业数据集。"
    },
    {
        "title": "A Literature Review on Simulation in Conversational Recommender Systems",
        "url": "http://arxiv.org/abs/2506.20291v1",
        "pub_date": "2025-06-25",
        "summary": "Conversational Recommender Systems (CRSs) have garnered attention as a novel approach to delivering personalized recommendations through multi-turn dialogues. This review developed a taxonomy framework to systematically categorize relevant publications into four groups: dataset construction, algorithm design, system evaluation, and empirical studies, providing a comprehensive analysis of simulation methods in CRSs research. Our analysis reveals that simulation methods play a key role in tackling CRSs' main challenges. For example, LLM-based simulation methods have been used to create conversational recommendation data, enhance CRSs algorithms, and evaluate CRSs. Despite several challenges, such as dataset bias, the limited output flexibility of LLM-based simulations, and the gap between text semantic space and behavioral semantics, persist due to the complexity in Human-Computer Interaction (HCI) of CRSs, simulation methods hold significant potential for advancing CRS research. This review offers a thorough summary of the current research landscape in this domain and identifies promising directions for future inquiry.",
        "translated": "对话推荐系统（CRS）作为一种通过多轮对话提供个性化推荐的新颖方法，受到了广泛关注。本综述构建了一个分类框架，将相关文献系统地分为数据集构建、算法设计、系统评估和实证研究四个类别，并对CRS研究中的模拟方法进行了全面分析。我们的分析表明，模拟方法在解决CRS的主要挑战中发挥着关键作用。例如，基于大型语言模型（LLM）的模拟方法已被用于创建对话推荐数据、增强CRS算法以及评估CRS。尽管存在数据集偏差、基于LLM的模拟输出灵活性不足以及文本语义空间与行为语义之间的鸿沟等挑战（这些挑战因CRS中人机交互（HCI）的复杂性而持续存在），但模拟方法在推进CRS研究方面仍具有巨大潜力。本综述对该领域当前的研究现状进行了全面总结，并指明了未来研究的几个有前景方向。"
    },
    {
        "title": "Irec: A Metacognitive Scaffolding for Self-Regulated Learning through\n  Just-in-Time Insight Recall: A Conceptual Framework and System Prototype",
        "url": "http://arxiv.org/abs/2506.20156v1",
        "pub_date": "2025-06-25",
        "summary": "The core challenge in learning has shifted from knowledge acquisition to effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting on one's learning. Existing digital tools, however, inadequately support metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized review, overlooking the role of context, while Personal Knowledge Management (PKM) tools require high manual maintenance.   To address these challenges, this paper introduces \"Insight Recall,\" a novel paradigm that conceptualizes the context-triggered retrieval of personal past insights as a metacognitive scaffold to promote SRL. We formalize this paradigm using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses a dynamic knowledge graph of the user's learning history. When a user faces a new problem, a hybrid retrieval engine recalls relevant personal \"insights.\" Subsequently, a large language model (LLM) performs a deep similarity assessment to filter and present the most relevant scaffold in a just-in-time manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline for LLM-based knowledge graph construction. We also propose an optional \"Guided Inquiry\" module, where users can engage in a Socratic dialogue with an expert LLM, using the current problem and recalled insights as context. The contribution of this paper is a solid theoretical framework and a usable system platform for designing next-generation intelligent learning systems that enhance metacognition and self-regulation.",
        "translated": "学习的核心挑战已从知识获取转向有效的自我调节学习（SRL），即对自身学习进行规划、监控和反思。然而，现有的数字工具未能充分支持元认知反思。间隔重复系统（SRS）采用脱离语境的复习方式，忽略了语境的作用，而个人知识管理（PKM）工具则需要大量手动维护。\n\n为应对这些挑战，本文引入了“洞察召回”（Insight Recall）这一新颖范式，该范式将语境触发的个人过往洞察检索概念化为促进SRL的元认知支架。我们使用即时自适应干预（JITAI）框架对该范式进行了形式化，并实现了一个名为Irec的原型系统以证明其可行性。Irec的核心是利用用户学习历史的动态知识图谱。当用户面临新问题时，一个混合检索引擎会召回相关的个人“洞察”。随后，一个大型语言模型（LLM）会进行深度相似度评估，以即时地筛选并呈现最相关的支架。为减少认知负荷，Irec采用了基于LLM的知识图谱构建人机协作（human-in-the-loop）流程。我们还提出了一个可选的“引导式探究”（Guided Inquiry）模块，用户可以在其中以当前问题和召回的洞察作为语境，与一个专家LLM进行苏格拉底式对话。本文的贡献在于提供了一个扎实的理论框架和一个可用的系统平台，用于设计旨在增强元认知和自我调节能力的下一代智能学习系统。"
    },
    {
        "title": "Multimodal Information Retrieval for Open World with Edit Distance Weak\n  Supervision",
        "url": "http://arxiv.org/abs/2506.20070v1",
        "pub_date": "2025-06-25",
        "summary": "Existing multi-media retrieval models either rely on creating a common subspace with modality-specific representation models or require schema mapping among modalities to measure similarities among multi-media data. Our goal is to avoid the annotation overhead incurred from considering retrieval as a supervised classification task and re-use the pretrained encoders in large language models and vision tasks. We propose \"FemmIR\", a framework to retrieve multimodal results relevant to information needs expressed with multimodal queries by example without any similarity label. Such identification is necessary for real-world applications where data annotations are scarce and satisfactory performance is required without fine-tuning with a common framework across applications. We curate a new dataset called MuQNOL for benchmarking progress on this task. Our technique is based on weak supervision introduced through edit distance between samples: graph edit distance can be modified to consider the cost of replacing a data sample in terms of its properties, and relevance can be measured through the implicit signal from the amount of edit cost among the objects. Unlike metric learning or encoding networks, FemmIR re-uses the high-level properties and maintains the property value and relationship constraints with a multi-level interaction score between data samples and the query example provided by the user. We empirically evaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs comparably to similar retrieval systems in delivering on-demand retrieval results with exact and approximate similarities while using the existing property identifiers in the system.",
        "translated": "现有多媒体检索模型要么依赖于通过模态特定表示模型创建一个公共子空间，要么需要进行模态间模式映射来衡量多媒体数据间的相似性。我们的目标是避免将检索视为监督分类任务所产生的标注开销，并重用大型语言模型和视觉任务中预训练好的编码器。我们提出了“FemmIR”，这是一个无需任何相似性标签，即可通过示例（by example）检索与多模态查询表达的信息需求相关的多模态结果的框架。这种识别能力对于数据标注稀缺且需要在不跨应用通用框架下进行微调的情况下获得满意性能的实际应用至关重要。我们整理了一个名为MuQNOL的新数据集，用于衡量该任务的进展。\n\n我们的技术基于通过样本间编辑距离引入的弱监督：图编辑距离可以被修改，以考虑根据数据样本属性替换其所产生的成本，并且相关性可以通过对象间编辑成本量化的隐式信号来衡量。不同于度量学习或编码网络，FemmIR重用高层属性，并通过数据样本与用户提供的查询示例之间的多级交互分数，来维持属性值和关系约束。我们使用MuQNOL数据集，在一个走失人员用例上对FemmIR进行了实证评估。FemmIR在利用系统中现有属性标识符的情况下，在提供具备精确和近似相似性的按需检索结果方面，表现出与类似检索系统相当的性能。"
    },
    {
        "title": "Controlled Retrieval-augmented Context Evaluation for Long-form RAG",
        "url": "http://arxiv.org/abs/2506.20051v1",
        "pub_date": "2025-06-24",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models by incorporating context retrieved from external knowledge sources. While the effectiveness of the retrieval module is typically evaluated with relevance-based ranking metrics, such metrics may be insufficient to reflect the retrieval's impact on the final RAG result, especially in long-form generation scenarios. We argue that providing a comprehensive retrieval-augmented context is important for long-form RAG tasks like report generation and propose metrics for assessing the context independent of generation. We introduce CRUX, a \\textbf{C}ontrolled \\textbf{R}etrieval-a\\textbf{U}gmented conte\\textbf{X}t evaluation framework designed to directly assess retrieval-augmented contexts. This framework uses human-written summaries to control the information scope of knowledge, enabling us to measure how well the context covers information essential for long-form generation. CRUX uses question-based evaluation to assess RAG's retrieval in a fine-grained manner. Empirical results show that CRUX offers more reflective and diagnostic evaluation. Our findings also reveal substantial room for improvement in current retrieval methods, pointing to promising directions for advancing RAG's retrieval. Our data and code are publicly available to support and advance future research on retrieval.",
        "translated": "检索增强生成（RAG）通过整合来自外部知识源的上下文来增强大型语言模型。尽管检索模块的有效性通常使用基于相关性的排序指标进行评估，但此类指标可能不足以反映检索对最终RAG结果的影响，尤其是在长篇生成场景中。我们认为，为报告生成等长篇RAG任务提供全面的检索增强上下文至关重要，并提出了独立于生成评估上下文的指标。我们介绍了CRUX，一个**受控检索增强上下文评估框架**（**C**ontrolled **R**etrieval-a**U**gmented conte**X**t evaluation framework），旨在直接评估检索增强上下文。该框架利用人工编写的摘要来控制知识的信息范围，使我们能够衡量上下文对长篇生成所需信息的覆盖程度。CRUX采用基于问题的评估来以细粒度方式评估RAG的检索。实验结果表明，CRUX提供了更具反映性和诊断性的评估。我们的发现还揭示了当前检索方法存在巨大的改进空间，为推进RAG的检索指明了有前景的方向。我们的数据和代码已公开可用，以支持和推动未来关于检索的研究。"
    },
    {
        "title": "CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2506.19993v1",
        "pub_date": "2025-06-24",
        "summary": "Recommender systems play a pivotal role in providing relevant content to users. With the rapid development of large language models (LLMs), researchers have begun utilizing LLMs to build more powerful recommender systems. However, existing approaches that focus on aligning LLMs with recommendation tasks do not fully leverage their sequential information processing capabilities, leading to suboptimal performance.   In this paper, we propose a novel system called compressed vocabulary expansion (CoVE). In CoVE, each item is assigned a unique ID within the expanded vocabulary. Our framework effectively capitalizes on sequence understanding abilities of LLMs, significantly enhancing their performance on recommendation tasks. Additionally, we compress the embedding layer, making CoVE practical for large-scale industrial applications. The effectiveness and performance of CoVE are demonstrated through comprehensive experiments on multiple recommendation datasets and comparisons with prior works. Our code can be found at https://github.com/HaochenZhang717/CoVE-official-Repo.",
        "translated": "推荐系统在为用户提供相关内容方面发挥着关键作用。随着大语言模型（LLMs）的快速发展，研究人员已开始利用LLMs来构建更强大的推荐系统。然而，现有侧重于将LLMs与推荐任务对齐的方法未能充分利用它们的序列信息处理能力，导致次优性能。\n\n在本文中，我们提出了一种名为压缩词汇扩展（CoVE）的新颖系统。在CoVE中，每个物品在扩展词汇内被赋予一个唯一的ID。我们的框架有效利用了LLMs的序列理解能力，显著增强了它们在推荐任务上的性能。此外，我们压缩了嵌入层，使CoVE在大规模工业应用中具有实用性。CoVE的有效性和性能通过在多个推荐数据集上的全面实验以及与先前工作的比较得到了验证。我们的代码已发布于 https://github.com/HaochenZhang717/CoVE-official-Repo。"
    },
    {
        "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base",
        "url": "http://arxiv.org/abs/2506.20608v1",
        "pub_date": "2025-06-25",
        "summary": "Generative AI, especially through large language models (LLMs), is transforming how technical knowledge can be accessed, reused, and extended. PETSc, a widely used numerical library for high-performance scientific computing, has accumulated a rich but fragmented knowledge base over its three decades of development, spanning source code, documentation, mailing lists, GitLab issues, Discord conversations, technical papers, and more. Much of this knowledge remains informal and inaccessible to users and new developers. To activate and utilize this knowledge base more effectively, the PETSc team has begun building an LLM-powered system that combines PETSc content with custom LLM tools -- including retrieval-augmented generation (RAG), reranking algorithms, and chatbots -- to assist users, support developers, and propose updates to formal documentation. This paper presents initial experiences designing and evaluating these tools, focusing on system architecture, using RAG and reranking for PETSc-specific information, evaluation methodologies for various LLMs and embedding models, and user interface design. Leveraging the Argonne Leadership Computing Facility resources, we analyze how LLM responses can enhance the development and use of numerical software, with an initial focus on scalable Krylov solvers. Our goal is to establish an extensible framework for knowledge-centered AI in scientific software, enabling scalable support, enriched documentation, and enhanced workflows for research and development. We conclude by outlining directions for expanding this system into a robust, evolving platform that advances software ecosystems to accelerate scientific discovery.",
        "translated": "生成式AI，特别是通过大语言模型（LLM），正在改变技术知识的获取、重用和扩展方式。PETSc是一个广泛用于高性能科学计算的数值计算库，在其三十年的发展过程中积累了丰富但碎片化的知识库，这些知识散布于源代码、文档、邮件列表、GitLab问题、Discord对话、技术论文等多个来源。其中大部分知识是非正式的，且用户和新开发者难以获取。\n\n为了更有效地激活和利用这些知识，PETSc团队已着手构建一个基于LLM的系统。该系统将PETSc内容与包括检索增强生成（RAG）、重排序算法和聊天机器人在内的定制LLM工具相结合，旨在协助用户、支持开发人员，并为正式文档的更新提供建议。本文介绍了设计和评估这些工具的初步经验，重点关注系统架构、如何将RAG和重排序应用于PETSc特定信息、针对各种LLM和嵌入模型的评估方法，以及用户界面设计。利用阿贡领导计算设施的资源，我们分析了LLM响应如何增强数值软件的开发和使用，初步侧重于可扩展的Krylov求解器。\n\n我们的目标是为科学软件中的知识中心化AI建立一个可扩展的框架，从而实现可扩展的支持、充实的文档和增强的研发工作流程。最后，我们概述了将该系统扩展为一个强大、不断演进的平台的方向，以期推动软件生态系统发展，加速科学发现。"
    },
    {
        "title": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of\n  Multi-Agent AI for Addressing Sustainable Protein Production Challenges",
        "url": "http://arxiv.org/abs/2506.20598v1",
        "pub_date": "2025-06-25",
        "summary": "The global demand for sustainable protein sources has accelerated the need for intelligent tools that can rapidly process and synthesise domain-specific scientific knowledge. In this study, we present a proof-of-concept multi-agent Artificial Intelligence (AI) framework designed to support sustainable protein production research, with an initial focus on microbial protein sources. Our Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based LLM agents: (1) a literature search agent that retrieves relevant scientific literature on microbial protein production for a specified microbial strain, and (2) an information extraction agent that processes the retrieved content to extract relevant biological and chemical information. Two parallel methodologies, fine-tuning and prompt engineering, were explored for agent optimisation. Both methods demonstrated effectiveness at improving the performance of the information extraction agent in terms of transformer-based cosine similarity scores between obtained and ideal outputs. Mean cosine similarity scores were increased by up to 25%, while universally reaching mean scores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved the mean scores to a greater extent (consistently of $\\geq 0.94$) compared to prompt engineering, although lower statistical uncertainties were observed with the latter approach. A user interface was developed and published for enabling the use of the multi-agent AI system, alongside preliminary exploration of additional chemical safety-based search capabilities",
        "translated": "全球对可持续蛋白质来源的需求加速了对智能工具的需求，这些工具能够快速处理和整合领域特定科学知识。本研究中，我们提出了一个概念验证性的多智能体人工智能（AI）框架，旨在支持可持续蛋白质生产研究，初期重点关注微生物蛋白质来源。我们面向检索增强生成（RAG）的系统由两个基于GPT的大型语言模型（LLM）智能体组成：(1) 一个文献检索智能体，负责为特定微生物菌株检索微生物蛋白质生产相关的科学文献；(2) 一个信息提取智能体，负责处理检索到的内容以提取相关的生物和化学信息。我们探索了两种并行方法——微调和提示工程——用于智能体优化。两种方法都证明能有效提升信息提取智能体的性能，衡量标准是获取输出与理想输出之间基于Transformer的余弦相似度分数。平均余弦相似度分数提升高达25%，并且普遍达到针对理想输出文本的平均分数$\\geq 0.89$。总体而言，微调比提示工程在更大程度上（始终达到$\\geq 0.94$）提升了平均分数，尽管后者方法观察到更低的统计不确定性。我们开发并发布了一个用户界面，以支持该多智能体AI系统的使用，并初步探索了额外的基于化学安全的搜索功能。"
    },
    {
        "title": "MMSearch-R1: Incentivizing LMMs to Search",
        "url": "http://arxiv.org/abs/2506.20670v1",
        "pub_date": "2025-06-25",
        "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search.",
        "translated": "大型多模态模型（LMMs）在现实世界场景中的鲁棒部署，鉴于现实世界信息的复杂性和动态性，需要访问外部知识源。现有方法，如检索增强生成（RAG）和基于提示工程的搜索智能体，依赖于固定的流程，往往导致低效或过度的搜索行为。\n\n我们提出了 MMSearch-R1，这是首个端到端强化学习框架，它使大型多模态模型（LMMs）能够在现实世界的互联网环境中执行按需、多轮搜索。我们的框架整合了图像和文本搜索工具，允许模型在基于结果的奖励和搜索惩罚的指导下，推断何时以及如何调用这些工具。为支持训练，我们通过半自动化流程收集了一个多模态搜索VQA数据集，该数据集涵盖了多样化的视觉和文本知识需求，并精心策划了一个搜索平衡子集，其中包含既需要搜索又无需搜索的样本，这对于塑造高效和按需的搜索行为至关重要。\n\n在知识密集型和信息搜索型VQA任务上进行的大量实验表明，我们的模型不仅优于相同模型规模的基于RAG的基线模型，而且与更大规模的基于RAG的模型性能相当，同时将搜索调用次数减少了30%以上。我们进一步分析了关键的实证发现，以期为促进多模态搜索领域的研究提供可行的见解。"
    },
    {
        "title": "Memento: Note-Taking for Your Future Self",
        "url": "http://arxiv.org/abs/2506.20642v1",
        "pub_date": "2025-06-25",
        "summary": "Large language models (LLMs) excel at reasoning-only tasks, but struggle when reasoning must be tightly coupled with retrieval, as in multi-hop question answering. To overcome these limitations, we introduce a prompting strategy that first decomposes a complex question into smaller steps, then dynamically constructs a database of facts using LLMs, and finally pieces these facts together to solve the question. We show how this three-stage strategy, which we call Memento, can boost the performance of existing prompting strategies across diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the performance of chain-of-thought (CoT) when all information is provided in context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1 percentage points, demonstrating its utility in agentic settings.",
        "translated": "大型语言模型 (LLMs) 擅长纯推理任务，但在推理必须与检索紧密耦合时（例如在多跳问答中）表现不佳。为了克服这些局限性，我们引入了一种提示策略，该策略首先将复杂问题分解为更小的步骤，然后利用LLM动态构建一个事实数据库，最后将这些事实整合起来以解决问题。我们表明，这种我们称之为 Memento 的三阶段策略，能够在各种不同设置下提升现有提示策略的性能。在9步 PhantomWiki 基准测试中，当所有信息都在上下文中提供时，Memento 使思维链 (CoT) 的性能翻倍。在 2WikiMultiHopQA 的开放域版本上，结合 Memento 的 CoT-RAG 比基础版 CoT-RAG 的 F1 分数提高了 20 多个百分点，并且比多跳 RAG 基线 IRCoT 的 F1 分数提高了 13 多个百分点。在具有挑战性的 MuSiQue 数据集上，Memento 使 ReAct 的 F1 分数提高了 3 多个百分点，这表明了其在代理设置中的实用性。"
    },
    {
        "title": "Maximal Matching Matters: Preventing Representation Collapse for Robust\n  Cross-Modal Retrieval",
        "url": "http://arxiv.org/abs/2506.21538v1",
        "pub_date": "2025-06-26",
        "summary": "Cross-modal image-text retrieval is challenging because of the diverse possible associations between content from different modalities. Traditional methods learn a single-vector embedding to represent semantics of each sample, but struggle to capture nuanced and diverse relationships that can exist across modalities. Set-based approaches, which represent each sample with multiple embeddings, offer a promising alternative, as they can capture richer and more diverse relationships. In this paper, we show that, despite their promise, these set-based representations continue to face issues including sparse supervision and set collapse, which limits their effectiveness. To address these challenges, we propose Maximal Pair Assignment Similarity to optimize one-to-one matching between embedding sets which preserve semantic diversity within the set. We also introduce two loss functions to further enhance the representations: Global Discriminative Loss to enhance distinction among embeddings, and Intra-Set Divergence Loss to prevent collapse within each set. Our method achieves state-of-the-art performance on MS-COCO and Flickr30k without relying on external data.",
        "translated": "跨模态图文检索具有挑战性，原因在于不同模态内容之间可能存在的多样化关联。传统方法学习一个单一向量嵌入来表示每个样本的语义，但难以捕捉跨模态之间可能存在的细致入微且多样化的关系。集合式方法通过使用多个嵌入来表示每个样本，提供了一种有前景的替代方案，因为它们能够捕获更丰富、更多样化的关系。在本文中，我们指出，尽管集合式表示具有潜力，但它们仍面临稀疏监督和集合坍缩等问题，从而限制了其有效性。为了解决这些挑战，我们提出了“最大对分配相似度 (Maximal Pair Assignment Similarity)”，旨在优化嵌入集合之间的一对一匹配，以保持集合内部的语义多样性。我们还引入了两个损失函数以进一步增强这些表示：全局判别损失 (Global Discriminative Loss, GDL) 用于增强嵌入之间的区分度，以及集合内散度损失 (Intra-Set Divergence Loss, ISDL) 用于防止每个集合内部发生坍缩。我们的方法在 MS-COCO 和 Flickr30k 数据集上实现了最先进的性能，且无需依赖外部数据。"
    },
    {
        "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond\n  English",
        "url": "http://arxiv.org/abs/2506.21445v1",
        "pub_date": "2025-06-26",
        "summary": "Recent advances in large language models have enabled natural language interfaces that translate user questions into database queries, such as Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database accessibility, most research today focuses solely on English, with limited evaluation in other languages. This paper investigates the performance of foundational LLMs on the Text2Cypher task across multiple languages. We create and release a multilingual test set by translating English questions into Spanish and Turkish while preserving the original Cypher queries, enabling fair cross-lingual comparison. We evaluate multiple foundational models using standardized prompts and metrics. Our results show a consistent performance pattern: highest on English, then Spanish, and lowest on Turkish. We attribute this to differences in training data availability and linguistic characteristics. Additionally, we explore the impact of translating task prompts into Spanish and Turkish. Results show little to no change in evaluation metrics, suggesting prompt translation has minor impact. Our findings highlight the need for more inclusive evaluation and development in multilingual query generation. Future work includes schema localization and fine-tuning across diverse languages.",
        "translated": "大语言模型的最新进展催生了将用户问题转化为数据库查询的自然语言接口，例如 Text2SQL、Text2SPARQL 和 Text2Cypher。尽管这些接口提升了数据库的可访问性，但当前大多数研究仍局限于英语，对其他语言的评估非常有限。本文探讨了基础大语言模型在多语言环境下执行 Text2Cypher 任务的性能。我们构建并发布了一个多语言测试集，其方法是将英语问题翻译成西班牙语和土耳其语，同时保留原始的 Cypher 查询，这使得公平的跨语言比较成为可能。我们使用标准化的提示和指标评估了多个基础模型。我们的结果显示出一致的性能模式：英语表现最佳，其次是西班牙语，土耳其语表现最差。我们将其归因于训练数据可用性和语言特征的差异。此外，我们还探究了将任务提示词翻译成西班牙语和土耳其语的影响。结果显示评估指标几乎没有变化，这表明提示词翻译的影响甚微。我们的研究结果凸显了在多语言查询生成方面，进行更具包容性的评估和开发的必要性。未来工作包括模式本地化和针对不同语言的微调。"
    },
    {
        "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2506.21384v1",
        "pub_date": "2025-06-26",
        "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant challenges when processing user queries that are often noisy, ambiguous, and contain multiple intents. While RAG enhances large language models (LLMs) with external knowledge, current systems typically struggle with such complex inputs, as they are often trained or evaluated on cleaner data. This paper introduces Omni-RAG, a novel framework designed to improve the robustness and effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs LLM-assisted query understanding to preprocess user inputs through three key modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs with tailored prompts to denoise queries (e.g., correcting spelling errors) and decompose multi-intent queries into structured sub-queries; (2) Intent-Aware Knowledge Retrieval, which performs retrieval for each sub-query from a corpus (i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking and Generation, where a reranker (i.e., BGE) refines document selection before a final response is generated by an LLM (i.e., Falcon-10B) using a chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG capabilities and the demands of real-world applications, such as those highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex and noisy queries.",
        "translated": "实时检索增强生成（RAG）系统在处理用户查询时面临重大挑战，这些查询通常嘈杂、模糊且包含多重意图。尽管RAG通过外部知识增强了大型语言模型（LLM），但当前系统通常难以处理此类复杂输入，因为它们通常是在更干净的数据上进行训练或评估的。本文介绍了Omni-RAG，这是一个新颖的框架，旨在提高RAG系统在实时、开放域环境中的鲁棒性和有效性。Omni-RAG采用LLM辅助的查询理解，通过三个关键模块对用户输入进行预处理：(1) **深度查询理解与分解**：该模块利用LLM结合定制化提示（prompts）对查询进行去噪（例如，纠正拼写错误），并将多意图查询分解为结构化的子查询；(2) **意图感知知识检索**：该模块从语料库（即使用OpenSearch的FineWeb）中对每个子查询执行检索，并聚合结果；(3) **重排序与生成**：该模块使用重排序器（即BGE）精炼文档选择，然后由LLM（即Falcon-10B）利用思维链提示（prompt）生成最终响应。Omni-RAG旨在通过鲁棒地处理复杂和嘈杂的查询，弥合当前RAG能力与真实世界应用需求之间的差距，例如SIGIR 2025 LiveRAG挑战赛所强调的需求。"
    },
    {
        "title": "Real-time and personalized product recommendations for large e-commerce\n  platforms",
        "url": "http://arxiv.org/abs/2506.21368v1",
        "pub_date": "2025-06-26",
        "summary": "We present a methodology to provide real-time and personalized product recommendations for large e-commerce platforms, specifically focusing on fashion retail. Our approach aims to achieve accurate and scalable recommendations with minimal response times, ensuring user satisfaction, leveraging Graph Neural Networks and parsimonious learning methodologies. Extensive experimentation with datasets from one of the largest e-commerce platforms demonstrates the effectiveness of our approach in forecasting purchase sequences and handling multi-interaction scenarios, achieving efficient personalized recommendations under real-world constraints.",
        "translated": "我们提出了一种方法，旨在为大型电商平台，尤其是时尚零售领域，提供实时且个性化的商品推荐。我们的方法利用图神经网络（GNNs）和节俭学习方法，旨在实现准确、可扩展且响应时间极短的推荐，从而确保用户满意度。我们利用来自一家最大的电商平台之一的数据集进行了广泛实验，结果表明，我们的方法在预测购买序列和处理多交互场景方面表现出有效性，并在真实世界约束下实现了高效的个性化推荐。"
    },
    {
        "title": "Enhancing Automatic Term Extraction with Large Language Models via\n  Syntactic Retrieval",
        "url": "http://arxiv.org/abs/2506.21222v1",
        "pub_date": "2025-06-26",
        "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that are crucial for downstream tasks such as machine translation and information retrieval. Although large language models (LLMs) have significantly advanced various NLP tasks, their potential for ATE has scarcely been examined. We propose a retrieval-based prompting strategy that, in the few-shot setting, selects demonstrations according to \\emph{syntactic} rather than semantic similarity. This syntactic retrieval method is domain-agnostic and provides more reliable guidance for capturing term boundaries. We evaluate the approach in both in-domain and cross-domain settings, analyzing how lexical overlap between the query sentence and its retrieved examples affects performance. Experiments on three specialized ATE benchmarks show that syntactic retrieval improves F1-score. These findings highlight the importance of syntactic cues when adapting LLMs to terminology-extraction tasks.",
        "translated": "自动术语提取（ATE）旨在识别领域特定表达，这些表达对于机器翻译和信息检索等下游任务至关重要。尽管大型语言模型（LLM）已显著推动了各种自然语言处理（NLP）任务的发展，但其在自动术语提取（ATE）方面的潜力却鲜有探究。\n\n本文提出了一种基于检索的提示策略，该策略在少样本设置下，根据*句法*相似性而非语义相似性来选择示例。这种句法检索方法是领域无关的，并能为捕获术语边界提供更可靠的指导。我们在域内和跨域设置下对该方法进行了评估，并分析了查询语句与所检索示例之间的词汇重叠度如何影响性能。在三个专门的自动术语提取基准数据集上的实验表明，句法检索能够提升F1分数。这些发现凸显了在将大型语言模型应用于术语提取任务时，句法线索的重要性。"
    },
    {
        "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query\n  Classification",
        "url": "http://arxiv.org/abs/2506.21049v1",
        "pub_date": "2025-06-26",
        "summary": "Query classification, including multiple subtasks such as intent and category prediction, is vital to e-commerce applications. E-commerce queries are usually short and lack context, and the information between labels cannot be used, resulting in insufficient prior information for modeling. Most existing industrial query classification methods rely on users' posterior click behavior to construct training samples, resulting in a Matthew vicious cycle. Furthermore, the subtasks of query classification lack a unified framework, leading to low efficiency for algorithm optimization.   In this paper, we propose a novel Semi-supervised Scalable Unified Framework (SSUF), containing multiple enhanced modules to unify the query classification tasks. The knowledge-enhanced module uses world knowledge to enhance query representations and solve the problem of insufficient query information. The label-enhanced module uses label semantics and semi-supervised signals to reduce the dependence on posterior labels. The structure-enhanced module enhances the label representation based on the complex label relations. Each module is highly pluggable, and input features can be added or removed as needed according to each subtask. We conduct extensive offline and online A/B experiments, and the results show that SSUF significantly outperforms the state-of-the-art models.",
        "translated": "查询分类，包括意图预测和类别预测等多个子任务，对电商应用至关重要。电商查询通常较短且缺乏上下文，标签间信息无法利用，导致建模的先验信息不足。大多数现有工业界查询分类方法依赖用户的后验点击行为来构建训练样本，导致马太效应式的恶性循环。此外，查询分类的子任务缺乏统一框架，导致算法优化效率低下。\n\n在本文中，我们提出了一种新颖的半监督可扩展统一框架（SSUF），其中包含多个增强模块，以统一查询分类任务。知识增强模块利用世界知识增强查询表示，解决查询信息不足的问题。标签增强模块利用标签语义和半监督信号，减少对后验标签的依赖。结构增强模块基于复杂的标签关系增强标签表示。每个模块都具有高度可插拔性，可以根据每个子任务的需要增删输入特征。我们进行了大量的离线和在线A/B实验，结果表明SSUF显著优于现有最先进的模型。"
    },
    {
        "title": "RecCoT: Enhancing Recommendation via Chain-of-Thought",
        "url": "http://arxiv.org/abs/2506.21032v1",
        "pub_date": "2025-06-26",
        "summary": "In real-world applications, users always interact with items in multiple aspects, such as through implicit binary feedback (e.g., clicks, dislikes, long views) and explicit feedback (e.g., comments, reviews). Modern recommendation systems (RecSys) learn user-item collaborative signals from these implicit feedback signals as a large-scale binary data-streaming, subsequently recommending other highly similar items based on users' personalized historical interactions. However, from this collaborative-connection perspective, the RecSys does not focus on the actual content of the items themselves but instead prioritizes higher-probability signals of behavioral co-occurrence among items. Consequently, under this binary learning paradigm, the RecSys struggles to understand why a user likes or dislikes certain items. To alleviate it, some works attempt to utilize the content-based reviews to capture the semantic knowledge to enhance recommender models. However, most of these methods focus on predicting the ratings of reviews, but do not provide a human-understandable explanation.",
        "translated": "在实际应用中，用户总是通过多个方面与物品进行交互，例如隐式二元反馈（如点击、不喜欢、长时间观看）以及显式反馈（如评论、评价）。现代推荐系统（RecSys）从这些隐式反馈信号中以大规模二元数据流的形式学习用户-物品协同信号，进而基于用户个性化的历史交互推荐其他高度相似的物品。然而，从这种协同关联的角度来看，推荐系统并不侧重于物品本身的实际内容，而是优先考虑物品间行为共现的高概率信号。因此，在这种二元学习范式下，推荐系统难以理解用户为什么喜欢或不喜欢某些物品。为了缓解这个问题，一些工作尝试利用基于内容的评论来捕获语义知识，以增强推荐模型。然而，这些方法大多侧重于预测评论的评分，但未能提供人类可理解的解释。"
    },
    {
        "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge",
        "url": "http://arxiv.org/abs/2506.21506v1",
        "pub_date": "2025-06-26",
        "summary": "Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems.",
        "translated": "智能体搜索，例如深度研究系统（Deep Research systems），其中大语言模型能够自主浏览网页、综合信息并返回全面且有引用支持的答案，代表了用户与网络规模信息交互方式的重大转变。尽管智能体搜索有望带来更高的效率和认知卸载，但其日益增长的复杂性和开放性已超越了现有的评估基准和方法。这些现有方法主要假设搜索范围较短且答案是静态的。\n\n在本文中，我们引入了Mind2Web 2，这是一个包含130个真实、高质量、长周期任务的基准，这些任务需要实时网页浏览和广泛的信息综合，其构建耗费了超过1,000小时的人工投入。为了解决评估时变且复杂答案的挑战，我们提出了一种新颖的“智能体即评判者”（Agent-as-a-Judge）框架。我们的方法基于树状结构评分标准设计构建任务特定的评判智能体，以自动评估答案的正确性和来源归因。\n\n我们对九个前沿智能体搜索系统和人类表现进行了全面评估，并进行了详细的错误分析，旨在为未来的发展提供见解。表现最佳的系统——OpenAI深度研究系统（OpenAI Deep Research）——已能达到人类表现的50-70%，同时只花费一半时间，展现出巨大的潜力。总而言之，Mind2Web 2为开发和基准测试下一代智能体搜索系统提供了严谨的基础。"
    },
    {
        "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness",
        "url": "http://arxiv.org/abs/2506.21288v1",
        "pub_date": "2025-06-26",
        "summary": "Augmenting large language models (LLMs) with external context significantly improves their performance in natural language processing (NLP) tasks. However, LLMs struggle to answer queries reliably when the provided context lacks information, often resorting to ungrounded speculation or internal knowledge. Groundedness - generating responses strictly supported by the context - is essential for ensuring factual consistency and trustworthiness. This study focuses on detecting whether a given query is grounded in a document provided in context before the costly answer generation by LLMs. Such a detection mechanism can significantly reduce both inference time and resource consumption. We show that lightweight, task specific encoder models such as RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in groundedness detection while reducing inference latency by orders of magnitude. The code is available at : https://github.com/chandarlab/Hallucinate-less",
        "translated": "通过外部上下文增强大型语言模型（LLMs）可以显著提升其在自然语言处理（NLP）任务中的性能。然而，当提供的上下文信息不足时，LLMs 难以可靠地回答查询，常常诉诸于无根据的推测或其内部知识。忠实性——即生成严格由上下文支持的响应——对于确保事实一致性和可信赖性至关重要。本研究着重于在 LLMs 进行耗时的答案生成之前，检测给定查询是否能在所提供的文档上下文中得到支持（即是否具备忠实性）。这种检测机制可以显著减少推理时间和资源消耗。我们研究显示，轻量级、任务专用的编码器模型（例如 RoBERTa 和 NomicBERT），在精心策划的数据集上进行微调后，在忠实性检测方面可以达到与 Llama3 8B 和 GPT4o 等最先进的 LLMs 相媲美的准确性，同时将推理延迟降低数个数量级。代码已开源于：https://github.com/chandarlab/Hallucinate-less"
    },
    {
        "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and\n  Measurement",
        "url": "http://arxiv.org/abs/2506.22372v1",
        "pub_date": "2025-06-27",
        "summary": "The presence of social biases in Natural Language Processing (NLP) and Information Retrieval (IR) systems is an ongoing challenge, which underlines the importance of developing robust approaches to identifying and evaluating such biases. In this paper, we aim to address this issue by leveraging Large Language Models (LLMs) to detect and measure gender bias in passage ranking. Existing gender fairness metrics rely on lexical- and frequency-based measures, leading to various limitations, e.g., missing subtle gender disparities. Building on our LLM-based gender bias detection method, we introduce a novel gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to address existing limitations. To measure the effectiveness of our proposed metric and study LLMs' effectiveness in detecting gender bias, we annotate a subset of the MS MARCO Passage Ranking collection and release our new gender bias collection, called MSMGenderBias, to foster future research in this area. Our extensive experimental results on various ranking models show that our proposed metric offers a more detailed evaluation of fairness compared to previous metrics, with improved alignment to human labels (58.77% for Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa agreement), effectively distinguishing gender bias in ranking. By integrating LLM-driven bias detection, an improved fairness metric, and gender bias annotations for an established dataset, this work provides a more robust framework for analyzing and mitigating bias in IR systems.",
        "translated": "自然语言处理（NLP）和信息检索（IR）系统中社会偏见的存在是一个持续存在的挑战，这凸显了开发鲁棒方法来识别和评估此类偏见的重要性。本文旨在通过利用大语言模型（LLM）来检测和测量段落排序中的性别偏见，从而解决这一问题。现有的性别公平性度量依赖于基于词汇和频率的度量，导致诸多局限性，例如无法捕捉到细微的性别差异。在我们基于LLM的性别偏见检测方法的基础上，我们引入了一种名为“类别加权曝光”（Class-wise Weighted Exposure, CWEx）的新型性别公平性度量，旨在解决现有局限性。为了衡量我们提出的度量的有效性并研究LLM在检测性别偏见方面的有效性，我们对MS MARCO 段落排序数据集的一个子集进行了标注，并发布了我们新的性别偏见数据集MSMGenderBias，以促进该领域的未来研究。我们在各种排序模型上的广泛实验结果表明，与现有度量相比，我们提出的度量提供了更细致的公平性评估，并且与人工标注的对齐度得到提升（使用Cohen's Kappa一致性系数衡量，Grep-BiasIR数据集为58.77%，MSMGenderBias数据集为18.51%），从而有效地区分了排序中的性别偏见。通过整合LLM驱动的偏见检测、改进的公平性度量以及对成熟数据集的性别偏见标注，这项工作为分析和缓解信息检索系统中的偏见提供了一个更鲁棒的框架。"
    },
    {
        "title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval",
        "url": "http://arxiv.org/abs/2506.22356v1",
        "pub_date": "2025-06-27",
        "summary": "The HLTCOE LiveRAG submission utilized the GPT-researcher framework for researching the context of the question, filtering the returned results, and generating the final answer. The retrieval system was a ColBERT bi-encoder architecture, which represents a passage with many dense tokens. Retrieval used a local, compressed index of the FineWeb10-BT collection created with PLAID-X, using a model fine-tuned for multilingual retrieval. Query generation from context was done with Qwen2.5-7B-Instruct, while filtering was accomplished with m2-bert-80M-8k-retrieval. Up to nine passages were used as context to generate an answer using Falcon3-10B. This system placed 5th in the LiveRAG automatic evaluation for correctness with a score of 1.07.",
        "translated": "HLTCOE LiveRAG 参赛系统采用了 GPT-researcher 框架，用于研究问题的上下文、筛选返回结果以及生成最终答案。检索系统采用了 ColBERT 双编码器架构，该架构使用多个稠密标记表示文本段。检索利用了使用 PLAID-X 创建的 FineWeb10-BT 数据集的本地压缩索引，并采用了针对多语言检索微调的模型。从上下文生成查询通过 Qwen2.5-7B-Instruct 完成，而筛选则通过 m2-bert-80M-8k-retrieval 实现。生成答案时，最多使用了九个文本段作为上下文，并通过 Falcon3-10B 模型完成。该系统在 LiveRAG 自动评估的正确性方面位列第五，取得了 1.07 的分数。"
    },
    {
        "title": "Education-Oriented Graph Retrieval-Augmented Generation for Learning\n  Path Recommendation",
        "url": "http://arxiv.org/abs/2506.22303v1",
        "pub_date": "2025-06-27",
        "summary": "Learning path recommendation seeks to provide learners with a structured sequence of learning items (e.g., knowledge concepts or exercises) to optimize their learning efficiency. Despite significant efforts in this area, most existing methods primarily rely on prerequisite relationships, which present two major limitations: 1) Many educational datasets do not explicitly provide prerequisite relationships between knowledge concepts, hindering the application of current learning path recommendation methods. 2) Relying solely on prerequisite relationships as the sole knowledge structure can impede learning progress and negatively impact student outcomes. To address these challenges, we propose a novel approach, Discrimination Learning Enhances Learning Path Recommendation (DLELP), which enhances learning path recommendations by incorporating both prerequisite and similarity relationships between knowledge concepts. Specifically, we introduce a knowledge concept structure graph generation module that adaptively constructs knowledge concept structure graphs for different educational datasets, significantly improving the generalizability of learning path recommendation methods. We then propose a Discrimination Learning-driven Reinforcement Learning (DLRL) framework, which mitigates the issue of blocked learning paths, further enhancing the efficacy of learning path recommendations. Finally, we conduct extensive experiments on three benchmark datasets, demonstrating that our method not only achieves state-of-the-art performance but also provides interpretable reasoning for the recommended learning paths.",
        "translated": "学习路径推荐旨在为学习者提供结构化的学习项目（例如，知识概念或练习）序列，以优化其学习效率。尽管在该领域已进行了大量研究，但大多数现有方法主要依赖于先决条件关系，这存在两大主要局限性：1）许多教育数据集并未明确提供知识概念之间的先决条件关系，这阻碍了当前学习路径推荐方法的应用。2）仅依赖先决条件关系作为唯一的知识结构会阻碍学习进度，并对学生学习成果产生负面影响。\n\n为解决这些挑战，我们提出了一种新颖的方法——判别学习增强学习路径推荐（Discrimination Learning Enhances Learning Path Recommendation, DLELP），该方法通过整合知识概念之间的先决条件和相似性关系来增强学习路径推荐。具体而言，我们引入了一个知识概念结构图生成模块，该模块能够自适应地针对不同的教育数据集构建知识概念结构图，显著提高了学习路径推荐方法的泛化能力。接着，我们提出了一个判别学习驱动的强化学习（Discrimination Learning-driven Reinforcement Learning, DLRL）框架，该框架缓解了学习路径受阻的问题，进一步提升了学习路径推荐的有效性。最后，我们在三个基准数据集上进行了广泛的实验，结果表明我们的方法不仅实现了最先进的性能，而且为推荐的学习路径提供了可解释的推理。"
    },
    {
        "title": "JointRank: Rank Large Set with Single Pass",
        "url": "http://arxiv.org/abs/2506.22262v1",
        "pub_date": "2025-06-27",
        "summary": "Efficiently ranking relevant items from large candidate pools is a cornerstone of modern information retrieval systems -- such as web search, recommendation, and retrieval-augmented generation. Listwise rerankers, which improve relevance by jointly considering multiple candidates, are often limited in practice: either by model input size constraints, or by degraded quality when processing large sets. We propose a model-agnostic method for fast reranking large sets that exceed a model input limits. The method first partitions candidate items into overlapping blocks, each of which is ranked independently in parallel. Implicit pairwise comparisons are then derived from these local rankings. Finally, these comparisons are aggregated to construct a global ranking using algorithms such as Winrate or PageRank. Experiments on TREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the 57.68 for full-context listwise approach using gpt-4.1-mini as long-context model, while reducing latency from 21 to 8 seconds.   The implementation of the algorithm and the experiments is available in the repository: https://github.com/V3RGANz/jointrank",
        "translated": "高效地从大规模候选集中对相关条目进行排序，是现代信息检索系统（如网络搜索、推荐系统和检索增强生成）的基石。列表式重排序器通过联合考虑多个候选来提高相关性，但在实践中常受到限制：要么受限于模型输入尺寸，要么在处理大型数据集时导致质量下降。我们提出了一种模型无关的方法，用于快速重排序超出模型输入限制的大型数据集。该方法首先将候选条目划分为重叠的块，每个块独立并行地进行排序。随后，从这些局部排序中导出隐式成对比较。最后，聚合这些比较以使用诸如Winrate或PageRank等算法构建一个全局排序。在TREC DL-2019上的实验表明，我们的方法实现了70.88的nDCG@10，而使用gpt-4.1-mini作为长上下文模型的全上下文列表式方法的nDCG@10为57.68，同时将延迟从21秒减少到8秒。该算法的实现和实验代码已在以下仓库中开源：https://github.com/V3RGANz/jointrank"
    },
    {
        "title": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation\n  of Responses",
        "url": "http://arxiv.org/abs/2506.22210v1",
        "pub_date": "2025-06-27",
        "summary": "Retrieval-augmented generation (RAG) faces challenges related to factual correctness, source attribution, and response completeness. The LiveRAG Challenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus and a shared, open-source LLM. We propose a modular pipeline that operates on information nuggets-minimal, atomic units of relevant information extracted from retrieved documents. This multistage pipeline encompasses query rewriting, passage retrieval and reranking, nugget detection and clustering, cluster ranking and summarization, and response fluency enhancement. This design inherently promotes grounding in specific facts, facilitates source attribution, and ensures maximum information inclusion within length constraints. In this challenge, we extend our focus to also address the retrieval component of RAG, building upon our prior work on multi-faceted query rewriting. Furthermore, for augmented generation, we concentrate on improving context curation capabilities, maximizing the breadth of information covered in the response while ensuring pipeline efficiency. Our results show that combining original queries with a few sub-query rewrites boosts recall, while increasing the number of documents used for reranking and generation beyond a certain point reduces effectiveness, without improving response quality.",
        "translated": "检索增强生成（RAG）面临着事实准确性、来源归因和回复完整性等方面的挑战。SIGIR'25 举办的 LiveRAG 挑战赛旨在利用固定语料库和共享的开源大型语言模型，推动 RAG 研究的进展。我们提出了一种模块化流水线，该流水线基于从检索到的文档中提取的“信息片段”（即最小的、原子化的相关信息单元）进行操作。这一多阶段流水线涵盖了查询重写、片段检索和重排、信息片段检测和聚类、聚类排序和摘要生成，以及回复流畅性增强。这种设计本质上促进了对特定事实的支撑，便于来源归因，并确保在长度限制内最大程度地涵盖信息。在本次挑战赛中，我们扩大了关注范围，旨在解决 RAG 的检索部分，并借鉴了我们先前在多方面查询重写方面的工作。此外，对于增强生成，我们专注于提升上下文编排能力，在确保流水线效率的同时，最大化回复中信息覆盖的广度。我们的结果表明，将原始查询与少量子查询重写相结合可以提升召回率，而用于重排和生成的文档数量超过某个点后，反而会降低有效性，且不提高回复质量。"
    },
    {
        "title": "The Missing Link: Joint Legal Citation Prediction using Heterogeneous\n  Graph Enrichment",
        "url": "http://arxiv.org/abs/2506.22165v1",
        "pub_date": "2025-06-27",
        "summary": "Legal systems heavily rely on cross-citations of legal norms as well as previous court decisions. Practitioners, novices and legal AI systems need access to these relevant data to inform appraisals and judgments. We propose a Graph-Neural-Network (GNN) link prediction model that can identify Case-Law and Case-Case citations with high proficiency through fusion of semantic and topological information. We introduce adapted relational graph convolutions operating on an extended and enriched version of the original citation graph that allow the topological integration of semantic meta-information. This further improves prediction by 3.1 points of average precision and by 8.5 points in data sparsity as well as showing robust performance over time and in challenging fully inductive prediction. Jointly learning and predicting case and norm citations achieves a large synergistic effect that improves case citation prediction by up to 4.7 points, at almost doubled efficiency.",
        "translated": "法律系统高度依赖法律规范以及过往判例的相互引用。从业者、新手以及法律AI系统都需要获取这些相关数据，以指导其进行评估和判断。为此，我们提出了一种图神经网络（GNN）链接预测模型，该模型通过融合语义和拓扑信息，能够高效准确地识别判例法引用（Case-Law citations）和判例间引用（Case-Case citations）。我们引入了改进的关系图卷积，其作用于原始引用图的扩展和丰富版本，从而实现了语义元信息的拓扑整合。这进一步将预测的平均精度（average precision）提高了3.1个百分点，并将数据稀疏性方面的性能提升了8.5个百分点。同时，该模型在时间维度上以及在具有挑战性的完全归纳预测（fully inductive prediction）任务中均表现出稳健的性能。联合学习并预测判例和规范引用产生了显著的协同效应，使判例引用预测的性能提高了多达4.7个百分点，同时效率几乎翻倍。"
    },
    {
        "title": "DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family\n  Level",
        "url": "http://arxiv.org/abs/2506.22141v1",
        "pub_date": "2025-06-27",
        "summary": "In the landscape of publicly available patent retrieval datasets, the need for explicit indomain and out-of-domain labeling, multi-jurisdiction coverage, balanced query domain representation and manageable sizes that support sub document level experiments on moderate computational resources is often overlooked. To address these gaps, we propose DAPFAM, a new open access domain-aware patent retrieval dataset constructed at the simple-family level. The dataset contains 1,247 domain balanced full text query families and 45,336 full text target families. The dataset is enriched by clear relevance judgments (forward/backward citations as positive links, random negatives), as well as explicit in-domain or out-of-domain relationships via a novel proposed labelling scheme based on via International Patent Classification (IPC) codes, resulting in 49,869 evaluation pairs. The dataset is multi jurisdictional, requires little to no preprocessing for retrieval evaluation, and remains of a size manageable for entities with limited ressources allowing for sub document level retrieval experiments without excessive computational costs. We describe our three-step data-curation pipeline, present comprehensive dataset statistics, and provide baseline experiments using lexical and neural retrieval methods. Our baseline experiments highlight significant challenges in crossdomain patent retrieval. The dataset will be publicly available (for now the access link is this repository: https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).",
        "translated": "在现有公开专利检索数据集领域中，对于明确的域内和域外标注、多司法管辖区覆盖、平衡的查询领域表示以及支持在适中计算资源上进行子文档级实验的可管理规模的需求常常被忽视。为解决这些空白，我们提出了DAPFAM，这是一个构建在专利简单同族级别上的新型开放获取领域感知专利检索数据集。\n\n该数据集包含1,247个领域平衡的全文查询同族和45,336个全文目标同族。该数据集通过明确的相关性判断（将正向和反向引用作为正向链接，随机负样本作为负向链接），以及通过一种基于国际专利分类（IPC）代码的新颖标注方案，明确了域内或域外关系，从而生成了49,869个评估对。该数据集是多司法管辖区的，进行检索评估时几乎无需预处理，并且其规模对于资源有限的实体来说仍是可管理的，使得在不产生过高计算成本的情况下进行子文档级检索实验成为可能。\n\n我们描述了我们的三步数据整理流程，提供了全面的数据集统计信息，并使用词法和神经检索方法进行了基线实验。我们的基线实验突显了跨领域专利检索中的重大挑战。该数据集将公开发布（目前可通过此仓库链接访问：https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b）。"
    },
    {
        "title": "Reward Balancing Revisited: Enhancing Offline Reinforcement Learning for\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2506.22112v1",
        "pub_date": "2025-06-27",
        "summary": "Offline reinforcement learning (RL) has emerged as a prevalent and effective methodology for real-world recommender systems, enabling learning policies from historical data and capturing user preferences. In offline RL, reward shaping encounters significant challenges, with past efforts to incorporate prior strategies for uncertainty to improve world models or penalize underexplored state-action pairs. Despite these efforts, a critical gap remains: the simultaneous balancing of intrinsic biases in world models and the diversity of policy recommendations. To address this limitation, we present an innovative offline RL framework termed Reallocated Reward for Recommender Systems (R3S). By integrating inherent model uncertainty to tackle the intrinsic fluctuations in reward predictions, we boost diversity for decision-making to align with a more interactive paradigm, incorporating extra penalizers with decay that deter actions leading to diminished state variety at both local and global scales. The experimental results demonstrate that R3S improves the accuracy of world models and efficiently harmonizes the heterogeneous preferences of the users.",
        "translated": "离线强化学习（RL）已成为真实世界推荐系统中的一种普遍且有效的方法论，能够从历史数据中学习策略并捕捉用户偏好。在离线强化学习中，奖励塑形面临显著挑战，过去的工作致力于结合处理不确定性的先验策略以改进世界模型，或惩罚探索不足的状态-动作对。尽管付出了这些努力，但仍存在一个关键空白：如何同时平衡世界模型中固有的偏差和策略推荐的多样性。\n\n为解决此局限，我们提出了一个创新的离线强化学习框架，命名为推荐系统重分配奖励（R3S）。通过整合固有的模型不确定性以应对奖励预测中的内在波动，我们提升了决策的多样性，使其契合更具交互性的范式。同时，我们引入了带有衰减的额外惩罚项，以阻止在局部和全局尺度上导致状态多样性降低的动作。实验结果表明，R3S 提高了世界模型的准确性，并有效地协调了用户的异构偏好。"
    },
    {
        "title": "Literature-Grounded Novelty Assessment of Scientific Ideas",
        "url": "http://arxiv.org/abs/2506.22026v1",
        "pub_date": "2025-06-27",
        "summary": "Automated scientific idea generation systems have made remarkable progress, yet the automatic evaluation of idea novelty remains a critical and underexplored challenge. Manual evaluation of novelty through literature review is labor-intensive, prone to error due to subjectivity, and impractical at scale. To address these issues, we propose the Idea Novelty Checker, an LLM-based retrieval-augmented generation (RAG) framework that leverages a two-stage retrieve-then-rerank approach. The Idea Novelty Checker first collects a broad set of relevant papers using keyword and snippet-based retrieval, then refines this collection through embedding-based filtering followed by facet-based LLM re-ranking. It incorporates expert-labeled examples to guide the system in comparing papers for novelty evaluation and in generating literature-grounded reasoning. Our extensive experiments demonstrate that our novelty checker achieves approximately 13% higher agreement than existing approaches. Ablation studies further showcases the importance of the facet-based re-ranker in identifying the most relevant literature for novelty evaluation.",
        "translated": "自动化科学思想生成系统已取得显著进展，然而，思想新颖性的自动评估仍是一个关键且未充分探索的挑战。通过文献综述进行人工新颖性评估是劳动密集型的，易因主观性而产生误差，且难以大规模实施。为解决这些问题，我们提出了一种名为“思想新颖性检查器”（Idea Novelty Checker）的框架，它是一个基于大型语言模型（LLM）的检索增强生成（RAG）框架，采用两阶段的“检索-重排序”方法。思想新颖性检查器首先利用关键词和片段检索收集广泛的相关论文，然后通过基于嵌入的过滤以及基于方面的LLM重排序来精炼此集合。该系统整合了专家标注的示例，以指导其在进行新颖性评估时比较论文，并生成有文献依据的推理。我们广泛的实验表明，我们的新颖性检查器比现有方法实现了约13%的更高一致性。消融研究进一步突显了基于方面的重排序器在识别新颖性评估最相关文献方面的重要性。"
    },
    {
        "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware\n  Layout Design",
        "url": "http://arxiv.org/abs/2506.21934v1",
        "pub_date": "2025-06-27",
        "summary": "Automated content-aware layout generation -- the task of arranging visual elements such as text, logos, and underlays on a background canvas -- remains a fundamental yet under-explored problem in intelligent design systems. While recent advances in deep generative models and large language models (LLMs) have shown promise in structured content generation, most existing approaches lack grounding in contextual design exemplars and fall short in handling semantic alignment and visual coherence. In this work we introduce CAL-RAG, a retrieval-augmented, agentic framework for content-aware layout generation that integrates multimodal retrieval, large language models, and collaborative agentic reasoning. Our system retrieves relevant layout examples from a structured knowledge base and invokes an LLM-based layout recommender to propose structured element placements. A vision-language grader agent evaluates the layout with visual metrics, and a feedback agent provides targeted refinements, enabling iterative improvement. We implement our framework using LangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in semantic and structural variability. CAL-RAG achieves state-of-the-art performance across multiple layout metrics -- including underlay effectiveness, element alignment, and overlap -- substantially outperforming strong baselines such as LayoutPrompter. These results demonstrate that combining retrieval augmentation with agentic multi-step reasoning yields a scalable, interpretable, and high-fidelity solution for automated layout generation.",
        "translated": "自动化内容感知布局生成——即在背景画布上排列文本、标志和底层元素等视觉元素的任务——在智能设计系统中仍是一个基础但有待深入探索的问题。尽管深度生成模型和大型语言模型（LLMs）的最新进展在结构化内容生成方面展现出潜力，但大多数现有方法缺乏基于上下文设计范例的支撑，并且在处理语义对齐和视觉一致性方面表现不足。\n\n在这项工作中，我们引入了CAL-RAG，这是一个用于内容感知布局生成的检索增强型智能体框架，它整合了多模态检索、大型语言模型以及协作式智能体推理。我们的系统从结构化知识库中检索相关的布局示例，并调用一个基于LLM的布局推荐器来提出结构化的元素放置方案。一个视觉-语言评估智能体利用视觉指标评估布局，而一个反馈智能体则提供有针对性的改进建议，从而实现迭代优化。\n\n我们使用LangGraph实现了我们的框架，并在PKU PosterLayout数据集上对其进行评估，该数据集是一个富含语义和结构多样性的基准。CAL-RAG在多项布局指标上达到了最先进的性能——包括底层元素有效性、元素对齐和重叠——显著优于LayoutPrompter等强劲基线。这些结果表明，将检索增强与智能体多步推理相结合，能够为自动化布局生成带来一个可扩展、可解释且高保真度的解决方案。"
    },
    {
        "title": "CLoVE: Personalized Federated Learning through Clustering of Loss Vector\n  Embeddings",
        "url": "http://arxiv.org/abs/2506.22427v1",
        "pub_date": "2025-06-27",
        "summary": "We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped into clusters based on their data distribution. However, identifying these clusters is challenging, as client assignments are unknown. CLoVE utilizes client embeddings derived from model losses on client data, and leverages the insight that clients in the same cluster share similar loss values, while those in different clusters exhibit distinct loss patterns. Based on these embeddings, CLoVE is able to iteratively identify and separate clients from different clusters and optimize cluster-specific models through federated aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its simplicity, (2) its applicability to both supervised and unsupervised settings, and (3) the fact that it eliminates the need for near-optimal model initialization, which makes it more robust and better suited for real-world applications. We establish theoretical convergence bounds, showing that CLoVE can recover clusters accurately with high probability in a single round and converges exponentially fast to optimal models in a linear setting. Our comprehensive experiments comparing with a variety of both CFL and generic Personalized Federated Learning (PFL) algorithms on different types of datasets and an extensive array of non-IID settings demonstrate that CLoVE achieves highly accurate cluster recovery in just a few rounds of training, along with state-of-the-art model accuracy, across a variety of both supervised and unsupervised PFL tasks.",
        "translated": "我们提出 CLoVE（损失向量嵌入聚类），这是一种用于联邦聚类学习（CFL）的新颖算法。在联邦聚类学习中，客户端根据其数据分布自然地分组为不同的簇。然而，识别这些簇极具挑战性，因为客户端的归属是未知的。CLoVE 利用从模型在客户端数据上的损失中提取的客户端嵌入，并借助于以下洞察：同一簇中的客户端共享相似的损失值，而不同簇中的客户端则表现出不同的损失模式。基于这些嵌入，CLoVE 能够迭代地识别并分离来自不同簇的客户端，并通过联邦聚合优化特定于簇的模型。相较于现有联邦聚类学习算法，CLoVE 的主要优势在于：(1) 其简单性；(2) 其适用于监督和无监督两种设置；(3) 它消除了对近似最优模型初始化的需求，这使其更具鲁棒性，更适合实际应用。我们建立了理论收敛边界，表明 CLoVE 可以在单轮中以高概率准确地恢复簇，并且在线性设置下以指数级速度收敛到最优模型。我们进行了全面的实验，将 CLoVE 与多种联邦聚类学习（CFL）和通用个性化联邦学习（PFL）算法在不同类型的数据集和广泛的非独立同分布（non-IID）设置下进行了比较。实验结果表明，CLoVE 仅在少数几轮训练中就能实现高度准确的簇恢复，并在各种监督和无监督的个性化联邦学习任务中同时达到最先进的模型准确性。"
    },
    {
        "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
        "url": "http://arxiv.org/abs/2506.22396v1",
        "pub_date": "2025-06-27",
        "summary": "Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms:   (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length.   Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (&lt;=0.2).",
        "translated": "大语言模型（LLM）部署中，推理过程占据了大部分延迟和能耗，通常超过总成本的90%。尽管训练阶段的效率已取得了长足进展，但运行时优化仍是关键瓶颈，尤其是在自回归解码过程中。现有方法——如剪枝、量化、提前退出和推测解码——通常需要重新训练、进行架构更改，或会破坏解码兼容性。\n\n我们引入了QuickSilver，这是一个模块化的令牌级框架，它能够在不改变模型权重或结构的情况下，在推理时实现语义自适应性。QuickSilver集成了四种协同机制：(i) **动态令牌停止**：对表示已收敛的令牌停止计算；(ii) **KV缓存跳过**：选择性地抑制内存写入以减少注意力开销；(iii) **上下文令牌融合**：将冗余令牌合并到共享路径中，以缩短序列长度。与推测解码或MoE路由不同，QuickSilver完全在冻结的密集模型上运行，并且不需要辅助网络。将QuickSilver应用于GPT-2和Llama-2模型，并在WikiText-103和C4数据集上进行测试，QuickSilver实现了高达39.6%的FLOPs（浮点运算）减少，同时困惑度下降可忽略不计（≤0.2）。"
    },
    {
        "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and\n  Measurement",
        "url": "http://arxiv.org/abs/2506.22372v1",
        "pub_date": "2025-06-27",
        "summary": "The presence of social biases in Natural Language Processing (NLP) and Information Retrieval (IR) systems is an ongoing challenge, which underlines the importance of developing robust approaches to identifying and evaluating such biases. In this paper, we aim to address this issue by leveraging Large Language Models (LLMs) to detect and measure gender bias in passage ranking. Existing gender fairness metrics rely on lexical- and frequency-based measures, leading to various limitations, e.g., missing subtle gender disparities. Building on our LLM-based gender bias detection method, we introduce a novel gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to address existing limitations. To measure the effectiveness of our proposed metric and study LLMs' effectiveness in detecting gender bias, we annotate a subset of the MS MARCO Passage Ranking collection and release our new gender bias collection, called MSMGenderBias, to foster future research in this area. Our extensive experimental results on various ranking models show that our proposed metric offers a more detailed evaluation of fairness compared to previous metrics, with improved alignment to human labels (58.77% for Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa agreement), effectively distinguishing gender bias in ranking. By integrating LLM-driven bias detection, an improved fairness metric, and gender bias annotations for an established dataset, this work provides a more robust framework for analyzing and mitigating bias in IR systems.",
        "translated": "自然语言处理（NLP）和信息检索（IR）系统中社会偏见的存在是一个持续存在的难题，这凸显了开发鲁棒方法来识别和评估此类偏见的重要性。在本文中，我们旨在通过利用大型语言模型（LLM）来检测和衡量段落排序中的性别偏见，从而解决这一问题。现有的性别公平性指标依赖于基于词汇和频率的度量方法，导致了各种局限性，例如遗漏了细微的性别差异。基于我们提出的LLM性别偏见检测方法，我们引入了一种新颖的性别公平性指标，名为“类别加权曝光度”（CWEx），旨在解决现有局限性。为了衡量我们提出的指标的有效性，并研究LLM在检测性别偏见方面的有效性，我们标注了MS MARCO段落排序数据集的一个子集，并发布了我们新的性别偏见数据集，命名为MSMGenderBias，以促进该领域的未来研究。我们在各种排序模型上进行的广泛实验结果表明，与现有指标相比，我们提出的指标提供了更详细的公平性评估，并且与人工标注的一致性更高（Grep-BiasIR数据集上为58.77%，MSMGenderBias数据集上为18.51%，使用Cohen's Kappa一致性系数衡量），从而有效地区分了排序中的性别偏见。通过整合LLM驱动的偏见检测、改进的公平性指标以及对既有数据集的性别偏见标注，这项工作为分析和缓解信息检索系统中的偏见提供了一个更鲁棒的框架。"
    },
    {
        "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation",
        "url": "http://arxiv.org/abs/2506.23662v1",
        "pub_date": "2025-06-30",
        "summary": "Context-aware embedding methods boost retrieval accuracy by conditioning on corpus statistics (e.g., term co-occurrence and topical patterns) extracted from neighboring documents. However, this context-aware approach requires access to the target corpus or requires domain-specific finetuning, posing practical barriers in privacy-sensitive or resource-constrained settings. We present ZEST, a zero-shot contextual adaptation framework that replaces real corpus access with a one-time offline synthesis of a compact proxy. Given only a handful exemplar documents representative of the general target domain, we use a multi-step hierarchical procedure to generate a synthetic context corpus of several hundred documents that aims to emulate key domain-specific distributions. At inference, the frozen context-aware encoder uses this proxy corpus -- without any finetuning or target corpus access -- to produce domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot synthetic context adaptation using only five example documents performs within 0.5% of models leveraging full target corpus access -- demonstrating remarkable efficacy without any retraining. ZEST thus provides a practical method for deploying high-performance, adaptable embeddings in constrained environments.",
        "translated": "上下文感知嵌入方法通过利用从相邻文档中提取的语料库统计信息（例如词项共现和主题模式）来提升检索精度。然而，这种上下文感知方法需要访问目标语料库或进行领域特定的微调，这在隐私敏感或资源受限的环境中构成了实际障碍。\n\n我们提出了 ZEST，一个零样本上下文适应框架，它通过一次性离线合成一个紧凑的代理来取代对真实语料库的访问。只需提供少量代表通用目标领域的示例文档，我们便能使用多步分层程序生成一个包含数百个文档的合成上下文语料库，旨在模拟关键的领域特定分布。在推理时，冻结的上下文感知编码器无需任何微调或目标语料库访问，即可使用这个代理语料库来生成领域适应性嵌入。\n\n在 MTEB 基准测试中，ZEST 仅使用五个示例文档进行零样本合成上下文适应，其表现与利用完整目标语料库访问的模型相差不到 0.5%，这表明其在无需任何再训练的情况下具有显著的效能。因此，ZEST 为在受限环境中部署高性能、适应性强的嵌入提供了一种实用方法。"
    },
    {
        "title": "Act-With-Think: Chunk Auto-Regressive Modeling for Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.23643v1",
        "pub_date": "2025-06-30",
        "summary": "Generative recommendation (GR) typically encodes behavioral or semantic aspects of item information into discrete tokens, leveraging the standard autoregressive (AR) generation paradigm to make predictions. However, existing methods tend to overlook their intrinsic relationship, that is, the semantic usually provides some reasonable explainability \"$\\textbf{why}$\" for the behavior \"$\\textbf{what}$\", which may constrain the full potential of GR. To this end, we present Chunk AutoRegressive Modeling (CAR), a new generation paradigm following the decision pattern that users usually think semantic aspects of items (e.g. brand) and then take actions on target items (e.g. purchase). Our CAR, for the $\\textit{first time}$, incorporates semantics (SIDs) and behavior (UID) into a single autoregressive transformer from an ``act-with-think'' dual perspective via chunk-level autoregression. Specifically, CAR packs SIDs and UID into a conceptual chunk for item unified representation, allowing each decoding step to make a holistic prediction. Experiments show that our CAR significantly outperforms existing methods based on traditional AR, improving Recall@5 by 7.93% to 22.30%. Furthermore, we verify the scaling effect between model performance and SIDs bit number, demonstrating that CAR preliminary emulates a kind of slow-thinking style mechanism akin to the reasoning processes observed in large language models (LLMs).",
        "translated": "生成式推荐（GR）通常将物品信息的行为或语义方面编码成离散标记，并利用标准的自回归（AR）生成范式进行预测。然而，现有方法往往忽视了它们内在的关系，即语义通常为行为“是什么”（$\\textbf{what}$）提供合理的解释“为什么”（$\\textbf{why}$），这可能限制了GR的全部潜力。为此，我们提出了块自回归建模（CAR），这是一种新的生成范式，遵循用户通常先思考物品的语义方面（例如品牌），然后对目标物品采取行动（例如购买）的决策模式。我们的CAR首次将语义信息（SIDs）和行为信息（UID）从“行动与思考”的双重视角，通过块级自回归整合到一个单一的自回归Transformer中。具体而言，CAR将SIDs和UID打包成一个概念块，用于物品的统一表示，使得每个解码步骤都能进行整体预测。实验表明，我们的CAR显著优于基于传统AR的现有方法，将Recall@5提升了7.93%至22.30%。此外，我们验证了模型性能与SIDs比特数之间的规模效应，证明CAR初步模拟了一种类似于大型语言模型（LLMs）中观察到的推理过程的慢思考（slow-thinking）式机制。"
    },
    {
        "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent",
        "url": "http://arxiv.org/abs/2506.23485v1",
        "pub_date": "2025-06-30",
        "summary": "Interactive recommendation is a typical information-seeking task that allows users to interactively express their needs through natural language and obtain personalized recommendations. Large language model-powered (LLM-powered) agents have become a new paradigm in interactive recommendations, effectively capturing users' real-time needs and enhancing personalized experiences. However, due to limited planning and generalization capabilities, existing formulations of LLM-powered interactive recommender agents struggle to effectively address diverse and complex user intents, such as intuitive, unrefined, or occasionally ambiguous requests. To tackle this challenge, we propose a novel thought-augmented interactive recommender agent system (TAIRA) that addresses complex user intents through distilled thought patterns. Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring a manager agent that orchestrates recommendation tasks by decomposing user needs and planning subtasks, with its planning capacity strengthened through Thought Pattern Distillation (TPD), a thought-augmentation method that extracts high-level thoughts from the agent's and human experts' experiences. Moreover, we designed a set of user simulation schemes to generate personalized queries of different difficulties and evaluate the recommendations based on specific datasets. Through comprehensive experiments conducted across multiple datasets, TAIRA exhibits significantly enhanced performance compared to existing methods. Notably, TAIRA shows a greater advantage on more challenging tasks while generalizing effectively on novel tasks, further validating its superiority in managing complex user intents within interactive recommendation systems. The code is publicly available at:https://github.com/Alcein/TAIRA.",
        "translated": "交互式推荐是一种典型的**信息检索任务**，它允许用户通过自然语言交互式地表达其需求，并获得个性化推荐。大语言模型（LLM）驱动的智能体已成为交互式推荐领域的新范式，能够有效捕获用户的实时需求并增强个性化体验。然而，由于规划和泛化能力的局限性，现有的大语言模型驱动的交互式推荐智能体设计难以有效应对多样化和复杂的用户意图，例如直观的、未经精炼的或偶尔模糊的请求。为解决这一挑战，我们提出了一种新颖的**思想增强型交互式推荐智能体系统（TAIRA）**，该系统通过蒸馏的思维模式来解决复杂的用户意图。具体来说，TAIRA 被设计为一个大语言模型驱动的多智能体系统，其核心是一个管理智能体，负责通过分解用户需求和规划子任务来协调推荐任务。其规划能力通过**思维模式蒸馏（TPD）**得到强化，这是一种从智能体自身和人类专家的经验中提取高层次思维的思想增强方法。此外，我们设计了一套用户模拟方案，以生成不同难度的个性化查询，并基于特定数据集评估推荐效果。通过在多个数据集上进行的综合实验，TAIRA 表现出与现有方法相比显著增强的性能。值得注意的是，TAIRA 在更具挑战性的任务上显示出更大的优势，同时在全新任务上也能有效泛化，这进一步验证了其在交互式推荐系统中管理复杂用户意图方面的优越性。代码已公开，地址为：https://github.com/Alcein/TAIRA。"
    },
    {
        "title": "KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation,\n  And Try-On",
        "url": "http://arxiv.org/abs/2506.23471v1",
        "pub_date": "2025-06-30",
        "summary": "The global fashion e-commerce industry has become integral to people's daily lives, leveraging technological advancements to offer personalized shopping experiences, primarily through recommendation systems that enhance customer engagement through personalized suggestions. To improve customers' experience in online shopping, we propose a novel comprehensive KiseKloset system for outfit retrieval, recommendation, and try-on. We explore two approaches for outfit retrieval: similar item retrieval and text feedback-guided item retrieval. Notably, we introduce a novel transformer architecture designed to recommend complementary items from diverse categories. Furthermore, we enhance the overall performance of the search pipeline by integrating approximate algorithms to optimize the search process. Additionally, addressing the crucial needs of online shoppers, we employ a lightweight yet efficient virtual try-on framework capable of real-time operation, memory efficiency, and maintaining realistic outputs compared to its predecessors. This virtual try-on module empowers users to visualize specific garments on themselves, enhancing the customers' experience and reducing costs associated with damaged items for retailers. We deployed our end-to-end system for online users to test and provide feedback, enabling us to measure their satisfaction levels. The results of our user study revealed that 84% of participants found our comprehensive system highly useful, significantly improving their online shopping experience.",
        "translated": "全球时尚电商行业已深度融入人们的日常生活，它借助技术进步提供个性化购物体验，主要通过推荐系统提供个性化建议以增强客户参与度。为了提升用户在线购物体验，我们提出了一种新颖的综合性KiseKloset系统，用于实现服饰搭配检索、推荐和试穿功能。在服饰搭配检索方面，我们探索了两种方法：相似商品检索和文本反馈引导的商品检索。值得注意的是，我们引入了一种新颖的Transformer架构，旨在推荐来自不同品类的互补商品。此外，我们通过整合近似算法来优化搜索过程，从而提升了整个搜索流程的整体性能。此外，为了满足在线购物者的关键需求，我们采用了一种轻量级但高效的虚拟试穿框架，该框架能够实现实时运行、高效利用内存，并与现有技术相比保持逼真的输出效果。这一虚拟试穿模块赋能用户直观地看到特定服装穿在自己身上的效果，从而提升了客户体验，并为零售商降低了因商品损坏产生的成本。我们将端到端系统部署供在线用户测试并提供反馈，以便衡量他们的满意度。我们的用户研究结果显示，84%的参与者认为我们这套综合系统非常实用，显著改善了他们的在线购物体验。"
    },
    {
        "title": "NaviX: A Native Vector Index Design for Graph DBMSs With Robust\n  Predicate-Agnostic Search Performance",
        "url": "http://arxiv.org/abs/2506.23397v1",
        "pub_date": "2025-06-29",
        "summary": "There is an increasing demand for extending existing DBMSs with vector indices so that they become unified systems capable of supporting modern predictive applications, which require joint querying of vector embeddings together with the structured properties and connections of objects. We present NaviX, a native vector index for graph DBMSs (GDBMSs) that has two main design goals. First, we aim to implement a disk-based vector index that leverages the core storage and query-processing capabilities of the underlying GDBMS. To this end, NaviX is built on the Hierarchical Navigable Small-World (HNSW) graph, which itself is a graph-based structure. Second, we aim to support predicate-agnostic filtered vector search queries, in which the k nearest neighbors (kNNs) of a query vector vQ are searched only within an arbitrary subset S of vectors defined by an ad-hoc selection sub-query QS. We adopt a prefiltering approach that evaluates QS first and passes the full description of subset S to the kNN search operator. We study how to design a prefiltering search algorithm that remains robust under varying selectivities and under different correlations between subset S and query vector vQ. We propose an adaptive algorithm that uses the local selectivity of each vector in the HNSW graph to choose an appropriate heuristic at every iteration of the kNN search. Finally, We demonstrate NaviX's robustness and efficiency through extensive experiments against both existing prefiltering- and postfiltering-based baselines.",
        "translated": "当前，业界对扩展现有数据库管理系统（DBMS）以支持向量索引的需求日益增长，旨在使其成为能够支持现代预测应用的统一系统。这些应用需要联合查询向量嵌入以及对象的结构化属性和连接。我们提出了NaviX，一个用于图数据库管理系统（GDBMS）的原生向量索引，它有两个主要设计目标。\n\n首先，我们旨在实现一个基于磁盘的向量索引，该索引能够利用底层GDBMS的核心存储和查询处理能力。为此，NaviX构建在分层可导航小世界（HNSW）图上，HNSW本身也是一种基于图的结构。其次，我们旨在支持谓词无关的过滤向量搜索查询，即查询向量vQ的k近邻（kNNs）仅在由临时选择子查询QS定义的任意向量子集S中进行搜索。我们采用了一种预过滤方法，该方法首先评估QS，并将子集S的完整描述传递给kNN搜索操作符。\n\n我们研究了如何设计一种预过滤搜索算法，使其在不同选择度下以及子集S与查询向量vQ之间存在不同相关性时，仍能保持鲁棒性。我们提出了一种自适应算法，该算法利用HNSW图中每个向量的局部选择度，在kNN搜索的每次迭代中选择合适的启发式方法。最后，通过与现有基于预过滤和后过滤的基线进行大量实验，我们证明了NaviX的鲁棒性和效率。"
    },
    {
        "title": "Learning to Rank with Variable Result Presentation Lengths",
        "url": "http://arxiv.org/abs/2506.23319v1",
        "pub_date": "2025-06-29",
        "summary": "Learning to Rank (LTR) methods generally assume that each document in a top-K ranking is presented in an equal format. However, previous work has shown that users' perceptions of relevance can be changed by varying presentations, i.e., allocating more vertical space to some documents to provide additional textual or image information. Furthermore, presentation length can also redirect attention, as users are more likely to notice longer presentations when scrolling through results. Deciding on the document presentation lengths in a fixed vertical space ranking is an important problem that has not been addressed by existing LTR methods.   We address this gap by introducing the variable presentation length ranking task, where simultaneously the ordering of documents and their presentation length is decided. Despite being a generalization of standard ranking, we show that this setting brings significant new challenges: Firstly, the probability ranking principle no longer applies to this setting, and secondly, the problem cannot be divided into separate ordering and length selection tasks.   We therefore propose VLPL - a new family of Plackett-Luce list-wise gradient estimation methods for the joint optimization of document ordering and lengths. Our semi-synthetic experiments show that VLPL can effectively balance the expected exposure and attractiveness of all documents, achieving the best performance across different ranking settings. Furthermore, we observe that even simple length-aware methods can achieve significant performance improvements over fixed-length models. Altogether, our theoretical and empirical results highlight the importance and difficulties of combining document presentation with LTR.",
        "translated": "学习排序（LTR）方法通常假设顶K排序中的每个文档都以相同的格式呈现。然而，先前的工作表明，通过改变呈现方式（即为某些文档分配更多垂直空间以提供额外的文本或图像信息），用户对相关性的感知会发生变化。此外，呈现长度也能重定向用户的注意力，因为用户在滚动浏览结果时更可能注意到较长的呈现内容。在固定垂直空间的排序中决定文档的呈现长度是一个重要问题，而现有LTR方法尚未解决。\n\n我们通过引入可变呈现长度排序任务来弥补这一空白，该任务同时决定文档的排序及其呈现长度。尽管这是标准排序的一种泛化，但我们表明此设置带来了重大的新挑战：首先，概率排序原则（PRP）不再适用于此设置；其次，该问题不能分解为独立的排序和长度选择任务。\n\n因此，我们提出了VLPL——一种新的Plackett-Luce列表式梯度估计方法家族，用于文档排序和长度的联合优化。我们的半合成实验表明，VLPL能够有效平衡所有文档的预期曝光度和吸引力，在不同排序设置下均实现了最佳性能。此外，我们观察到，即使是简单的长度感知方法，相较于固定长度模型也能实现显著的性能提升。总而言之，我们的理论和经验结果凸显了将文档呈现与LTR相结合的重要性和困难性。"
    },
    {
        "title": "On the Predictive Power of Representation Dispersion in Language Models",
        "url": "http://arxiv.org/abs/2506.24106v1",
        "pub_date": "2025-06-30",
        "summary": "We show that a language model's ability to predict text is tightly linked to the breadth of its embedding space: models that spread their contextual representations more widely tend to achieve lower perplexity. Concretely, we find that representation dispersion - the average pairwise cosine distance among hidden vectors - strongly and negatively correlates with perplexity across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia, news, scientific abstracts). Beyond illustrating this link, we show how dispersion can be leveraged for a range of practical tasks without requiring labeled data. First, measuring dispersion on unlabeled text allows us to predict downstream accuracy in new domains, offering a data-efficient tool for model selection. Next, we find that identifying layers with higher dispersion pinpoints the best representations for retrieval-based methods such as kNN-LM, bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple push-away objective into training, which increases dispersion in both single-domain and cross-domain scenarios and directly improves perplexity in each.",
        "translated": "我们发现，语言模型的文本预测能力与其嵌入空间的广度紧密相关：那些能够将上下文表示更广泛地分散的模型，往往能实现更低的困惑度。具体而言，我们发现表示分散度——即隐藏向量之间的平均成对余弦距离——在多种模型家族（LLaMA、Qwen等）和不同领域（维基百科、新闻、科学摘要）中，与困惑度呈强烈负相关。\n\n除了阐明这一关联之外，我们还展示了如何在无需标注数据的情况下，利用表示分散度来执行一系列实际任务。首先，在未标注文本上测量分散度，使我们能够预测模型在新领域中的下游任务准确性，从而提供了一种数据高效的模型选择工具。其次，我们发现识别出具有更高分散度的层，能够精确定位用于基于检索的方法（如kNN-LM）的最佳表示，从而避免了详尽的逐层搜索。最后，我们还在训练中整合了一个简单的“推开”（push-away）目标，它能增加单领域和跨领域场景中的表示分散度，并直接改善了这两种情况下的困惑度。"
    },
    {
        "title": "Computational Detection of Intertextual Parallels in Biblical Hebrew: A\n  Benchmark Study Using Transformer-Based Language Models",
        "url": "http://arxiv.org/abs/2506.24117v1",
        "pub_date": "2025-06-30",
        "summary": "Identifying parallel passages in biblical Hebrew is foundational in biblical scholarship for uncovering intertextual relationships. Traditional methods rely on manual comparison, which is labor-intensive and prone to human error. This study evaluates the potential of pre-trained transformer-based language models, including E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in the Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings and Chronicles, I assessed each model's capability to generate word embeddings that delineate parallel from non-parallel passages. Utilizing cosine similarity and Wasserstein Distance measures, I found that E5 and AlephBERT show significant promise, with E5 excelling in parallel detection and AlephBERT demonstrating stronger non-parallel differentiation. These findings indicate that pre-trained models can enhance the efficiency and accuracy of detecting intertextual parallels in ancient texts, suggesting broader applications for ancient language studies.",
        "translated": "识别希伯来圣经中的平行篇章在圣经研究中具有基础性意义，有助于揭示互文关系。传统方法依赖人工比对，耗时耗力且容易出现人为错误。本研究评估了预训练的基于Transformer的语言模型（包括E5、AlephBERT、MPNet和LaBSE）在检测希伯来圣经文本平行篇章方面的潜力。我侧重于撒母耳记/列王纪与历代志之间的已知平行篇章，评估了每个模型生成词嵌入以区分平行篇章与非平行篇章的能力。利用余弦相似度和Wasserstein距离度量，我发现E5和AlephBERT展现出显著潜力，其中E5在平行检测方面表现出色，而AlephBERT则在非平行区分方面表现出更强的能力。这些发现表明，预训练模型可以提高古代文本中互文平行检测的效率和准确性，预示着其在古代语言研究中更广泛的应用前景。"
    },
    {
        "title": "On the Predictive Power of Representation Dispersion in Language Models",
        "url": "http://arxiv.org/abs/2506.24106v1",
        "pub_date": "2025-06-30",
        "summary": "We show that a language model's ability to predict text is tightly linked to the breadth of its embedding space: models that spread their contextual representations more widely tend to achieve lower perplexity. Concretely, we find that representation dispersion - the average pairwise cosine distance among hidden vectors - strongly and negatively correlates with perplexity across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia, news, scientific abstracts). Beyond illustrating this link, we show how dispersion can be leveraged for a range of practical tasks without requiring labeled data. First, measuring dispersion on unlabeled text allows us to predict downstream accuracy in new domains, offering a data-efficient tool for model selection. Next, we find that identifying layers with higher dispersion pinpoints the best representations for retrieval-based methods such as kNN-LM, bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple push-away objective into training, which increases dispersion in both single-domain and cross-domain scenarios and directly improves perplexity in each.",
        "translated": "我们发现，语言模型预测文本的能力与其嵌入空间的广度紧密相关：那些能更广泛地分散其上下文表示的模型，往往能实现更低的困惑度。具体而言，我们发现表示分散度——即隐藏向量之间的平均成对余弦距离——在不同的模型家族（LLaMA、Qwen等）和领域（维基百科、新闻、科学摘要）中，与困惑度呈现强烈负相关。\n\n除了阐明这一联系，我们还展示了如何在无需标注数据的情况下，将分散度应用于一系列实际任务。首先，在未标注文本上测量分散度使我们能够预测在新领域中的下游准确性，为模型选择提供了一种数据高效的工具。其次，我们发现识别具有更高分散度的层可以精确定位基于检索的方法（如kNN-LM）的最佳表示，从而绕过详尽的逐层搜索。最后，我们将一个简单的“推开”（push-away）目标集成到训练中，这在单领域和跨领域场景中都增加了分散度，并直接改善了各自的困惑度。"
    },
    {
        "title": "Deep Recommender Models Inference: Automatic Asymmetric Data Flow\n  Optimization",
        "url": "http://arxiv.org/abs/2507.01676v1",
        "pub_date": "2025-07-02",
        "summary": "Deep Recommender Models (DLRMs) inference is a fundamental AI workload accounting for more than 79% of the total AI workload in Meta's data centers. DLRMs' performance bottleneck is found in the embedding layers, which perform many random memory accesses to retrieve small embedding vectors from tables of various sizes. We propose the design of tailored data flows to speedup embedding look-ups. Namely, we propose four strategies to look up an embedding table effectively on one core, and a framework to automatically map the tables asymmetrically to the multiple cores of a SoC. We assess the effectiveness of our method using the Huawei Ascend AI accelerators, comparing it with the default Ascend compiler, and we perform high-level comparisons with Nvidia A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload distributions, and more than 20x for extremely unbalanced distributions. Furthermore, the method proves to be much more independent of the query distribution than the baseline.",
        "translated": "深度推荐模型（DLRMs）的推理是Meta数据中心中一项核心的AI工作负载，其占总AI工作负载的比例超过79%。DLRMs的性能瓶颈在于其嵌入层，该层需要执行大量的随机内存访问，以从各种大小的表中检索小型嵌入向量。为此，我们提出设计定制的数据流，以加速嵌入查找。具体而言，我们提出了四种在一核上高效查找嵌入表的策略，以及一个将这些表非对称地自动映射到片上系统（SoC）多核心的框架。我们使用华为昇腾AI加速器评估了我们方法的有效性，将其与默认的昇腾编译器进行了比较，并与英伟达A100进行了高层级比较。结果显示，对于实际工作负载分布，加速比达到1.5倍至6.5倍；对于极度不平衡的分布，加速则超过20倍。此外，结果表明该方法相比基线而言，对查询分布的依赖性要小得多。"
    },
    {
        "title": "Enhanced Influence-aware Group Recommendation for Online Media\n  Propagation",
        "url": "http://arxiv.org/abs/2507.01616v1",
        "pub_date": "2025-07-02",
        "summary": "Group recommendation over social media streams has attracted significant attention due to its wide applications in domains such as e-commerce, entertainment, and online news broadcasting. By leveraging social connections and group behaviours, group recommendation (GR) aims to provide more accurate and engaging content to a set of users rather than individuals. Recently, influence-aware GR has emerged as a promising direction, as it considers the impact of social influence on group decision-making. In earlier work, we proposed Influence-aware Group Recommendation (IGR) to solve this task. However, this task remains challenging due to three key factors: the large and ever-growing scale of social graphs, the inherently dynamic nature of influence propagation within user groups, and the high computational overhead of real-time group-item matching.   To tackle these issues, we propose an Enhanced Influence-aware Group Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based Sampling (GES) strategy to minimise redundancy across multiple temporal social graphs and effectively capture the evolving dynamics of both groups and items. Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict how influence propagates over time across social items and user groups. Finally, we develop a two-level hash-based User Group Index (UG-Index) to efficiently organise user groups and enable real-time recommendation generation. Extensive experiments on real-world datasets demonstrate that our proposed framework, EIGR, consistently outperforms state-of-the-art baselines in both effectiveness and efficiency.",
        "translated": "社交媒体流上的群组推荐因其在电子商务、娱乐、在线新闻广播等领域的广泛应用而引起了广泛关注。通过利用社交关系和群组行为，群组推荐（GR）旨在为一组用户而非个体提供更准确、更具吸引力的内容。近年来，影响力感知群组推荐（influence-aware GR）作为一个有前景的方向而兴起，因为它考虑了社交影响力对群组决策的影响。在我们之前的工作中，我们提出了影响力感知群组推荐（IGR）来解决这项任务。然而，这项任务仍然具有挑战性，原因在于三个关键因素：社交图谱规模庞大且不断增长、用户群组内影响力传播固有的动态性，以及实时群组-项目匹配的高计算开销。\n\n为了解决这些问题，我们提出了一个增强型影响力感知群组推荐（EIGR）框架。首先，我们引入了一种基于图提取的采样（GES）策略，以最小化跨多个时间社交图谱的冗余，并有效捕捉群组和项目的演变动态。其次，我们设计了一个新颖的动态独立级联（DYIC）模型，用于预测影响力如何随时间跨社交项目和用户群组传播。最后，我们开发了一个两级哈希用户群组索引（UG-Index），以有效组织用户群组并实现实时推荐生成。在真实世界数据集上进行的大量实验表明，我们提出的框架 EIGR 在有效性和效率两方面都始终优于最先进的基线。"
    },
    {
        "title": "DARTS: A Dual-View Attack Framework for Targeted Manipulation in\n  Federated Sequential Recommendation",
        "url": "http://arxiv.org/abs/2507.01383v1",
        "pub_date": "2025-07-02",
        "summary": "Federated recommendation (FedRec) preserves user privacy by enabling decentralized training of personalized models, but this architecture is inherently vulnerable to adversarial attacks. Significant research has been conducted on targeted attacks in FedRec systems, motivated by commercial and social influence considerations. However, much of this work has largely overlooked the differential robustness of recommendation models. Moreover, our empirical findings indicate that existing targeted attack methods achieve only limited effectiveness in Federated Sequential Recommendation(FSR) tasks. Driven by these observations, we focus on investigating targeted attacks in FSR and propose a novel dualview attack framework, named DV-FSR. This attack method uniquely combines a sampling-based explicit strategy with a contrastive learning-based implicit gradient strategy to orchestrate a coordinated attack. Additionally, we introduce a specific defense mechanism tailored for targeted attacks in FSR, aiming to evaluate the mitigation effects of the attack method we proposed. Extensive experiments validate the effectiveness of our proposed approach on representative sequential models. Our codes are publicly available.",
        "translated": "联邦推荐（FedRec）通过去中心化训练个性化模型来保护用户隐私，但其架构本质上容易受到对抗性攻击。针对FedRec系统中的定向攻击已进行了大量研究，其动机源于商业和社交流量考量。然而，其中大部分工作在很大程度上忽略了推荐模型的差异鲁棒性。此外，我们的实证研究发现，现有定向攻击方法在联邦序列推荐（FSR）任务中效果有限。\n\n受这些观察启发，我们着重研究FSR中的定向攻击，并提出了一个名为DV-FSR的新型双视角攻击框架。该攻击方法独特地结合了基于采样的显式策略和基于对比学习的隐式梯度策略，以协同发动攻击。此外，我们引入了一种针对FSR中定向攻击量身定制的防御机制，旨在评估其对我们所提攻击方法的缓解效果。广泛的实验验证了我们所提方法在代表性序列模型上的有效性。我们的代码已公开发布。"
    },
    {
        "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive\n  Benchmarks",
        "url": "http://arxiv.org/abs/2507.01297v1",
        "pub_date": "2025-07-02",
        "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited settings, such as factoid question answering; more challenging, reasoning-intensive benchmarks have seen limited success from minimal RAG. In this work, we challenge this prevailing view on established, reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We identify a key missing component in prior work: a usable, web-scale datastore aligned with the breadth of pretraining data. To this end, we introduce CompactDS: a diverse, high-quality, web-scale datastore that achieves high retrieval accuracy and subsecond latency on a single-node. The key insights are (1) most web content can be filtered out without sacrificing coverage, and a compact, high-quality subset is sufficient; and (2) combining in-memory approximate nearest neighbor (ANN) retrieval and on-disk exact search balances speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves consistent accuracy improvements across all benchmarks and model sizes (8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA, and 19% on MATH. No single data source suffices alone, highlighting the importance of diversity of sources (web crawls, curated math, academic papers, textbooks). Finally, we show that our carefully designed in-house datastore matches or outperforms web search engines such as Google Search, as well as recently proposed, complex agent-based RAG systems--all while maintaining simplicity, reproducibility, and self-containment. We release CompactDS and our retrieval pipeline, supporting future research exploring retrieval-based AI systems.",
        "translated": "检索增强生成（RAG）主要在有限场景下进行了研究，例如事实型问答；而更具挑战性、推理密集型基准上，极简RAG（minimal RAG）的表现却不尽如人意。在这项工作中，我们挑战了关于极简RAG在现有推理密集型基准（MMLU、MMLU Pro、AGI Eval、GPQA和MATH）上表现不佳的普遍看法。我们发现先前工作中缺少一个关键组件：一个可用、网络规模且与预训练数据内容范围相匹配的数据存储。\n\n为此，我们引入了CompactDS：一个多样化、高质量、网络规模的数据存储，它能在单节点上实现高检索精度和亚秒级延迟。其核心见解在于：(1) 大部分网络内容可以在不牺牲覆盖范围的情况下被过滤掉，一个紧凑、高质量的子集就已足够；(2) 结合内存近似最近邻（ANN）检索和磁盘精确搜索可以平衡速度与召回率。\n\n借助CompactDS，我们展示了极简RAG流水线在所有基准和不同模型规模（80亿至700亿参数）上均实现了持续的精度提升，在MMLU上取得了10%的相对增益，MMLU Pro上为33%，GPQA上为14%，MATH上为19%。没有单一数据源能独自满足需求，这凸显了数据源多样性（网络爬取数据、精选数学资料、学术论文、教科书）的重要性。最后，我们展示了我们精心设计的内部数据存储能媲美或超越谷歌搜索等网络搜索引擎，以及最近提出的复杂基于代理的RAG系统——同时保持了简单性、可复现性和自包含性。我们发布了CompactDS及其检索流水线，以支持未来对基于检索的AI系统的研究。"
    },
    {
        "title": "Far From Sight, Far From Mind: Inverse Distance Weighting for Graph\n  Federated Recommendation",
        "url": "http://arxiv.org/abs/2507.01285v1",
        "pub_date": "2025-07-02",
        "summary": "Graph federated recommendation systems offer a privacy-preserving alternative to traditional centralized recommendation architectures, which often raise concerns about data security. While federated learning enables personalized recommendations without exposing raw user data, existing aggregation methods overlook the unique properties of user embeddings in this setting. Indeed, traditional aggregation methods fail to account for their complexity and the critical role of user similarity in recommendation effectiveness. Moreover, evolving user interactions require adaptive aggregation while preserving the influence of high-relevance anchor users (the primary users before expansion in graph-based frameworks). To address these limitations, we introduce Dist-FedAvg, a novel distance-based aggregation method designed to enhance personalization and aggregation efficiency in graph federated learning. Our method assigns higher aggregation weights to users with similar embeddings, while ensuring that anchor users retain significant influence in local updates. Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg consistently outperforms baseline aggregation techniques, improving recommendation accuracy while maintaining seamless integration into existing federated learning frameworks.",
        "translated": "图联邦推荐系统为传统的中心化推荐架构提供了一种保护隐私的替代方案，后者常常引发数据安全担忧。尽管联邦学习能够在不暴露原始用户数据的情况下实现个性化推荐，但现有的聚合方法忽略了在此背景下用户嵌入的独特性质。事实上，传统聚合方法未能充分考虑其复杂性以及用户相似性在推荐效果中的关键作用。此外，不断演变的用户交互需要自适应聚合，同时还要保留高相关性锚点用户（图基框架中扩展前的原始用户）的影响力。为了解决这些局限性，我们引入了 Dist-FedAvg，这是一种新颖的基于距离的聚合方法，旨在增强图联邦学习中的个性化和聚合效率。我们的方法为具有相似嵌入的用户分配更高的聚合权重，同时确保锚点用户在本地更新中保持显著影响力。在多个数据集上的实证评估表明，Dist-FedAvg 始终优于基线聚合技术，在提高推荐准确性的同时，还能保持与现有联邦学习框架的无缝集成。"
    },
    {
        "title": "Towards a Signal Detection Based Measure for Assessing Information\n  Quality of Explainable Recommender Systems",
        "url": "http://arxiv.org/abs/2507.01168v1",
        "pub_date": "2025-07-01",
        "summary": "There is growing interest in explainable recommender systems that provide recommendations along with explanations for the reasoning behind them. When evaluating recommender systems, most studies focus on overall recommendation performance. Only a few assess the quality of the explanations. Explanation quality is often evaluated through user studies that subjectively gather users' opinions on representative explanatory factors that shape end-users' perspective towards the results, not about the explanation contents itself. We aim to fill this gap by developing an objective metric to evaluate Veracity: the information quality of explanations. Specifically, we decompose Veracity into two dimensions: Fidelity and Attunement. Fidelity refers to whether the explanation includes accurate information about the recommended item. Attunement evaluates whether the explanation reflects the target user's preferences. By applying signal detection theory, we first determine decision outcomes for each dimension and then combine them to calculate a sensitivity, which serves as the final Veracity value. To assess the effectiveness of the proposed metric, we set up four cases with varying levels of information quality to validate whether our metric can accurately capture differences in quality. The results provided meaningful insights into the effectiveness of our proposed metric.",
        "translated": "对提供推荐结果并解释其背后推理过程的**可解释推荐系统**（explainable recommender systems）的兴趣日益增长。在评估推荐系统时，大多数研究侧重于**整体推荐性能**，而只有少数研究会**评估解释的质量**。解释质量通常通过**用户研究**进行评估，这些研究主观地收集用户对影响最终用户看待推荐结果的**代表性解释因素**的看法，而非针对**解释内容本身**。\n\n为填补这一空白，我们旨在开发一个**客观指标**来评估**真实性（Veracity）**，即解释的**信息质量**。具体而言，我们将**真实性**分解为两个维度：**忠实度（Fidelity）**和**契合度（Attunement）**。**忠实度**指解释是否包含关于所推荐物品的**准确信息**。**契合度**评估解释是否**反映目标用户的偏好**。\n\n通过应用**信号检测理论（Signal Detection Theory, SDT）**，我们首先确定每个维度的**决策结果**，然后将它们组合起来计算一个**敏感度（sensitivity）**，该敏感度即作为最终的**真实性值**。为了评估所提出指标的有效性，我们设置了**四种不同信息质量水平的案例**，以验证我们的指标是否能**准确捕捉质量差异**。结果为我们所提出的指标的有效性提供了**有意义的见解**。"
    },
    {
        "title": "Digital Collections Explorer: An Open-Source, Multimodal Viewer for\n  Searching Digital Collections",
        "url": "http://arxiv.org/abs/2507.00961v1",
        "pub_date": "2025-07-01",
        "summary": "We present Digital Collections Explorer, a web-based, open-source exploratory search platform that leverages CLIP (Contrastive Language-Image Pre-training) for enhanced visual discovery of digital collections. Our Digital Collections Explorer can be installed locally and configured to run on a visual collection of interest on disk in just a few steps. Building upon recent advances in multimodal search techniques, our interface enables natural language queries and reverse image searches over digital collections with visual features. This paper describes the system's architecture, implementation, and application to various cultural heritage collections, demonstrating its potential for democratizing access to digital archives, especially those with impoverished metadata. We present case studies with maps, photographs, and PDFs extracted from web archives in order to demonstrate the flexibility of the Digital Collections Explorer, as well as its ease of use. We demonstrate that the Digital Collections Explorer scales to hundreds of thousands of images on a MacBook Pro with an M4 chip. Lastly, we host a public demo of Digital Collections Explorer.",
        "translated": "我们推出数字馆藏探索器（Digital Collections Explorer），这是一个基于网络的开源探索性搜索平台，它利用 CLIP（对比语言-图像预训练模型）来增强数字馆藏的视觉发现能力。该数字馆藏探索器可以本地安装，并仅需简单几步即可配置运行于磁盘上任意感兴趣的视觉藏品集。基于多模态搜索技术的最新进展，该界面支持对具有视觉特征的数字馆藏进行自然语言查询和反向图像搜索。本文描述了该系统的架构、实现及其在各种文化遗产馆藏中的应用，展示了其在促进数字档案访问民主化方面的潜力，特别是对于那些元数据匮乏的档案。我们展示了利用从网络档案中提取的地图、照片和PDF文件进行的案例研究，以证明数字馆藏探索器的灵活性和易用性。我们证明，数字馆藏探索器在搭载M4芯片的MacBook Pro上可扩展处理数十万张图像。最后，我们提供了数字馆藏探索器的公开演示。"
    },
    {
        "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2507.01915v1",
        "pub_date": "2025-07-02",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user's specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness.",
        "translated": "强化学习自人类反馈（RLHF）已成为使大型语言模型（LLMs）与人类偏好对齐的强大技术。然而，使LLMs与多样化的人类偏好有效对齐仍然是一个重大挑战，尤其当这些偏好相互冲突时。为解决此问题，我们将人类价值对齐建模为一个多目标优化问题，旨在最大化一组可能相互冲突的目标。\n\n我们引入了梯度自适应策略优化（Gradient-Adaptive Policy Optimization, GAPO），这是一种新颖的微调范式，它采用多梯度下降来使LLMs与多样化的偏好分布对齐。GAPO自适应地重新缩放每个目标的梯度，以确定一个能够最佳地平衡各目标间权衡的更新方向。此外，我们还提出了P-GAPO，它纳入了用户在不同目标上的偏好，并实现了能够更好地满足用户特定需求的帕累托解。我们的理论分析表明，GAPO收敛于多目标的帕累托最优解。在Mistral-7B上的实证结果显示，GAPO优于现有最先进（SOTA）方法，在有用性（helpfulness）和无害性（harmlessness）方面均取得了卓越性能。"
    },
    {
        "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2507.01915v1",
        "pub_date": "2025-07-02",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user's specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness.",
        "translated": "强化学习与人类反馈（RLHF）已发展成为一种强大的技术，用于使大语言模型（LLMs）与人类偏好对齐。然而，有效使大语言模型与多样化的人类偏好对齐仍是一个重大挑战，尤其当这些偏好相互冲突时。为解决此问题，我们将人类价值观对齐建模为一个多目标优化问题，旨在最大化一组可能相互冲突的目标。\n\n我们引入了梯度自适应策略优化（Gradient-Adaptive Policy Optimization, GAPO），这是一种新颖的微调范式，它采用多梯度下降来使大语言模型与多样化的偏好分布对齐。GAPO 自适应地重缩放每个目标的梯度，以确定一个能够最佳平衡各目标之间权衡的更新方向。此外，我们还引入了 P-GAPO，它融入了用户在不同目标上的偏好，从而实现了更符合用户特定需求的帕累托解。我们的理论分析表明，GAPO 能够收敛到针对多目标的帕累托最优解。在 Mistral-7B 上的实证结果表明，GAPO 优于当前最先进的方法，在有益性和无害性两方面均取得了卓越的性能。"
    },
    {
        "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System",
        "url": "http://arxiv.org/abs/2507.01872v1",
        "pub_date": "2025-07-02",
        "summary": "Existing language learning tools, even those powered by Large Language Models (LLMs), often lack support for polyglot learners to build linguistic connections across vocabularies in multiple languages, provide limited customization for individual learning paces or needs, and suffer from detrimental cognitive offloading. To address these limitations, we design Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system that supports polyglot language learning. DIY-MKG allows the user to build personalized vocabulary knowledge graphs, which are constructed by selective expansion with related words suggested by an LLM. The system further enhances learning through rich annotation capabilities and an adaptive review module that leverages LLMs for dynamic, personalized quiz generation. In addition, DIY-MKG allows users to flag incorrect quiz questions, simultaneously increasing user engagement and providing a feedback loop for prompt refinement. Our evaluation of LLM-based components in DIY-MKG shows that vocabulary expansion is reliable and fair across multiple languages, and that the generated quizzes are highly accurate, validating the robustness of DIY-MKG.",
        "translated": "现有的语言学习工具，即使是那些由大型语言模型（LLMs）驱动的工具，通常缺乏对多语种学习者的支持，无法帮助他们在多种语言的词汇之间建立语言联系；它们为个人学习进度或需求提供的个性化定制也十分有限；并且，它们还存在有害的认知卸载问题。为了解决这些局限性，我们设计了“自制多语种知识图谱”（DIY-MKG），这是一个支持多语种语言学习的开源系统。DIY-MKG允许用户构建个性化的词汇知识图谱，这些图谱通过LLM建议的相关词汇进行选择性扩展来构建。该系统通过丰富的注释功能和利用LLM进行动态、个性化测验生成的自适应复习模块，进一步增强了学习效果。此外，DIY-MKG允许用户标记错误的测验问题，这不仅提高了用户参与度，还为提示词的优化提供了反馈循环。我们对DIY-MKG中基于LLM的组件进行的评估表明，词汇扩展在多种语言中是可靠且公平的，并且生成的测验高度准确，验证了DIY-MKG的鲁棒性。"
    },
    {
        "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for\n  Deep Search",
        "url": "http://arxiv.org/abs/2507.02652v1",
        "pub_date": "2025-07-03",
        "summary": "Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.",
        "translated": "实际搜索场景中的复杂信息需求需要跨多种来源进行深度推理和知识综合，而传统检索增强生成（RAG）流水线难以有效应对。当前基于推理的方法存在一个根本局限：它们使用单一模型来同时处理高层规划和详细执行，这导致了推理效率低下和可扩展性有限。在本文中，我们引入了 HiRA，一个将战略规划与专业化执行分离的分层框架。我们的方法将复杂搜索任务分解为聚焦的子任务，将每个子任务分配给配备外部工具和推理能力的领域特定代理，并通过结构化集成机制协调结果。这种分离防止了执行细节干扰高层推理，同时使系统能够利用专业知识处理不同类型的信息。在四个复杂的、跨模态的深度搜索基准上进行的实验表明，HiRA 显著优于最先进的 RAG 和基于代理的系统。我们的结果表明在回答质量和系统效率方面都有所改进，突出了解耦规划和执行对于多步信息查询任务的有效性。我们的代码可在 https://github.com/ignorejjj/HiRA 获取。"
    },
    {
        "title": "Calibrated Recommendations: Survey and Future Directions",
        "url": "http://arxiv.org/abs/2507.02643v1",
        "pub_date": "2025-07-03",
        "summary": "The idea of calibrated recommendations is that the properties of the items that are suggested to users should match the distribution of their individual past preferences. Calibration techniques are therefore helpful to ensure that the recommendations provided to a user are not limited to a certain subset of the user's interests. Over the past few years, we have observed an increasing number of research works that use calibration for different purposes, including questions of diversity, biases, and fairness. In this work, we provide a survey on the recent developments in the area of calibrated recommendations. We both review existing technical approaches for calibration and provide an overview on empirical and analytical studies on the effectiveness of calibration for different use cases. Furthermore, we discuss limitations and common challenges when implementing calibration in practice.",
        "translated": "校准推荐的核心思想在于，推荐给用户的物品属性应与其个体过往偏好的分布相匹配。因此，校准技术有助于确保为用户提供的推荐不会局限于其兴趣的某一特定子集。在过去几年中，我们注意到越来越多的研究工作将校准应用于不同目的，如多样性、偏置和公平性等问题。\n\n本文对校准推荐领域的最新进展进行了综述。我们不仅回顾了现有的校准技术方法，还概述了关于校准在不同应用场景中有效性的实证和分析研究。此外，我们讨论了在校准的实际应用中存在的局限性与常见挑战。"
    },
    {
        "title": "Resolving CAP Through Automata-Theoretic Economic Design: A Unified\n  Mathematical Framework for Real-Time Partition-Tolerant Systems",
        "url": "http://arxiv.org/abs/2507.02464v1",
        "pub_date": "2025-07-03",
        "summary": "The CAP theorem asserts a trilemma between consistency, availability, and partition tolerance. This paper introduces a rigorous automata-theoretic and economically grounded framework that reframes the CAP trade-off as a constraint optimization problem. We model distributed systems as partition-aware state machines and embed economic incentive layers to stabilize consensus behavior across adversarially partitioned networks. By incorporating game-theoretic mechanisms into the global transition semantics, we define provable bounds on convergence, liveness, and correctness. Our results demonstrate that availability and consistency can be simultaneously preserved within bounded epsilon margins, effectively extending the classical CAP limits through formal economic control.",
        "translated": "CAP 定理断言一致性（Consistency）、可用性（Availability）和分区容错性（Partition Tolerance）之间存在三难困境。本文引入了一个严谨的、基于自动机理论和经济学基础的框架，将 CAP 权衡重新定义为一个约束优化问题。我们将分布式系统建模为分区感知状态机，并嵌入经济激励层，以稳定对抗性分区网络中的共识行为。通过将博弈论机制整合到全局转换语义中，我们定义了关于收敛性、活性和正确性的可证明界限。我们的结果表明，可用性和一致性可以在有界epsilon误差范围内同时保持，通过形式化经济控制有效拓展了经典的 CAP 限制。"
    },
    {
        "title": "Content filtering methods for music recommendation: A review",
        "url": "http://arxiv.org/abs/2507.02282v1",
        "pub_date": "2025-07-03",
        "summary": "Recommendation systems have become essential in modern music streaming platforms, shaping how users discover and engage with songs. One common approach in recommendation systems is collaborative filtering, which suggests content based on the preferences of users with similar listening patterns to the target user. However, this method is less effective on media where interactions are sparse. Music is one such medium, since the average user of a music streaming service will never listen to the vast majority of tracks. Due to this sparsity, there are several challenges that have to be addressed with other methods. This review examines the current state of research in addressing these challenges, with an emphasis on the role of content filtering in mitigating biases inherent in collaborative filtering approaches. We explore various methods of song classification for content filtering, including lyrical analysis using Large Language Models (LLMs) and audio signal processing techniques. Additionally, we discuss the potential conflicts between these different analysis methods and propose avenues for resolving such discrepancies.",
        "translated": "推荐系统在现代音乐流媒体平台中变得必不可少，它塑造了用户发现和互动歌曲的方式。推荐系统的一种常见方法是协同过滤，它根据与目标用户听歌模式相似的用户的偏好来推荐内容。然而，这种方法在交互稀疏的媒体上效果不佳。音乐就是这样一种媒体，因为音乐流媒体服务的普通用户绝大部分曲目都不会收听。由于这种稀疏性，存在一些必须通过其他方法解决的挑战。本综述探讨了解决这些挑战的当前研究现状，重点关注内容过滤在减轻协同过滤方法固有偏差方面的作用。我们探讨了用于内容过滤的各种歌曲分类方法，包括使用大型语言模型（LLMs）进行歌词分析和音频信号处理技术。此外，我们讨论了这些不同分析方法之间潜在的冲突，并提出了解决此类差异的途径。"
    },
    {
        "title": "Listwise Preference Alignment Optimization for Tail Item Recommendation",
        "url": "http://arxiv.org/abs/2507.02255v1",
        "pub_date": "2025-07-03",
        "summary": "Preference alignment has achieved greater success on Large Language Models (LLMs) and drawn broad interest in recommendation research. Existing preference alignment methods for recommendation either require explicit reward modeling or only support pairwise preference comparison. The former directly increases substantial computational costs, while the latter hinders training efficiency on negative samples. Moreover, no existing effort has explored preference alignment solutions for tail-item recommendation. To bridge the above gaps, we propose LPO4Rec, which extends the Bradley-Terry model from pairwise comparison to listwise comparison, to improve the efficiency of model training. Specifically, we derive a closed form optimal policy to enable more efficient and effective training without explicit reward modeling. We also present an adaptive negative sampling and reweighting strategy to prioritize tail items during optimization and enhance performance in tail-item recommendations. Besides, we theoretically prove that optimizing the listwise preference optimization (LPO) loss is equivalent to maximizing the upper bound of the optimal reward. Our experiments on three public datasets show that our method outperforms 10 baselines by a large margin, achieving up to 50% performance improvement while reducing 17.9% GPU memory usage when compared with direct preference optimization (DPO) in tail-item recommendation. Our code is available at https://github.com/Yuhanleeee/LPO4Rec.",
        "translated": "偏好对齐在大型语言模型（LLM）上取得了显著成功，并在推荐系统研究中引起了广泛兴趣。现有推荐系统中的偏好对齐方法要么需要显式奖励建模，要么只支持成对偏好比较。前者直接增加了巨大的计算成本，而后者则阻碍了在负样本上的训练效率。此外，现有工作尚未探索针对长尾商品推荐的偏好对齐解决方案。\n\n为了弥补上述空白，我们提出了LPO4Rec，该方法将Bradley-Terry模型从成对比较扩展到列表式比较，以提高模型训练效率。具体来说，我们推导了一个闭式最优策略，以实现在无需显式奖励建模的情况下进行更高效且有效的训练。我们还提出了一种自适应负采样和重加权策略，以在优化过程中优先考虑长尾商品，并提升长尾商品推荐的性能。此外，我们理论证明，优化列表式偏好优化（LPO）损失等价于最大化最优奖励的上限。\n\n我们在三个公开数据集上的实验表明，我们的方法大幅超越了10个基线模型，性能提升高达50%，同时在长尾商品推荐方面与直接偏好优化（DPO）相比，降低了17.9%的GPU内存使用。我们的代码已在 https://github.com/Yuhanleeee/LPO4Rec 公开。"
    },
    {
        "title": "When LLMs Disagree: Diagnosing Relevance Filtering Bias and Retrieval\n  Divergence in SDG Search",
        "url": "http://arxiv.org/abs/2507.02139v1",
        "pub_date": "2025-07-02",
        "summary": "Large language models (LLMs) are increasingly used to assign document relevance labels in information retrieval pipelines, especially in domains lacking human-labeled data. However, different models often disagree on borderline cases, raising concerns about how such disagreement affects downstream retrieval. This study examines labeling disagreement between two open-weight LLMs, LLaMA and Qwen, on a corpus of scholarly abstracts related to Sustainable Development Goals (SDGs) 1, 3, and 7. We isolate disagreement subsets and examine their lexical properties, rank-order behavior, and classification predictability. Our results show that model disagreement is systematic, not random: disagreement cases exhibit consistent lexical patterns, produce divergent top-ranked outputs under shared scoring functions, and are distinguishable with AUCs above 0.74 using simple classifiers. These findings suggest that LLM-based filtering introduces structured variability in document retrieval, even under controlled prompting and shared ranking logic. We propose using classification disagreement as an object of analysis in retrieval evaluation, particularly in policy-relevant or thematic search tasks.",
        "translated": "大型语言模型（LLM）正越来越多地被用于在信息检索流程中分配文档相关性标签，尤其是在缺乏人工标注数据的领域。然而，不同模型在边缘案例上经常存在分歧，这引发了人们对这种分歧如何影响下游检索的担忧。本研究考察了两个开放权重LLM（LLaMA和Qwen）在与可持续发展目标（SDG）1、3和7相关的学术摘要语料库上的标签分歧。我们分离出分歧子集，并考察它们的词汇特性、排名行为和分类可预测性。我们的结果表明，模型分歧是系统性的，而非随机的：分歧案例表现出一致的词汇模式，在共同的评分函数下产生分歧的顶部排名输出，并且可以使用简单分类器以高于0.74的AUC值进行区分。这些发现表明，即使在受控提示和共享排名逻辑下，基于LLM的过滤也会在文档检索中引入结构化变异性。我们建议将分类分歧作为检索评估中的一个分析对象，尤其是在政策相关或主题搜索任务中。"
    },
    {
        "title": "The Future is Agentic: Definitions, Perspectives, and Open Challenges of\n  Multi-Agent Recommender Systems",
        "url": "http://arxiv.org/abs/2507.02097v1",
        "pub_date": "2025-07-02",
        "summary": "Large language models (LLMs) are rapidly evolving from passive engines of text generation into agentic entities that can plan, remember, invoke external tools, and co-operate with one another. This perspective paper investigates how such LLM agents (and societies thereof) can transform the design space of recommender systems.   We introduce a unified formalism that (i) models an individual agent as a tuple comprising its language core, tool set, and hierarchical memory, and (ii) captures a multi-agent recommender as a triple of agents, shared environment, and communication protocol. Within this framework, we present four end-to-end use cases-interactive party planning, synthetic user-simulation for offline evaluation, multi-modal furniture recommendation, and brand-aligned explanation generation-each illustrating a distinct capability unlocked by agentic orchestration.   We then surface five cross-cutting challenge families: protocol complexity, scalability, hallucination and error propagation, emergent misalignment (including covert collusion), and brand compliance.   For each, we formalize the problem, review nascent mitigation strategies, and outline open research questions. The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools that keep pace with this new degree of autonomy. By unifying agentic abstractions with recommender objectives, the paper lays the groundwork for the next generation of personalized, trustworthy, and context-rich recommendation services.",
        "translated": "大型语言模型（LLM）正从被动的文本生成引擎迅速发展为具备规划、记忆、调用外部工具及相互协作能力的智能体实体。本视角论文探讨了这类LLM智能体（及其群落）如何重塑推荐系统的设计空间。\n\n我们引入了一个统一的形式化框架，该框架（i）将单个智能体建模为由其语言核心、工具集和分层记忆组成的元组，以及（ii）将多智能体推荐系统建模为由智能体、共享环境和通信协议构成的三元组。在此框架内，我们提出了四个端到端用例——交互式派对规划、用于离线评估的合成用户模拟、多模态家具推荐和品牌对齐的解释生成——每个用例都展示了智能体编排所解锁的独特能力。\n\n随后，我们提出了五大跨领域挑战：协议复杂性、可伸缩性、幻觉与错误传播、涌现的错位（包括隐蔽串通）以及品牌合规性。针对每个挑战，我们都对其问题进行了形式化描述，回顾了新兴的缓解策略，并概述了开放的研究问题。本文成果既是一份蓝图，也是一份议程：一份展示如何将记忆增强、工具使用的LLM智能体组合成稳健推荐管道的蓝图；一份邀请推荐系统社区开发基准、理论保证和治理工具以与这种新程度的自主性发展同步的议程。通过将智能体抽象与推荐目标相结合，本文为下一代个性化、可信赖且上下文丰富的推荐服务奠定了基础。"
    },
    {
        "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large\n  Language Model",
        "url": "http://arxiv.org/abs/2507.02822v1",
        "pub_date": "2025-07-03",
        "summary": "With the widespread adoption of large language models (LLMs) in practical applications, selecting an appropriate model requires balancing not only performance but also operational cost. The emergence of reasoning-capable models has further widened the cost gap between \"thinking\" (high reasoning) and \"non-thinking\" (fast, low-cost) modes. In this work, we reveal that approximately 58% of medical questions can be accurately answered by the non-thinking mode alone, without requiring the high-cost reasoning process. This highlights a clear dichotomy in problem complexity and suggests that dynamically routing queries to the appropriate mode based on complexity could optimize accuracy, cost-efficiency, and overall user experience. Based on this, we further propose SynapseRoute, a machine learning-based dynamic routing framework that intelligently assigns input queries to either thinking or non-thinking modes. Experimental results on several medical datasets demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs. 0.8272) compared to the thinking mode alone but also reduces inference time by 36.8% and token consumption by 39.66%. Importantly, qualitative analysis indicates that over-reasoning on simpler queries can lead to unnecessary delays and even decreased accuracy, a pitfall avoided by our adaptive routing. Finally, this work further introduces the Accuracy-Inference-Token (AIT) index to comprehensively evaluate the trade-offs among accuracy, latency, and token cost.",
        "translated": "随着大型语言模型（LLMs）在实际应用中的广泛普及，选择合适的模型不仅需要平衡性能，还需要考虑运营成本。具备推理能力的模型出现后，进一步拉大了“思考型”（高推理）和“非思考型”（快速、低成本）模式之间的成本差距。在这项工作中，我们发现大约58%的医学问题仅凭非思考型模式即可准确回答，无需启动高成本的推理过程。这突出表明了问题复杂性存在清晰的二元性，并暗示基于复杂性将查询动态路由到适当模式，能够优化准确性、成本效益和整体用户体验。\n\n基于此，我们进一步提出了SynapseRoute，这是一个基于机器学习的动态路由框架，能够智能地将输入查询分配给思考型或非思考型模式。在多个医学数据集上的实验结果表明，与仅使用思考型模式相比，SynapseRoute不仅提高了整体准确性（0.8390 对 0.8272），而且将推理时间缩短了36.8%，并将token消耗降低了39.66%。重要的是，定性分析表明，对简单查询进行过度推理可能导致不必要的延迟甚至降低准确性，而我们的自适应路由则避免了这一缺陷。最后，这项工作还引入了准确性-推理-Token（AIT）指标，以全面评估准确性、延迟和Token成本之间的权衡。"
    },
    {
        "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge\n  Injection to All Users",
        "url": "http://arxiv.org/abs/2507.02850v1",
        "pub_date": "2025-07-03",
        "summary": "We describe a vulnerability in language models (LMs) trained with user feedback, whereby a single user can persistently alter LM knowledge and behavior given only the ability to provide prompts and upvote / downvote feedback on LM outputs. To implement the attack, the attacker prompts the LM to stochastically output either a \"poisoned\" or benign response, then upvotes the poisoned response or downvotes the benign one. When feedback signals are used in a subsequent preference tuning behavior, LMs exhibit increased probability of producing poisoned responses even in contexts without malicious prompts. We show that this attack can be used to (1) insert factual knowledge the model did not previously possess, (2) modify code generation patterns in ways that introduce exploitable security flaws, and (3) inject fake financial news. Our finding both identifies a new qualitative feature of language model preference tuning (showing that it even highly restricted forms of preference data can be used to exert fine-grained control over behavior), and a new attack mechanism for LMs trained with user feedback (extending work on pretraining-time data poisoning and deployment-time prompt injection).",
        "translated": "我们描述了一种在通过用户反馈训练的语言模型（LM）中存在的漏洞，使得单个用户仅通过提供提示以及对LM输出进行赞成/反对反馈的能力，就能持续改变LM的知识和行为。为实现该攻击，攻击者提示LM随机生成“投毒”或良性响应，然后赞成投毒响应，或反对良性响应。当反馈信号被用于后续的偏好微调时，LM即使在没有恶意提示的上下文中，也会表现出生成投毒响应的更高概率。我们表明这种攻击可用于：(1) 插入模型之前不具备的事实知识；(2) 修改代码生成模式，以引入可利用的安全漏洞；以及 (3) 注入虚假金融新闻。我们的发现既揭示了语言模型偏好微调的一个新的定性特征（表明即使是高度受限的偏好数据形式也可用于对行为施加细粒度控制），也提出了一种针对通过用户反馈训练的LM的新型攻击机制（扩展了预训练时数据投毒和部署时提示注入的工作）。"
    },
    {
        "title": "In-Context Learning as an Effective Estimator of Functional Correctness\n  of LLM-Generated Code",
        "url": "http://arxiv.org/abs/2507.05200v1",
        "pub_date": "2025-07-07",
        "summary": "When applying LLM-based code generation to software development projects that follow a feature-driven or rapid application development approach, it becomes necessary to estimate the functional correctness of the generated code in the absence of test cases. Just as a user selects a relevant document from a ranked list of retrieved ones, a software generation workflow requires a developer to choose (and potentially refine) a generated solution from a ranked list of alternative solutions, ordered by their posterior likelihoods. This implies that estimating the quality of a ranked list -- akin to estimating \"relevance\" for query performance prediction (QPP) in IR -- is also crucial for generative software development, where quality is defined in terms of \"functional correctness\". In this paper, we propose an in-context learning (ICL) based approach for code quality estimation. Our findings demonstrate that providing few-shot examples of functionally correct code from a training set enhances the performance of existing QPP approaches as well as a zero-shot-based approach for code quality estimation.",
        "translated": "在将基于大模型的代码生成应用于遵循功能驱动或快速应用开发方法的软件开发项目时，在缺少测试用例的情况下，估算生成代码的功能正确性变得至关重要。正如用户从检索到的排序结果列表中选择相关文档一样，软件生成工作流也要求开发者从一个依照后验概率排序的备选解决方案列表中，选择（并可能细化）一个生成的解决方案。这表明，估算排序列表的质量——类似于信息检索 (IR) 中为查询性能预测 (QPP) 估算“相关性”——对于生成式软件开发也至关重要，其中质量被定义为“功能正确性”。在本文中，我们提出了一种基于上下文学习 (ICL) 的代码质量估算方法。我们的研究结果表明，从训练集中提供功能正确的少样本代码示例，可以提升现有 QPP 方法以及基于零样本的代码质量估算方法的性能。"
    },
    {
        "title": "Do We Really Need Specialization? Evaluating Generalist Text Embeddings\n  for Zero-Shot Recommendation and Search",
        "url": "http://arxiv.org/abs/2507.05006v2",
        "pub_date": "2025-07-07",
        "summary": "Pre-trained language models (PLMs) are widely used to derive semantic representations from item metadata in recommendation and search. In sequential recommendation, PLMs enhance ID-based embeddings through textual metadata, while in product search, they align item characteristics with user intent. Recent studies suggest task and domain-specific fine-tuning are needed to improve representational power. This paper challenges this assumption, showing that Generalist Text Embedding Models (GTEs), pre-trained on large-scale corpora, can guarantee strong zero-shot performance without specialized adaptation. Our experiments demonstrate that GTEs outperform traditional and fine-tuned models in both sequential recommendation and product search. We attribute this to a superior representational power, as they distribute features more evenly across the embedding space. Finally, we show that compressing embedding dimensions by focusing on the most informative directions (e.g., via PCA) effectively reduces noise and improves the performance of specialized models. To ensure reproducibility, we provide our repository at https://split.to/gte4ps.",
        "translated": "预训练语言模型（PLMs）被广泛应用于从推荐和搜索中的物品元数据中提取语义表示。在序列推荐中，PLMs通过文本元数据增强了基于ID的嵌入；而在商品搜索中，它们将物品特征与用户意图对齐。近期研究表明，需要进行任务和领域特定的微调以提高表示能力。本文挑战了这一假设，表明在大型语料库上预训练的通用文本嵌入模型（GTEs）无需专门适应即可保证强大的零样本性能。我们的实验表明，GTEs在序列推荐和商品搜索中均优于传统模型和微调模型。我们将此归因于它们更优越的表示能力，因为它们在嵌入空间中更均匀地分布特征。最后，我们表明，通过关注信息量最大的方向（例如，通过PCA）来压缩嵌入维度，可以有效减少噪声并提高专门模型的性能。为确保可复现性，我们提供了我们的代码库：https://split.to/gte4ps。"
    },
    {
        "title": "Interest Networks (iNETs) for Cities: Cross-Platform Insights and Urban\n  Behavior Explanations",
        "url": "http://arxiv.org/abs/2507.04995v1",
        "pub_date": "2025-07-07",
        "summary": "Location-Based Social Networks (LBSNs) provide a rich foundation for modeling urban behavior through iNETs (Interest Networks), which capture how user interests are distributed throughout urban spaces. This study compares iNETs across platforms (Google Places and Foursquare) and spatial granularities, showing that coarser levels reveal more consistent cross-platform patterns, while finer granularities expose subtle, platform-specific behaviors. Our analysis finds that, in general, user interest is primarily shaped by geographic proximity and venue similarity, while socioeconomic and political contexts play a lesser role. Building on these insights, we develop a multi-level, explainable recommendation system that predicts high-interest urban regions for different user types. The model adapts to behavior profiles -- such as explorers, who are driven by proximity, and returners, who prefer familiar venues -- and provides natural-language explanations using explainable AI (XAI) techniques. To support our approach, we introduce h3-cities, a tool for multi-scale spatial analysis, and release a public demo for interactively exploring personalized urban recommendations. Our findings contribute to urban mobility research by providing scalable, context-aware, and interpretable recommendation systems.",
        "translated": "定位社交网络（LBSNs）为通过兴趣网络（iNETs）建模城市行为提供了丰富的土壤，iNETs能够捕捉用户兴趣在城市空间中的分布方式。本研究比较了不同平台（如Google Places和Foursquare）及不同空间粒度下的iNETs，结果表明，较粗的粒度揭示了更一致的跨平台模式，而较细的粒度则暴露了微妙的、特定于平台的行为。我们的分析发现，总体而言，用户兴趣主要受地理邻近性和场所相似性的影响，而社会经济和政治背景则作用较小。\n\n基于这些洞察，我们开发了一个多层次、可解释的推荐系统，能够预测不同用户类型的高兴趣城市区域。该模型能够适应不同的行为画像——例如受邻近性驱动的“探索者”和偏好熟悉场所的“回头客”——并利用可解释人工智能（XAI）技术提供自然语言解释。为支持我们的方法，我们引入了h3-cities——一个用于多尺度空间分析的工具，并发布了一个公开演示，供用户交互式探索个性化城市推荐。我们的研究结果通过提供可伸缩、上下文感知且可解释的推荐系统，为城市流动性研究做出了贡献。"
    },
    {
        "title": "SIGIR 2025 -- LiveRAG Challenge Report",
        "url": "http://arxiv.org/abs/2507.04942v2",
        "pub_date": "2025-07-07",
        "summary": "The LiveRAG Challenge at SIGIR 2025, held between March and May 2025, provided a competitive platform for advancing Retrieval-Augmented Generation (RAG) technologies. Participants from academia and industry were invited to develop a RAG-based question-answering system using a fixed corpus (Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal was to facilitate challenging comparisons of retrieval and prompting strategies. During the Live Challenge Day, 70 teams from 27 different countries provided answers and supportive information to 500 unseen questions within a strict two-hour time window. Evaluation was conducted in two stages: first an automated LLM-as-a-judge approach was used to compute correctness and faithfulness score, then a manual review of top ranked submissions was conducted. The finalists were announced on June 12, 2025, with prizes awarded during the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.",
        "translated": "SIGIR 2025 LiveRAG 挑战赛于2025年3月至5月期间举行，为推动检索增强生成（RAG）技术的发展提供了一个竞争平台。挑战赛邀请了来自学术界和工业界的参与者，利用固定的语料库（Fineweb-10BT）和一个通用的开源大型语言模型（Falcon3-10B-Instruct）来开发基于RAG的问答系统。其目标是促进对检索和提示策略的深入比较。在现场挑战日，来自27个不同国家的70支团队在严格的两小时内，为500个未曾见过的问题提供了答案和支持信息。评估分两个阶段进行：首先采用自动化的大型语言模型作为评委（LLM-as-a-judge）的方法计算答案的正确性和忠实度分数，随后对排名靠前的提交进行了人工审核。决赛入围者于2025年6月12日公布，奖项在意大利帕多瓦举行的SIGIR 2025 LiveRAG 研讨会期间颁发。"
    },
    {
        "title": "SimLab: A Platform for Simulation-based Evaluation of Conversational\n  Information Access Systems",
        "url": "http://arxiv.org/abs/2507.04888v1",
        "pub_date": "2025-07-07",
        "summary": "Research on interactive and conversational information access systems, including search engines, recommender systems, and conversational assistants, has been hindered by the difficulty in evaluating such systems with reproducible experiments. User simulation provides a promising solution, but there is a lack of infrastructure and tooling to support this kind of evaluation. To facilitate simulation-based evaluation of conversational information access systems, we introduce SimLab, the first cloud-based platform to provide a centralized general solution for the community to benchmark both conversational systems and user simulators in a controlled and reproducible environment. We articulate requirements for such a platform and propose a general infrastructure to address these requirements. We then present the design and implementation of an initial version of SimLab and showcase its features with an initial evaluation task of conversational movie recommendation, which is made publicly available. Furthermore, we discuss the sustainability of the platform and its future opportunities. This paper is a call for the community to contribute to the platform to drive progress in the field of conversational information access and user simulation.",
        "translated": "鉴于在可复现实验中评估交互式会话信息访问系统（包括搜索引擎、推荐系统和会话助手）存在困难，相关研究进展一直受到阻碍。用户模拟提供了一个有前景的解决方案，但目前缺乏支持此类评估的基础设施和工具。为了促进基于模拟的会话信息访问系统评估，我们推出了SimLab，这是首个基于云的平台，旨在为社区提供一个集中式通用解决方案，以便在受控且可复现的环境中对会话系统和用户模拟器进行基准测试。本文阐明了对此类平台的需求，并提出了一个通用基础设施来满足这些需求。接着，我们介绍了SimLab初始版本的设计与实现，并通过一项已公开的会话式电影推荐初步评估任务展示了其功能。此外，我们还讨论了平台的可持续性及其未来发展机遇。本文呼吁社区为该平台贡献力量，以共同推动会话信息访问和用户模拟领域的进步。"
    },
    {
        "title": "Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking\n  Distillation",
        "url": "http://arxiv.org/abs/2507.04820v1",
        "pub_date": "2025-07-07",
        "summary": "While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is one of the most effective zero-shot document ranking methods, it has a quadratic computational complexity with respect to the number of documents to be ranked, as it requires an enumeration over all possible document pairs. Consequently, the outstanding ranking performance of PRP has remained unreachable for most real-world ranking applications.   In this work, we propose to harness the effectiveness of PRP through pairwise distillation. Specifically, we distill a pointwise student ranker from pairwise teacher labels generated by PRP, resulting in an efficient student model that retains the performance of PRP with substantially lower computational costs. Furthermore, we find that the distillation process can be made sample-efficient: with only 2% of pairs, we are able to obtain the same performance as using all pairs for teacher labels. Thus, our novel approach provides a solution to harness the ranking performance of PRP without incurring high computational costs during both distillation and serving.",
        "translated": "尽管大语言模型（LLMs）的对偶排序提示（Pairwise Ranking Prompting, PRP）是目前最有效的零样本文档排序方法之一，但它对需要排序的文档数量具有二次计算复杂度，因为它需要枚举所有可能的文档对。因此，PRP 出色的排序性能在大多数实际排序应用中仍难以企及。\n\n在这项工作中，我们提出通过对偶蒸馏来利用PRP的有效性。具体而言，我们从PRP生成的对偶教师标签中蒸馏出一个逐点学生排序器，从而得到一个高效的学生模型，该模型保留了PRP的性能，同时计算成本大幅降低。此外，我们发现蒸馏过程可以做到样本高效：仅使用2%的对，我们就能获得与使用所有对作为教师标签相同的性能。因此，我们的新颖方法提供了一种解决方案，可以在蒸馏和部署（serving）阶段，在不产生高昂计算成本的情况下，利用PRP的排序性能。"
    },
    {
        "title": "\"This Suits You the Best\": Query Focused Comparative Explainable\n  Summarization",
        "url": "http://arxiv.org/abs/2507.04733v1",
        "pub_date": "2025-07-07",
        "summary": "Product recommendations inherently involve comparisons, yet traditional opinion summarization often fails to provide holistic comparative insights. We propose the novel task of generating Query-Focused Comparative Explainable Summaries (QF-CES) using Multi-Source Opinion Summarization (M-OS). To address the lack of query-focused recommendation datasets, we introduce MS-Q2P, comprising 7,500 queries mapped to 22,500 recommended products with metadata. We leverage Large Language Models (LLMs) to generate tabular comparative summaries with query-specific explanations. Our approach is personalized, privacy-preserving, recommendation engine-agnostic, and category-agnostic. M-OS as an intermediate step reduces inference latency approximately by 40% compared to the direct input approach (DIA), which processes raw data directly. We evaluate open-source and proprietary LLMs for generating and assessing QF-CES. Extensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity, faithfulness, informativeness, format adherence, and query relevance) showed an average Spearman correlation of 0.74 with human judgments, indicating its potential for QF-CES evaluation.",
        "translated": "产品推荐天然地包含比较，然而传统的意见摘要往往无法提供全面的比较性洞察。我们提出了一项新颖的任务：利用多源意见摘要 (M-OS) 生成查询导向的比较性可解释摘要 (QF-CES)。为解决查询导向推荐数据集的缺乏，我们引入了 MS-Q2P，该数据集包含 7,500 个查询，这些查询映射到 22,500 个带有元数据的推荐产品。我们利用大语言模型 (LLMs) 生成带有查询相关解释的表格形式比较摘要。我们的方法具有个性化、隐私保护、推荐引擎无关和类别无关的特点。M-OS 作为中间步骤，与直接处理原始数据的直接输入方法 (DIA) 相比，将推理延迟大约减少了 40%。我们评估了开源和专有 LLMs 在生成和评估 QF-CES 方面的表现。使用 QF-CES-PROMPT 在 5 个维度（清晰度、忠实性、信息量、格式依从性和查询相关性）上进行的广泛评估显示，其与人工判断的平均 Spearman 相关性达到 0.74，表明其在 QF-CES 评估中具有巨大潜力。"
    },
    {
        "title": "FindRec: Stein-Guided Entropic Flow for Multi-Modal Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2507.04651v1",
        "pub_date": "2025-07-07",
        "summary": "Modern recommendation systems face significant challenges in processing multimodal sequential data, particularly in temporal dynamics modeling and information flow coordination. Traditional approaches struggle with distribution discrepancies between heterogeneous features and noise interference in multimodal signals. We propose \\textbf{FindRec}~ (\\textbf{F}lexible unified \\textbf{in}formation \\textbf{d}isentanglement for multi-modal sequential \\textbf{Rec}ommendation), introducing a novel \"information flow-control-output\" paradigm. The framework features two key innovations: (1) A Stein kernel-based Integrated Information Coordination Module (IICM) that theoretically guarantees distribution consistency between multimodal features and ID streams, and (2) A cross-modal expert routing mechanism that adaptively filters and combines multimodal features based on their contextual relevance. Our approach leverages multi-head subspace decomposition for routing stability and RBF-Stein gradient for unbiased distribution alignment, enhanced by linear-complexity Mamba layers for efficient temporal modeling. Extensive experiments on three real-world datasets demonstrate FindRec's superior performance over state-of-the-art baselines, particularly in handling long sequences and noisy multimodal inputs. Our framework achieves both improved recommendation accuracy and enhanced model interpretability through its modular design. The implementation code is available anonymously online for easy reproducibility~\\footnote{https://github.com/Applied-Machine-Learning-Lab/FindRec}.",
        "translated": "现代推荐系统在处理多模态序列数据方面面临严峻挑战，尤其是在时序动态建模和信息流协调方面。传统方法难以处理异构特征之间的分布差异以及多模态信号中的噪声干扰。我们提出了**FindRec**（**F**lexible unified **in**formation **d**isentanglement for multi-modal sequential **Rec**ommendation，即“多模态序列推荐中的灵活统一信息解耦”），引入了一种新颖的“信息流-控制-输出”范式。\n\n该框架包含两项关键创新：(1) 一个基于Stein核的集成信息协调模块（IICM），在理论上保证了多模态特征与ID流之间的分布一致性；(2) 一个跨模态专家路由机制，能够基于多模态特征的上下文相关性自适应地过滤和组合它们。我们的方法利用多头子空间分解以确保路由的稳定性，并利用RBF-Stein梯度实现无偏分布对齐，同时通过线性复杂度的Mamba层增强了高效的时序建模能力。\n\n在三个真实世界数据集上进行的大量实验表明，FindRec在处理长序列和噪声多模态输入方面表现尤为突出，性能超越了最先进的基线模型。我们的框架通过其模块化设计，同时实现了推荐准确性的提升和模型可解释性的增强。实施代码已在线匿名提供，方便复现。"
    },
    {
        "title": "Heterogeneous User Modeling for LLM-based Recommendation",
        "url": "http://arxiv.org/abs/2507.04626v1",
        "pub_date": "2025-07-07",
        "summary": "Leveraging Large Language Models (LLMs) for recommendation has demonstrated notable success in various domains, showcasing their potential for open-domain recommendation. A key challenge to advancing open-domain recommendation lies in effectively modeling user preferences from users' heterogeneous behaviors across multiple domains. Existing approaches, including ID-based and semantic-based modeling, struggle with poor generalization, an inability to compress noisy interactions effectively, and the domain seesaw phenomenon. To address these challenges, we propose a Heterogeneous User Modeling (HUM) method, which incorporates a compression enhancer and a robustness enhancer for LLM-based recommendation. The compression enhancer uses a customized prompt to compress heterogeneous behaviors into a tailored token, while a masking mechanism enhances cross-domain knowledge extraction and understanding. The robustness enhancer introduces a domain importance score to mitigate the domain seesaw phenomenon by guiding domain optimization. Extensive experiments on heterogeneous datasets validate that HUM effectively models user heterogeneity by achieving both high efficacy and robustness, leading to superior performance in open-domain recommendation.",
        "translated": "大语言模型（LLM）在推荐领域的应用已在多个领域取得了显著成功，展现了它们在开放域推荐方面的巨大潜力。推动开放域推荐发展的关键挑战之一在于如何有效地从用户在多个领域中的异构行为中建模用户偏好。现有方法，包括基于ID和基于语义的建模，普遍存在泛化能力差、难以有效压缩噪声交互以及领域跷跷板现象等问题。为解决这些挑战，我们提出了一种异构用户建模（HUM）方法，该方法为基于LLM的推荐引入了压缩增强器和鲁棒性增强器。压缩增强器通过定制化提示词将异构行为压缩成一个定制化token，同时引入掩码机制以增强跨领域知识的提取与理解。鲁棒性增强器则引入了领域重要性分数，通过引导领域优化来缓解领域跷跷板现象。在异构数据集上进行的大量实验验证了HUM通过实现高有效性（efficacy）和鲁棒性，能够有效地建模用户异构性，从而在开放域推荐中取得了卓越的性能。"
    },
    {
        "title": "Hierarchical Intent-guided Optimization with Pluggable LLM-Driven\n  Semantics for Session-based Recommendation",
        "url": "http://arxiv.org/abs/2507.04623v1",
        "pub_date": "2025-07-07",
        "summary": "Session-based Recommendation (SBR) aims to predict the next item a user will likely engage with, using their interaction sequence within an anonymous session. Existing SBR models often focus only on single-session information, ignoring inter-session relationships and valuable cross-session insights. Some methods try to include inter-session data but struggle with noise and irrelevant information, reducing performance. Additionally, most models rely on item ID co-occurrence and overlook rich semantic details, limiting their ability to capture fine-grained item features. To address these challenges, we propose a novel hierarchical intent-guided optimization approach with pluggable LLM-driven semantic learning for session-based recommendations, called HIPHOP. First, we introduce a pluggable embedding module based on large language models (LLMs) to generate high-quality semantic representations, enhancing item embeddings. Second, HIPHOP utilizes graph neural networks (GNNs) to model item transition relationships and incorporates a dynamic multi-intent capturing module to address users' diverse interests within a session. Additionally, we design a hierarchical inter-session similarity learning module, guided by user intent, to capture global and local session relationships, effectively exploring users' long-term and short-term interests. To mitigate noise, an intent-guided denoising strategy is applied during inter-session learning. Finally, we enhance the model's discriminative capability by using contrastive learning to optimize session representations. Experiments on multiple datasets show that HIPHOP significantly outperforms existing methods, demonstrating its effectiveness in improving recommendation quality. Our code is available: https://github.com/hjx159/HIPHOP.",
        "translated": "会话推荐（Session-based Recommendation, SBR）旨在利用用户在匿名会话中的交互序列，预测用户下一个可能感兴趣的物品。现有SBR模型通常只关注单会话信息，忽略了会话间关系和有价值的跨会话洞察。尽管一些方法尝试纳入会话间数据，但它们往往受噪声和无关信息困扰，从而降低了性能。此外，大多数模型依赖物品ID共现，忽略了丰富的语义细节，限制了其捕获细粒度物品特征的能力。\n\n为解决这些挑战，我们提出了一种新颖的、面向会话推荐的分层意图引导优化方法，并引入了可插拔的LLM驱动语义学习，命名为HIPHOP。首先，我们引入了一个基于大型语言模型（LLM）的可插拔嵌入模块，用于生成高质量的语义表示，从而增强物品嵌入。其次，HIPHOP利用图神经网络（GNN）建模物品转换关系，并结合一个动态多意图捕获模块，以处理用户在会话内的多样化兴趣。此外，我们设计了一个由用户意图引导的分层会话间相似性学习模块，以捕获全局和局部会话关系，有效探索用户的长期和短期兴趣。为缓解噪声，在会话间学习过程中应用了意图引导去噪策略。最后，我们通过使用对比学习优化会话表示，增强了模型的判别能力。在多个数据集上的实验表明，HIPHOP显著优于现有方法，证明了其在提升推荐质量方面的有效性。我们的代码已开源：https://github.com/hjx159/HIPHOP。"
    },
    {
        "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions",
        "url": "http://arxiv.org/abs/2507.05257v1",
        "pub_date": "2025-07-07",
        "summary": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and conflict resolution. Existing datasets either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Furthermore, no existing benchmarks cover all four competencies. Therefore, we introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark combines reformulated existing datasets with newly constructed ones, covering the above four memory competencies, providing a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.",
        "translated": "近期针对大语言模型（LLM）智能体的基准测试主要侧重于评估其推理、规划和执行能力，然而，另一个关键组成部分——记忆（涵盖智能体如何记忆、更新和检索长期信息）——由于缺乏基准而未得到充分评估。我们将具备记忆机制的智能体称为记忆智能体。\n\n在本文中，我们识别了对记忆智能体至关重要的四项核心能力：准确检索、测试时学习、长期理解和冲突解决。现有数据集要么依赖于有限的上下文长度，要么是为静态、长上下文设置（如基于书籍的问答）量身定制的，这些都无法反映记忆智能体逐步积累信息的交互式、多轮性质。此外，没有任何现有基准能涵盖所有这四项能力。\n\n因此，我们引入了MemoryAgentBench，这是一个专门为记忆智能体设计的新基准。我们的基准结合了重新制定的现有数据集和新构建的数据集，涵盖了上述四项记忆能力，为评估记忆质量提供了一个系统且具有挑战性的测试平台。我们评估了多种记忆智能体，从简单的基于上下文和检索增强生成（RAG）系统，到具备外部记忆模块和工具集成的高级智能体。经验结果表明，当前方法未能完全掌握所有这四项能力，这强调了需要进一步研究针对LLM智能体的全面记忆机制。"
    },
    {
        "title": "Pre-Trained Policy Discriminators are General Reward Models",
        "url": "http://arxiv.org/abs/2507.05197v1",
        "pub_date": "2025-07-07",
        "summary": "We offer a novel perspective on reward modeling by formulating it as a policy discriminator, which quantifies the difference between two policies to generate a reward signal, guiding the training policy towards a target policy with desired behaviors. Based on this conceptual insight, we propose a scalable pre-training method named Policy Discriminative Learning (POLAR), which trains a reward model (RM) to discern identical policies and discriminate different ones. Unlike traditional reward modeling methods relying on absolute preferences, POLAR captures the relative difference between one policy and an arbitrary target policy, which is a scalable, high-level optimization objective suitable for modeling generic ranking relationships. Leveraging the POLAR pre-training paradigm, we present a series of RMs with parameter scales from 1.8B to 7B. Empirical results show that POLAR substantially outperforms traditional non-pre-trained methods, significantly enhancing RM performance. For instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on STEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA baselines. POLAR also shows robust generalization capabilities in RLHF using Reinforcement Fine-tuning (RFT), providing reliable reward signals and markedly enhancing policy performance--improving LLaMa3.1-8B from an average of 47.36% to 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover, scaling experiments reveal a clear power-law relationship between computation and performance, supported by linear correlation coefficients approaching 0.99. The impressive performance, strong generalization, and scaling properties suggest that POLAR is a promising direction for developing general and strong reward models.",
        "translated": "我们提出了一种新颖的奖励建模视角，将其表述为策略判别器。该判别器量化了两种策略之间的差异，以生成奖励信号，引导训练策略趋向于具有期望行为的目标策略。基于这一概念性洞察，我们提出了一种可扩展的预训练方法，名为策略判别学习（Policy Discriminative Learning, POLAR）。POLAR通过训练奖励模型（RM）来识别相同策略并区分不同策略。与传统依赖绝对偏好的奖励建模方法不同，POLAR捕捉的是一个策略与任意目标策略之间的相对差异。这是一种可扩展、高层次的优化目标，适用于建模通用的排序关系。\n\n利用POLAR预训练范式，我们提出了一系列参数规模从18亿到70亿的奖励模型。实证结果表明，POLAR显著优于传统的非预训练方法，极大地提升了奖励模型性能。例如，与现有最佳（SOTA）基线相比，POLAR-7B在STEM任务上能将偏好准确率从54.8%提高到81.0%，在创意写作任务上从57.9%提高到85.5%。POLAR在使用强化微调（Reinforcement Fine-tuning, RFT）的RLHF（人类反馈强化学习）中也展现出强大的泛化能力，提供了可靠的奖励信号，并显著增强了策略性能——在20个基准测试中，将LLaMa3.1-8B的平均性能从47.36%提升到56.33%，将Qwen2.5-32B的平均性能从64.49%提升到70.47%。此外，扩展性实验揭示了计算与性能之间存在清晰的幂律关系，且其线性相关系数接近0.99。其卓越的性能、强大的泛化能力和良好的扩展特性表明，POLAR是开发通用且强大的奖励模型的一个有前途的方向。"
    }
]