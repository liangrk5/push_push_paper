[
    {
        "title": "RecGPT: A Foundation Model for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2506.06270v1",
        "pub_date": "2025-06-06",
        "summary": "This work addresses a fundamental barrier in recommender systems: the inability to generalize across domains without extensive retraining. Traditional ID-based approaches fail entirely in cold-start and cross-domain scenarios where new users or items lack sufficient interaction history. Inspired by foundation models' cross-domain success, we develop a foundation model for sequential recommendation that achieves genuine zero-shot generalization capabilities. Our approach fundamentally departs from existing ID-based methods by deriving item representations exclusively from textual features. This enables immediate embedding of any new item without model retraining. We introduce unified item tokenization with Finite Scalar Quantization that transforms heterogeneous textual descriptions into standardized discrete tokens. This eliminates domain barriers that plague existing systems. Additionally, the framework features hybrid bidirectional-causal attention that captures both intra-item token coherence and inter-item sequential dependencies. An efficient catalog-aware beam search decoder enables real-time token-to-item mapping. Unlike conventional approaches confined to their training domains, RecGPT naturally bridges diverse recommendation contexts through its domain-invariant tokenization mechanism. Comprehensive evaluations across six datasets and industrial scenarios demonstrate consistent performance advantages.",
        "translated": "本工作旨在解决推荐系统面临的一个根本性障碍：即在不进行大量再训练的情况下，难以实现跨领域泛化。传统的基于ID的方法在冷启动和跨域场景中完全失效，因为新用户或新物品缺乏足够的交互历史。受基础模型跨域成功的启发，我们开发了一个用于序列推荐的基础模型，它实现了真正的零样本泛化能力。\n\n我们的方法与现有基于ID的方法从根本上不同，因为它仅从文本特征中推导物品表征。这使得任何新物品都无需模型再训练即可立即嵌入。我们引入了采用有限标量量化（FSQ）的统一物品分词方法，将异构文本描述转换为标准化离散词元。这消除了困扰现有系统的领域障碍。此外，该框架还具有混合双向-因果注意力机制，能够同时捕捉物品内词元连贯性和物品间序列依赖性。一个高效的目录感知束搜索解码器能够实现实时词元到物品的映射。\n\n与局限于其训练领域的传统方法不同，RecGPT通过其领域不变的分词机制自然地弥合了多样化的推荐上下文。在六个数据集和工业场景中的全面评估表明了其持续的性能优势。"
    },
    {
        "title": "Optimizing Recall or Relevance? A Multi-Task Multi-Head Approach for\n  Item-to-Item Retrieval in Recommendation",
        "url": "http://arxiv.org/abs/2506.06239v1",
        "pub_date": "2025-06-06",
        "summary": "The task of item-to-item (I2I) retrieval is to identify a set of relevant and highly engaging items based on a given trigger item. It is a crucial component in modern recommendation systems, where users' previously engaged items serve as trigger items to retrieve relevant content for future engagement. However, existing I2I retrieval models in industry are primarily built on co-engagement data and optimized using the recall measure, which overly emphasizes co-engagement patterns while failing to capture semantic relevance. This often leads to overfitting short-term co-engagement trends at the expense of long-term benefits such as discovering novel interests and promoting content diversity. To address this challenge, we propose MTMH, a Multi-Task and Multi-Head I2I retrieval model that achieves both high recall and semantic relevance. Our model consists of two key components: 1) a multi-task learning loss for formally optimizing the trade-off between recall and semantic relevance, and 2) a multi-head I2I retrieval architecture for retrieving both highly co-engaged and semantically relevant items. We evaluate MTMH using proprietary data from a commercial platform serving billions of users and demonstrate that it can improve recall by up to 14.4% and semantic relevance by up to 56.6% compared with prior state-of-the-art models. We also conduct live experiments to verify that MTMH can enhance both short-term consumption metrics and long-term user-experience-related metrics. Our work provides a principled approach for jointly optimizing I2I recall and semantic relevance, which has significant implications for improving the overall performance of recommendation systems.",
        "translated": "物品-物品（I2I）召回的任务是基于给定的触发物品，识别出一组相关且高度吸引人的物品。它是现代推荐系统中的一个关键组成部分，其中用户的历史互动物品可作为触发物品，用于召回相关内容以供未来互动。然而，现有行业中的I2I召回模型主要基于协同互动数据构建，并使用召回率指标进行优化，这过度强调了协同互动模式，却未能捕捉到语义相关性。这通常会导致模型过拟合短期协同互动趋势，从而牺牲了发现新兴趣和促进内容多样性等长期效益。为应对这一挑战，我们提出了MTMH，一个多任务多头（Multi-Task and Multi-Head）I2I召回模型，它能够同时实现高召回率和语义相关性。我们的模型包含两个关键组成部分：1) 一个多任务学习损失函数，用于正式优化召回率和语义相关性之间的权衡；2) 一个多头I2I召回架构，用于召回高度协同互动和语义相关的物品。我们使用来自一个服务数十亿用户的商业平台的专有数据对MTMH进行了评估，结果表明，相比于现有最先进的模型，它能将召回率提升高达14.4%，将语义相关性提升高达56.6%。我们还进行了在线实验，验证MTMH能够提升短期消费指标和长期用户体验相关指标。我们的工作为联合优化I2I召回率和语义相关性提供了一种原则性的方法，这对提升推荐系统的整体性能具有重要意义。"
    },
    {
        "title": "Recommender systems, stigmergy, and the tyranny of popularity",
        "url": "http://arxiv.org/abs/2506.06162v1",
        "pub_date": "2025-06-06",
        "summary": "Scientific recommender systems, such as Google Scholar and Web of Science, are essential tools for discovery. Search algorithms that power work through stigmergy, a collective intelligence mechanism that surfaces useful paths through repeated engagement. While generally effective, this ``rich-get-richer'' dynamic results in a small number of high-profile papers that dominate visibility. This essay argues argue that these algorithm over-reliance on popularity fosters intellectual homogeneity and exacerbates structural inequities, stifling innovative and diverse perspectives critical for scientific progress. We propose an overhaul of search platforms to incorporate user-specific calibration, allowing researchers to manually adjust the weights of factors like popularity, recency, and relevance. We also advise platform developers on how word embeddings and LLMs could be implemented in ways that increase user autonomy. While our suggestions are particularly pertinent to aligning recommender systems with scientific values, these ideas are broadly applicable to information access systems in general. Designing platforms that increase user autonomy is an important step toward more robust and dynamic information",
        "translated": "诸如 Google 学术（Google Scholar）和 Web of Science 等科学推荐系统，是重要的科研发现工具。驱动这些系统运行的搜索算法通过“触发式协作”（stigmergy）机制发挥作用，这是一种通过重复互动来揭示有用路径的集体智能机制。尽管这种机制通常有效，但其“富者愈富”（rich-get-richer）的动态会导致少数备受关注的论文占据主导可见度。\n\n本文认为，这些算法对流行度的过度依赖助长了学术思想的同质化，加剧了结构性不平等，从而扼杀了对科学进步至关重要的创新和多样化视角。我们建议对搜索平台进行彻底改革，以纳入用户特定的校准功能，允许研究人员手动调整流行度、时新度、相关性等因素的权重。我们还就平台开发者如何实施词嵌入（word embeddings）和大型语言模型（LLMs）以提高用户自主性提出建议。\n\n尽管我们的建议在使推荐系统与科学价值观保持一致方面尤其适用，但这些理念也普遍适用于一般的信息获取系统。设计能够提高用户自主性的平台是迈向更强大、更动态信息获取的重要一步。"
    },
    {
        "title": "CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval",
        "url": "http://arxiv.org/abs/2506.06144v1",
        "pub_date": "2025-06-06",
        "summary": "Online video web content is richly multimodal: a single video blends vision, speech, ambient audio, and on-screen text. Retrieval systems typically treat these modalities as independent retrieval sources, which can lead to noisy and subpar retrieval. We explore multimodal video content retrieval, where relevance can be scored from one particular modality or jointly across multiple modalities simultaneously. Consequently, an effective retriever must dynamically choose which modality (or set of modalities) best addresses the query. We introduce CLaMR, a multimodal, late-interaction retriever that jointly indexes 4 modalities: video frames, transcribed speech, on-screen text, and metadata. CLaMR jointly encodes all modalities with a unified multimodal backbone for improved contextualization and is trained to enhance dynamic modality selection via two key innovations. First, given the lack of training data for multimodal retrieval, we introduce MultiVENT 2.0++, a large-scale synthetic training dataset built on MultiVENT 2.0 (event-centric videos in various languages paired with queries) with modality-targeted queries. Next, we propose a modality-aware loss that jointly trains according to a standard contrastive objective alongside an objective for learning correct modality usage. On the test sets of MultiVENT 2.0++ and MSRVTT, conventional aggregation strategies, such as averaging similarities for baseline retrievers, degrade performance by introducing noise from irrelevant modalities. In contrast, CLaMR consistently outperforms existing retrievers: on MultiVENT 2.0++, CLaMR improves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4 over the best multi-modality retriever. We illustrate CLaMR's downstream utility on long-video QA, retrieving relevant frames and obtaining a 3.50% boost over LanguageBind on Video-MME and 1.42% over dense sampling on LongVideoBench.",
        "translated": "在线视频网络内容模态丰富：单个视频融合了视觉、语音、环境音频和屏幕文本。传统的检索系统通常将这些模态视为独立的检索源，这可能导致检索结果嘈杂且不理想。我们探索多模态视频内容检索，其中相关性可以从某一特定模态或同时从多个模态联合评估。因此，一个有效的检索器必须动态地选择哪个模态（或模态组合）最能满足查询需求。我们引入了CLaMR，一个多模态的晚期交互检索器，它联合索引了四种模态：视频帧、转录语音、屏幕文本和元数据。CLaMR通过统一的多模态骨干网络对所有模态进行联合编码，以提升上下文理解能力，并通过两项关键创新来增强动态模态选择的训练。首先，鉴于多模态检索训练数据的缺乏，我们引入了MultiVENT 2.0++，这是一个基于MultiVENT 2.0（包含多种语言的事件中心视频与查询配对）构建的大规模合成训练数据集，并加入了模态导向的查询。其次，我们提出了一种模态感知损失，它根据标准对比目标与学习正确模态使用的目标进行联合训练。在MultiVENT 2.0++和MSRVTT的测试集上，传统的聚合策略（例如对基线检索器进行相似度平均）会因为引入不相关模态的噪声而导致性能下降。相比之下，CLaMR持续超越现有检索器：在MultiVENT 2.0++上，CLaMR相较于最佳单模态检索器，nDCG@10提升了25.6；相较于最佳多模态检索器，提升了35.4。我们展示了CLaMR在长视频问答中的下游应用价值，通过检索相关帧，在Video-MME上比LanguageBind提升了3.50%，在LongVideoBench上比密集采样提升了1.42%。"
    },
    {
        "title": "Phonetically-Augmented Discriminative Rescoring for Voice Search Error\n  Correction",
        "url": "http://arxiv.org/abs/2506.06117v1",
        "pub_date": "2025-06-06",
        "summary": "End-to-end (E2E) Automatic Speech Recognition (ASR) models are trained using paired audio-text samples that are expensive to obtain, since high-quality ground-truth data requires human annotators. Voice search applications, such as digital media players, leverage ASR to allow users to search by voice as opposed to an on-screen keyboard. However, recent or infrequent movie titles may not be sufficiently represented in the E2E ASR system's training data, and hence, may suffer poor recognition.   In this paper, we propose a phonetic correction system that consists of (a) a phonetic search based on the ASR model's output that generates phonetic alternatives that may not be considered by the E2E system, and (b) a rescorer component that combines the ASR model recognition and the phonetic alternatives, and select a final system output.   We find that our approach improves word error rate between 4.4 and 7.6% relative on benchmarks of popular movie titles over a series of competitive baselines.",
        "translated": "端到端（E2E）自动语音识别（ASR）模型使用配对的音频-文本样本进行训练，但这些样本获取成本高昂，因为高质量的真实标注数据需要人工标注者。数字媒体播放器等语音搜索应用利用ASR技术，使用户能够通过语音进行搜索，而非使用屏幕键盘。然而，最近上映或不常见的电影名称可能在E2E ASR系统的训练数据中没有得到充分体现，因此可能导致较差的识别效果。\n\n本文中，我们提出了一种语音校正系统，该系统包含：(a) 基于ASR模型输出的语音搜索，用于生成E2E系统可能未考虑的语音备选项；以及 (b) 一个重打分组件，用于结合ASR模型识别结果和语音备选项，并选择最终的系统输出。\n\n我们发现，与一系列具有竞争力的基线相比，我们的方法在流行电影名称的基准测试中，相对降低了词错误率（WER）4.4%至7.6%。"
    },
    {
        "title": "On the Merits of LLM-Based Corpus Enrichment",
        "url": "http://arxiv.org/abs/2506.06015v1",
        "pub_date": "2025-06-06",
        "summary": "Generative AI (genAI) technologies -- specifically, large language models (LLMs) -- and search have evolving relations. We argue for a novel perspective: using genAI to enrich a document corpus so as to improve query-based retrieval effectiveness. The enrichment is based on modifying existing documents or generating new ones. As an empirical proof of concept, we use LLMs to generate documents relevant to a topic which are more retrievable than existing ones. In addition, we demonstrate the potential merits of using corpus enrichment for retrieval augmented generation (RAG) and answer attribution in question answering.",
        "translated": "生成式人工智能（genAI）技术——特别是大型语言模型（LLMs）——与搜索的关系正在不断演变。我们提出一种新颖的视角：利用生成式人工智能（genAI）技术来丰富文档语料库，以提高基于查询的检索有效性。这种丰富方法基于修改现有文档或生成新文档。作为一项实证性概念验证，我们使用大型语言模型（LLMs）生成了针对特定主题的文档，这些文档比现有文档更易于检索。此外，我们展示了将语料库丰富应用于检索增强生成（RAG）以及问答系统中的答案归因的潜在优势。"
    },
    {
        "title": "Respecting Temporal-Causal Consistency: Entity-Event Knowledge Graphs\n  for Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2506.05939v1",
        "pub_date": "2025-06-06",
        "summary": "Retrieval-augmented generation (RAG) based on large language models often falters on narrative documents with inherent temporal structures. Standard unstructured RAG methods rely solely on embedding-similarity matching and lack any general mechanism to encode or exploit chronological information, while knowledge graph RAG (KG-RAG) frameworks collapse every mention of an entity into a single node, erasing the evolving context that drives many queries. To formalize this challenge and draw the community's attention, we construct ChronoQA, a robust and discriminative QA benchmark that measures temporal, causal, and character consistency understanding in narrative documents (e.g., novels) under the RAG setting. We then introduce Entity-Event RAG (E^2RAG), a dual-graph framework that keeps separate entity and event subgraphs linked by a bipartite mapping, thereby preserving the temporal and causal facets needed for fine-grained reasoning. Across ChronoQA, our approach outperforms state-of-the-art unstructured and KG-based RAG baselines, with notable gains on causal and character consistency queries. E^2RAG therefore offers a practical path to more context-aware retrieval for tasks that require precise answers grounded in chronological information.",
        "translated": "基于大语言模型的检索增强生成（RAG）在处理具有固有时间结构的叙事文档时常常表现不佳。标准的非结构化RAG方法仅依赖于嵌入相似度匹配，并缺乏编码或利用时间信息的通用机制；而知识图谱RAG（KG-RAG）框架则将实体的每一次提及都合并为一个单一节点，抹去了驱动许多查询的演变上下文。\n\n为了形式化这一挑战并引起社区关注，我们构建了ChronoQA，这是一个鲁棒且具有区分度的问答基准，用于衡量RAG设置下叙事文档（例如小说）中的时间、因果和人物一致性理解能力。随后，我们提出了实体-事件RAG（E^2RAG），这是一个双图框架，它通过二分图映射将独立的实体子图和事件子图连接起来，从而保留了细粒度推理所需的时间和因果方面。\n\n在ChronoQA基准测试中，我们的方法优于最先进的非结构化和基于KG的RAG基线，在因果和人物一致性查询上取得了显著提升。因此，E^2RAG为需要基于时间信息提供精确答案的任务，提供了一条更具上下文感知能力的实用检索路径。"
    },
    {
        "title": "Research on Personalized Financial Product Recommendation by Integrating\n  Large Language Models and Graph Neural Networks",
        "url": "http://arxiv.org/abs/2506.05873v1",
        "pub_date": "2025-06-06",
        "summary": "With the rapid growth of fintech, personalized financial product recommendations have become increasingly important. Traditional methods like collaborative filtering or content-based models often fail to capture users' latent preferences and complex relationships. We propose a hybrid framework integrating large language models (LLMs) and graph neural networks (GNNs). A pre-trained LLM encodes text data (e.g., user reviews) into rich feature vectors, while a heterogeneous user-product graph models interactions and social ties. Through a tailored message-passing mechanism, text and graph information are fused within the GNN to jointly optimize embeddings. Experiments on public and real-world financial datasets show our model outperforms standalone LLM or GNN in accuracy, recall, and NDCG, with strong interpretability. This work offers new insights for personalized financial recommendations and cross-modal fusion in broader recommendation tasks.",
        "translated": "随着金融科技的快速发展，个性化金融产品推荐变得愈发重要。传统的协同过滤或基于内容的模型通常难以捕获用户的潜在偏好和复杂的相互关系。为此，我们提出一种融合大语言模型（LLM）和图神经网络（GNN）的混合框架。预训练的LLM负责将文本数据（如用户评论）编码为丰富的特征向量，而异构用户-产品图则用于建模用户与产品之间的交互以及用户的社交关系。通过定制化的消息传递机制，文本和图信息在GNN内部得到融合，从而联合优化嵌入表示。在公开和真实世界的金融数据集上进行的实验表明，我们的模型在准确率、召回率和NDCG方面均优于单独的LLM或GNN，并展现出强大的可解释性。这项工作为个性化金融推荐以及更广泛推荐任务中的跨模态融合提供了新的见解。"
    },
    {
        "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
        "url": "http://arxiv.org/abs/2506.06266v1",
        "pub_date": "2025-06-06",
        "summary": "Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.",
        "translated": "大型语言模型常用于回答基于大型文本语料库（例如代码库、法律文档或聊天记录）的查询，其方法是将整个语料库放入上下文窗口中，并利用上下文学习（ICL）的能力。尽管当前模型支持10万到100万个词元的上下文，但这种设置的服务成本很高，因为KV缓存的内存消耗随输入长度线性增长。\n\n我们探索了一种替代方案：为每个语料库离线训练一个更小的KV缓存。在推理时，我们加载这个经过训练的KV缓存（我们称之为Cartridge），并解码生成响应。关键在于，Cartridge的训练成本可以分摊到所有引用相同语料库的查询中。\n\n然而，我们发现，使用朴素的下一个词元预测方法在语料库上训练Cartridge，其效果无法与ICL媲美。取而代之的是，我们提出了一种名为“自学习”（self-study）的训练方案，其中我们生成关于语料库的合成对话，并利用上下文蒸馏目标来训练Cartridge。我们发现，通过自学习训练的Cartridge能够复现ICL的功能，同时服务成本显著降低。\n\n在具有挑战性的长上下文基准测试中，通过自学习训练的Cartridge在性能上与ICL相当，同时内存使用量减少了38.6倍，吞吐量提高了26.4倍。自学习还扩展了模型的有效上下文长度（例如在MTOB上从12.8万个词元扩展到48.4万个词元），并且令人惊讶的是，它使得Cartridge无需重新训练即可在推理时进行组合。"
    },
    {
        "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at\n  Test Time",
        "url": "http://arxiv.org/abs/2506.06254v1",
        "pub_date": "2025-06-06",
        "summary": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.",
        "translated": "大语言模型（LLM）驱动的智能体近期作为先进范式涌现，在广泛的领域和任务中展现出令人印象深刻的能力。尽管其潜力巨大，当前的LLM智能体却普遍采用“一刀切”的方法，缺乏根据用户多样化需求和偏好进行响应的灵活性。这一局限性促使我们开发了PersonaAgent，这是首个旨在解决多样化个性化任务的LLM个性化智能体框架。具体而言，PersonaAgent整合了两个互补的组件：一个包含情景记忆和语义记忆机制的个性化记忆模块；以及一个使智能体能够执行为用户量身定制的工具行动的个性化行动模块。其核心在于，角色（定义为每个用户的独特系统提示）充当着中介：它利用个性化记忆中的洞察来控制智能体的行动，而这些行动的结果反过来又会优化记忆。基于该框架，我们提出了一种测试时用户偏好对齐策略，该策略通过模拟最近的n次交互来优化角色提示，并利用模拟响应与真实响应之间的文本损失反馈，确保实时用户偏好对齐。实验评估表明，PersonaAgent不仅有效实现了行动空间的个性化，而且在测试时的实际应用中具有良好的可扩展性，显著优于其他基线方法。这些结果凸显了我们方法在提供量身定制、动态用户体验方面的可行性和潜力。"
    },
    {
        "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval",
        "url": "http://arxiv.org/abs/2506.06220v1",
        "pub_date": "2025-06-06",
        "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image retrieval benchmarks. However, bridging this success to real-world applications remains a challenge. In practice, human search behavior is rarely a one-shot action. Instead, it is often a multi-round process guided by clues in mind, that is, a mental image ranging from vague recollections to vivid mental representations of the target image. Motivated by this gap, we study the task of Mental Image Retrieval (MIR), which targets the realistic yet underexplored setting where users refine their search for a mentally envisioned image through multi-round interactions with an image search engine. Central to successful interactive retrieval is the capability of machines to provide users with clear, actionable feedback; however, existing methods rely on indirect or abstract verbal feedback, which can be ambiguous, misleading, or ineffective for users to refine the query. To overcome this, we propose GenIR, a generative multi-round retrieval paradigm leveraging diffusion-based image generation to explicitly reify the AI system's understanding at each round. These synthetic visual representations provide clear, interpretable feedback, enabling users to refine their queries intuitively and effectively. We further introduce a fully automated pipeline to generate a high-quality multi-round MIR dataset. Experimental results demonstrate that GenIR significantly outperforms existing interactive methods in the MIR scenario. This work establishes a new task with a dataset and an effective generative retrieval method, providing a foundation for future research in this direction.",
        "translated": "视觉-语言模型（VLM）在文本到图像检索基准测试中表现出强大的性能。然而，将这一成功推广到现实世界应用仍然是一个挑战。在实践中，人类的搜索行为很少是单次操作。相反，它通常是一个多轮过程，由脑海中的线索所引导，即一种心理图像，从模糊的回忆到目标图像生动的心理表征。受此差距启发，我们研究了心理图像检索（MIR）任务，该任务旨在解决用户通过与图像搜索引擎进行多轮交互来细化其对脑海中构想的图像的搜索这一现实但未充分探索的场景。\n\n成功的交互式检索关键在于机器能够向用户提供清晰、可操作的反馈；然而，现有方法依赖于间接或抽象的语言反馈，这可能对用户细化查询而言是模糊、误导性或低效的。为了克服这一问题，我们提出了GenIR，这是一种生成式多轮检索范式，它利用基于扩散的图像生成技术来明确地具象化AI系统在每一轮中的理解。这些合成视觉表示提供了清晰、可解释的反馈，使用户能够直观且有效地细化其查询。我们进一步引入了一个全自动流程来生成高质量的多轮心理图像检索数据集。实验结果表明，GenIR在心理图像检索场景中显著优于现有的交互式方法。这项工作建立了一项新任务，并提供了数据集和一种有效的生成式检索方法，为未来该方向的研究奠定了基础。"
    },
    {
        "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at\n  Test Time",
        "url": "http://arxiv.org/abs/2506.06254v1",
        "pub_date": "2025-06-06",
        "summary": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.",
        "translated": "大型语言模型（LLM）驱动的智能体近期作为先进范式涌现，在广泛的领域和任务中展现出卓越的能力。尽管其潜力巨大，当前的LLM智能体通常采用“一刀切”的方式，缺乏灵活性以响应用户多样化的需求和偏好。这一局限性促使我们开发了PersonaAgent，这是首个旨在处理多样化个性化任务的个性化LLM智能体框架。具体而言，PersonaAgent集成了两个互补的组件：一个包含情景记忆和语义记忆机制的个性化记忆模块；以及一个使智能体能够执行为用户量身定制的工具操作的个性化行动模块。其核心在于，人格（定义为每个用户的独特系统提示）充当中间层：它利用个性化记忆中的洞察来控制智能体行动，而这些行动的结果反过来又会反哺记忆。基于该框架，我们提出了一种测试时用户偏好对齐策略，该策略通过模拟最近的n次交互来优化人格提示，并利用模拟响应与真实响应之间的文本损失反馈，确保实时用户偏好对齐。实验评估表明，PersonaAgent显著优于其他基线方法，不仅有效地个性化了行动空间，而且在测试时真实世界应用中展现出良好的扩展能力。这些结果凸显了我们方法在提供量身定制的、动态的用户体验方面的可行性和潜力。"
    },
    {
        "title": "Reflect-then-Plan: Offline Model-Based Planning through a Doubly\n  Bayesian Lens",
        "url": "http://arxiv.org/abs/2506.06261v1",
        "pub_date": "2025-06-06",
        "summary": "Offline reinforcement learning (RL) is crucial when online exploration is costly or unsafe but often struggles with high epistemic uncertainty due to limited data. Existing methods rely on fixed conservative policies, restricting adaptivity and generalization. To address this, we propose Reflect-then-Plan (RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach. RefPlan unifies uncertainty modeling and MB planning by recasting planning as Bayesian posterior estimation. At deployment, it updates a belief over environment dynamics using real-time observations, incorporating uncertainty into MB planning via marginalization. Empirical results on standard benchmarks show that RefPlan significantly improves the performance of conservative offline RL policies. In particular, RefPlan maintains robust performance under high epistemic uncertainty and limited data, while demonstrating resilience to changing environment dynamics, improving the flexibility, generalizability, and robustness of offline-learned policies.",
        "translated": "离线强化学习 (RL) 在在线探索成本高昂或不安全时至关重要，但由于数据有限，它往往难以应对高认知不确定性。现有方法依赖固定的保守策略，这限制了其适应性和泛化能力。为解决此问题，我们提出了一种新颖的双重贝叶斯离线基于模型 (MB) 的规划方法：Reflect-then-Plan (RefPlan)。RefPlan 通过将规划重构为贝叶斯后验估计，统一了不确定性建模和MB规划。在部署时，它利用实时观测更新对环境动态的信念，并通过边缘化将不确定性融入MB规划。在标准基准上的实验结果表明，RefPlan 显著提升了保守离线强化学习策略的性能。具体而言，RefPlan 在高认知不确定性和数据有限的情况下仍能保持鲁棒性能，同时展现出对环境动态变化的韧性，从而提升了离线学习策略的灵活性、泛化能力和鲁棒性。"
    },
    {
        "title": "Bridging External and Parametric Knowledge: Mitigating Hallucination of\n  LLMs with Shared-Private Semantic Synergy in Dual-Stream Knowledge",
        "url": "http://arxiv.org/abs/2506.06240v1",
        "pub_date": "2025-06-06",
        "summary": "Retrieval-augmented generation (RAG) is a cost-effective approach to mitigate the hallucination of Large Language Models (LLMs) by incorporating the retrieved external knowledge into the generation process. However, external knowledge may conflict with the parametric knowledge of LLMs. Furthermore, current LLMs lack inherent mechanisms for resolving such knowledge conflicts, making traditional RAG methods suffer from degraded performance and stability. Thus, we propose a Dual-Stream Knowledge-Augmented Framework for Shared-Private Semantic Synergy (DSSP-RAG). Central to the framework is a novel approach that refines self-attention into a mixed-attention, distinguishing shared and private semantics for a controlled internal-external knowledge integration. To effectively facilitate DSSP in RAG, we further introduce an unsupervised hallucination detection method based on cognitive uncertainty, ensuring the necessity of introducing knowledge, and an Energy Quotient (EQ) based on attention difference matrices to reduce noise in the retrieved external knowledge. Extensive experiments on benchmark datasets show that DSSP-RAG can effectively resolve conflicts and enhance the complementarity of dual-stream knowledge, leading to superior performance over strong baselines.",
        "translated": "检索增强生成（RAG）是一种经济高效的方法，通过将检索到的外部知识融入生成过程，以缓解大型语言模型（LLM）的幻觉问题。然而，外部知识可能与LLM的参数知识发生冲突。此外，当前的LLM缺乏解决此类知识冲突的内在机制，导致传统RAG方法在性能和稳定性方面均有所下降。\n\n因此，我们提出了一种用于共享-私有语义协同的双流知识增强框架（DSSP-RAG）。该框架的核心是一种新颖的方法，它将自注意力机制细化为混合注意力机制，以区分共享和私有语义，从而实现对内外部知识的受控整合。为了在RAG中有效促进DSSP，我们进一步引入了一种基于认知不确定性的无监督幻觉检测方法，以确保引入知识的必要性；以及一种基于注意力差异矩阵的能量商（EQ），以减少检索到的外部知识中的噪声。在基准数据集上进行的大量实验表明，DSSP-RAG能够有效解决冲突并增强双流知识的互补性，从而实现超越强大基线的优异性能。"
    },
    {
        "title": "Building Models of Neurological Language",
        "url": "http://arxiv.org/abs/2506.06208v1",
        "pub_date": "2025-06-06",
        "summary": "This report documents the development and evaluation of domain-specific language models for neurology. Initially focused on building a bespoke model, the project adapted to rapid advances in open-source and commercial medical LLMs, shifting toward leveraging retrieval-augmented generation (RAG) and representational models for secure, local deployment. Key contributions include the creation of neurology-specific datasets (case reports, QA sets, textbook-derived data), tools for multi-word expression extraction, and graph-based analyses of medical terminology. The project also produced scripts and Docker containers for local hosting. Performance metrics and graph community results are reported, with future possible work open for multimodal models using open-source architectures like phi-4.",
        "translated": "本报告记录了神经病学领域专用语言模型的开发和评估。项目最初专注于构建定制模型，但随着开源和商业医疗大型语言模型（LLMs）的快速发展，项目策略也随之调整，转而侧重于利用检索增强生成（RAG）和表征模型，以实现安全、本地化部署。主要贡献包括：创建了神经病学专用数据集（涵盖病例报告、问答集和教材衍生数据），开发了多词表达提取工具，并进行了医学术语的基于图的分析。该项目还提供了用于本地部署的脚本和 Docker 容器。报告中提供了性能指标和图社区结果。未来的工作方向可能包括使用 phi-4 等开源架构的多模态模型。"
    },
    {
        "title": "PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels",
        "url": "http://arxiv.org/abs/2506.07606v1",
        "pub_date": "2025-06-09",
        "summary": "Stance detection identifies the viewpoint expressed in text toward a specific target, such as a political figure. While previous datasets have focused primarily on tweet-level stances from established platforms, user-level stance resources, especially on emerging platforms like Bluesky remain scarce. User-level stance detection provides a more holistic view by considering a user's complete posting history rather than isolated posts. We present the first stance detection dataset for the 2024 U.S. presidential election, collected from Bluesky and centered on Kamala Harris and Donald Trump. The dataset comprises 16,044 user-target stance pairs enriched with engagement metadata, interaction graphs, and user posting histories. PolitiSky24 was created using a carefully evaluated pipeline combining advanced information retrieval and large language models, which generates stance labels with supporting rationales and text spans for transparency. The labeling approach achieves 81\\% accuracy with scalable LLMs. This resource addresses gaps in political stance analysis through its timeliness, open-data nature, and user-level perspective. The dataset is available at https://doi.org/10.5281/zenodo.15616911",
        "translated": "立场检测旨在识别文本中针对特定目标（如政治人物）所表达的观点。尽管以往的数据集主要关注来自现有平台的推文级立场，但用户级立场资源，特别是在Bluesky等新兴平台上的此类资源，仍然稀缺。用户级立场检测通过考虑用户完整的发帖历史而非孤立的帖子，提供了一种更全面的视角。我们提出了首个针对2024年美国总统大选的立场检测数据集，该数据集从Bluesky平台收集，并以卡马拉·哈里斯和唐纳德·特朗普为中心。该数据集包含16,044个用户-目标立场对，并辅以互动元数据、互动图谱和用户发帖历史。PolitiSky24是通过结合先进信息检索（IR）技术和大型语言模型（LLM）的精心评估流程创建的，它能生成立场标签，并提供支持性理由和文本片段，以增强透明度。该标注方法在使用可扩展大型语言模型时，能达到81%的准确率。该资源通过其及时性、开放数据特性和用户级视角，弥补了政治立场分析中的不足。该数据集可在 https://doi.org/10.5281/zenodo.15616911 获取。"
    },
    {
        "title": "MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with\n  Expert Specialization",
        "url": "http://arxiv.org/abs/2506.07563v2",
        "pub_date": "2025-06-09",
        "summary": "Personalized recommendation systems must adapt to user interactions across different domains. Traditional approaches like MLoRA apply a single adaptation per domain but lack flexibility in handling diverse user behaviors. To address this, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is first trained independently to specialize in its domain before a gating network is trained to weight their contributions dynamically. We evaluate MoE-MLoRA across eight CTR models on Movielens and Taobao, showing that it improves performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20) but offers limited benefits in structured datasets with low domain diversity and sparsity. Further analysis of the number of experts per domain reveals that larger ensembles do not always improve performance, indicating the need for model-aware tuning. Our findings highlight the potential of expert-based architectures for multi-domain recommendation systems, demonstrating that task-aware specialization and adaptive gating can enhance predictive accuracy in complex environments. The implementation and code are available in our GitHub repository.",
        "translated": "个性化推荐系统必须适应跨域的用户交互。传统的MLoRA等方法在每个域采用单一的适配策略，但在处理多样化的用户行为时缺乏灵活性。为解决此问题，我们提出了MoE-MLoRA，这是一个混合专家（MoE）框架。在该框架中，每个专家首先被独立训练，以专注于其特定领域，然后训练一个门控网络来动态加权它们的贡献。\n\n我们在Movielens和淘宝数据集上的八种点击率（CTR）模型上评估了MoE-MLoRA，结果表明，它在大规模、动态数据集（在Taobao-20上加权AUC提升了1.45）中显著提升了性能，但在领域多样性低和稀疏的结构化数据集中收益有限。对每个域的专家数量的进一步分析表明，更大的集成模型并非总能提升性能，这预示着需要进行模型感知（model-aware）调优。\n\n我们的研究结果凸显了基于专家的架构在多域推荐系统中的潜力，证明了任务感知专业化和自适应门控能够在复杂环境中提高预测准确性。本研究的实现代码已在我们的GitHub仓库中开源。"
    },
    {
        "title": "Addressing Correlated Latent Exogenous Variables in Debiased Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2506.07517v1",
        "pub_date": "2025-06-09",
        "summary": "Recommendation systems (RS) aim to provide personalized content, but they face a challenge in unbiased learning due to selection bias, where users only interact with items they prefer. This bias leads to a distorted representation of user preferences, which hinders the accuracy and fairness of recommendations. To address the issue, various methods such as error imputation based, inverse propensity scoring, and doubly robust techniques have been developed. Despite the progress, from the structural causal model perspective, previous debiasing methods in RS assume the independence of the exogenous variables. In this paper, we release this assumption and propose a learning algorithm based on likelihood maximization to learn a prediction model. We first discuss the correlation and difference between unmeasured confounding and our scenario, then we propose a unified method that effectively handles latent exogenous variables. Specifically, our method models the data generation process with latent exogenous variables under mild normality assumptions. We then develop a Monte Carlo algorithm to numerically estimate the likelihood function. Extensive experiments on synthetic datasets and three real-world datasets demonstrate the effectiveness of our proposed method. The code is at https://github.com/WallaceSUI/kdd25-background-variable.",
        "translated": "推荐系统（RS）旨在提供个性化内容，但由于选择偏差（即用户只与他们偏好的项目进行交互），它们在无偏学习方面面临挑战。这种偏差导致用户偏好表示失真，进而阻碍了推荐的准确性和公平性。为了解决这个问题，研究人员已经开发了各种方法，例如基于误差插补、逆倾向加权和双重鲁棒技术。\n\n尽管取得了进展，但从结构因果模型的角度来看，以往推荐系统中的去偏方法都假设外生变量是独立的。在本文中，我们放宽了这一假设，并提出了一种基于似然最大化的学习算法来学习一个预测模型。我们首先讨论了未测量混杂变量与我们场景之间的关联与区别，然后提出了一种统一的方法，能够有效处理潜在外生变量。具体来说，我们的方法在温和的正态性假设下，对带有潜在外生变量的数据生成过程进行了建模。随后，我们开发了一种蒙特卡洛算法来数值估计似然函数。在合成数据集和三个真实世界数据集上进行的大量实验证明了我们所提出方法的有效性。\n\n代码位于：https://github.com/WallaceSUI/kdd25-background-variable。"
    },
    {
        "title": "Leveraging Historical and Current Interests for Continual Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.07466v1",
        "pub_date": "2025-06-09",
        "summary": "Sequential recommendation models based on the Transformer architecture show superior performance in harnessing long-range dependencies within user behavior via self-attention. However, naively updating them on continuously arriving non-stationary data streams incurs prohibitive computation costs or leads to catastrophic forgetting. To address this, we propose Continual Sequential Transformer for Recommendation (CSTRec) that effectively leverages well-preserved historical user interests while capturing current interests. At its core is Continual Sequential Attention (CSA), a linear attention mechanism that retains past knowledge without direct access to old data. CSA integrates two key components: (1) Cauchy-Schwarz Normalization that stabilizes training under uneven interaction frequencies, and (2) Collaborative Interest Enrichment that mitigates forgetting through shared, learnable interest pools. We further introduce a technique that facilitates learning for cold-start users by transferring historical knowledge from behaviorally similar existing users. Extensive experiments on three real-world datasets indicate that CSTRec outperforms state-of-the-art baselines in both knowledge retention and acquisition.",
        "translated": "基于Transformer架构的序列推荐模型通过自注意力机制，在捕获用户行为序列中的长程依赖方面表现出卓越的性能。然而，若对其在持续到达的非平稳数据流上进行朴素更新，则会产生高昂的计算成本或导致灾难性遗忘。为解决此问题，我们提出了持续序列Transformer推荐模型（CSTRec），该模型能够有效利用保存完好的历史用户兴趣，同时捕获当前兴趣。\n\n其核心是持续序列注意力机制（CSA），这是一种线性注意力机制，能够在无需直接访问旧数据的情况下保留过往知识。CSA集成了两个关键组件：(1) 柯西-施瓦茨归一化（Cauchy-Schwarz Normalization），旨在稳定不均匀交互频率下的训练；以及(2) 协作兴趣丰富（Collaborative Interest Enrichment），通过共享的可学习兴趣池缓解遗忘。我们进一步引入了一种技术，通过从行为相似的现有用户中迁移历史知识，从而促进冷启动用户的学习。在三个真实世界数据集上进行的大量实验表明，CSTRec在知识保留和获取两方面均超越了最先进的基线模型。"
    },
    {
        "title": "LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework\n  for LLM-Based Ranking",
        "url": "http://arxiv.org/abs/2506.07449v1",
        "pub_date": "2025-06-09",
        "summary": "Recent advances in Large Language Models (LLMs) have driven their adoption in recommender systems through Retrieval-Augmented Generation (RAG) frameworks. However, existing RAG approaches predominantly rely on flat, similarity-based retrieval that fails to leverage the rich relational structure inherent in user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass, end-to-end trainable framework that integrates personalized knowledge graph context into LLM-based recommendation ranking. Our approach extends the LlamaRec architecture by incorporating a lightweight user preference module that dynamically identifies salient relation paths within a heterogeneous knowledge graph constructed from user behavior and item metadata. These personalized subgraphs are seamlessly integrated into prompts for a fine-tuned Llama-2 model, enabling efficient and interpretable recommendations through a unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty datasets demonstrate consistent and significant improvements over LlamaRec across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates the critical value of structured reasoning in LLM-based recommendations and establishes a foundation for scalable, knowledge-aware personalization in next-generation recommender systems. Code is available at~\\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.",
        "translated": "大型语言模型（LLM）的最新进展，推动了它们通过检索增强生成（RAG）框架在推荐系统中的应用。然而，现有的RAG方法主要依赖于扁平的、基于相似度的检索，未能充分利用用户-物品交互中固有的丰富关系结构。\n\n我们引入了LlamaRec-LKG-RAG，这是一种新颖的单次、端到端可训练的框架，它将个性化知识图谱上下文集成到基于LLM的推荐排名中。我们的方法扩展了LlamaRec架构，通过引入一个轻量级的用户偏好模块，该模块能够动态识别从用户行为和物品元数据构建的异构知识图谱中的显著关系路径。这些个性化子图被无缝集成到微调后的Llama-2模型的提示中，通过统一的推理步骤实现高效且可解释的推荐。\n\n在ML-100K和Amazon Beauty数据集上进行的全面实验表明，相对于LlamaRec，该方法在关键排名指标（MRR、NDCG、Recall）上取得了持续且显著的改进。LlamaRec-LKG-RAG证明了结构化推理在基于LLM的推荐中的关键价值，并为下一代推荐系统中可扩展、知识感知的个性化奠定了基础。代码已开源于[仓库](https://github.com/VahidAz/LlamaRec-LKG-RAG)。"
    },
    {
        "title": "HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language\n  Models for Efficient Multimodal Hotel Retrieval",
        "url": "http://arxiv.org/abs/2506.07296v1",
        "pub_date": "2025-06-08",
        "summary": "We present HotelMatch-LLM, a multimodal dense retrieval model for the travel domain that enables natural language property search, addressing the limitations of traditional travel search engines which require users to start with a destination and editing search parameters. HotelMatch-LLM features three key innovations: (1) Domain-specific multi-task optimization with three novel retrieval, visual, and language modeling objectives; (2) Asymmetrical dense retrieval architecture combining a small language model (SLM) for efficient online query processing and a large language model (LLM) for embedding hotel data; and (3) Extensive image processing to handle all property image galleries. Experiments on four diverse test sets show HotelMatch-LLM significantly outperforms state-of-the-art models, including VISTA and MARVEL. Specifically, on the test set -- main query type -- we achieve 0.681 for HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our analysis highlights the impact of our multi-task optimization, the generalizability of HotelMatch-LLM across LLM architectures, and its scalability for processing large image galleries.",
        "translated": "我们提出了 HotelMatch-LLM，这是一种专为旅游领域设计的多模态稠密检索模型，它支持自然语言酒店信息搜索，解决了传统旅游搜索引擎要求用户必须先选择目的地并手动调整搜索参数的局限性。HotelMatch-LLM 具有三项关键创新：(1) 领域特定多任务优化，包含三个新颖的检索、视觉和语言建模目标；(2) 非对称稠密检索架构，结合了用于高效在线查询处理的小型语言模型（SLM）和用于嵌入酒店数据的大型语言模型（LLM）；以及 (3) 大量的图像处理能力，以处理所有酒店的图片库。在四个多样化的测试集上进行的实验表明，HotelMatch-LLM 显著优于包括 VISTA 和 MARVEL 在内的最先进模型。具体而言，在测试集（主要查询类型）上，HotelMatch-LLM 的性能指标达到了 0.681，而最有效的基线模型 MARVEL 仅为 0.603。我们的分析强调了多任务优化的影响、HotelMatch-LLM 在不同 LLM 架构下的泛化能力，以及其处理大型图片库的可扩展性。"
    },
    {
        "title": "RADAR: Recall Augmentation through Deferred Asynchronous Retrieval",
        "url": "http://arxiv.org/abs/2506.07261v1",
        "pub_date": "2025-06-08",
        "summary": "Modern large-scale recommender systems employ multi-stage ranking funnel (Retrieval, Pre-ranking, Ranking) to balance engagement and computational constraints (latency, CPU). However, the initial retrieval stage, often relying on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles to effectively surface the most engaging items from billion-scale catalogs, particularly distinguishing highly relevant and engaging candidates from merely relevant ones. We introduce Recall Augmentation through Deferred Asynchronous Retrieval (RADAR), a novel framework that leverages asynchronous, offline computation to pre-rank a significantly larger candidate set for users using the full complexity ranking model. These top-ranked items are stored and utilized as a high-quality retrieval source during online inference, bypassing online retrieval and pre-ranking stages for these candidates. We demonstrate through offline experiments that RADAR significantly boosts recall (2X Recall@200 vs DNN retrieval baseline) by effectively combining a larger retrieved candidate set with a more powerful ranking model. Online A/B tests confirm a +0.8% lift in topline engagement metrics, validating RADAR as a practical and effective method to improve recommendation quality under strict online serving constraints.",
        "translated": "现代大规模推荐系统采用多阶段排序漏斗（召回、初排、精排）以平衡用户参与度与计算资源限制（如延迟、CPU）。然而，初始召回阶段通常依赖于K近邻（KNN）等高效但不那么精确的方法，这使得它难以在十亿级别的商品目录中有效发现最具吸引力的物品，尤其难以区分高度相关且能吸引用户参与的候选物品与仅仅相关的物品。\n\n我们提出了通过延迟异步召回进行召回增强（RADAR）这一新颖框架，它利用异步离线计算，使用全复杂度排序模型为用户预先排序一个显著更大的候选集。这些排名靠前的物品被存储起来，并在在线推理期间用作高质量的召回源，从而使这些候选物品绕过在线召回和初排阶段。我们通过离线实验证明，RADAR通过有效结合更大的召回候选集和更强大的排序模型，显著提升了召回率（相较于DNN召回基线，Recall@200提升2倍）。在线A/B测试证实，核心用户参与度指标提升了0.8%，验证了RADAR是在严格在线服务约束下提升推荐质量的实用且有效方法。"
    },
    {
        "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
        "url": "http://arxiv.org/abs/2506.07976v2",
        "pub_date": "2025-06-09",
        "summary": "The current paradigm of test-time scaling relies on generating long reasoning traces (\"thinking\" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.",
        "translated": "当前测试时规模化的范式依赖于在生成响应之前生成长的推理轨迹（即“思考”更多）。在需要交互的智能体问题中，这可以通过在环境中行动之前生成思考轨迹来实现。然而，这种方法不允许智能体从环境中获取新信息，也无法随时间推移调整其行为。\n\n在这项工作中，我们提出对测试时交互进行规模化，这是测试时规模化一个尚未开发的维度，它增加了智能体的交互视野，从而能够在单次推演（rollout）中运行丰富的行为，例如探索、回溯和动态重新规划。为了证明这一规模化维度的潜力，我们研究了网络智能体领域。我们首先展示，即使是基于提示的交互规模化，在没有任何训练的情况下，也能显著提高网络基准上的任务成功率。在此基础上，我们引入了TTI（Test-Time Interaction，测试时交互），这是一种基于课程的在线强化学习（RL）方法，通过自适应调整智能体的推演长度来训练它们。\n\n使用Gemma 3 12B模型，TTI在WebVoyager和WebArena基准上生成了最先进的开源、开放数据网络智能体。我们进一步表明，TTI使智能体能够自适应地平衡探索与利用。我们的结果确立了交互规模化作为扩展每步计算量（per-step compute）的强大且互补的维度，为训练自适应智能体提供了新途径。"
    },
    {
        "title": "Discrete Scale-invariant Metric Learning for Efficient Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2506.09898v1",
        "pub_date": "2025-06-11",
        "summary": "Metric learning has attracted extensive interest for its ability to provide personalized recommendations based on the importance of observed user-item interactions. Current metric learning methods aim to push negative items away from the corresponding users and positive items by an absolute geometrical distance margin. However, items may come from imbalanced categories with different intra-class variations. Thus, the absolute distance margin may not be ideal for estimating the difference between user preferences over imbalanced items. To this end, we propose a new method, named discrete scale-invariant metric learning (DSIML), by adding binary constraints to users and items, which maps users and items into binary codes of a shared Hamming subspace to speed up the online recommendation. Specifically, we firstly propose a scale-invariant margin based on angles at the negative item points in the shared Hamming subspace. Then, we derive a scale-invariant triple hinge loss based on the margin. To capture more preference difference information, we integrate a pairwise ranking loss into the scale-invariant loss in the proposed model. Due to the difficulty of directly optimizing the mixed integer optimization problem formulated with \\textit{log-sum-exp} functions, we seek to optimize its variational quadratic upper bound and learn hash codes with an alternating optimization strategy. Experiments on benchmark datasets clearly show that our proposed method is superior to competitive metric learning and hashing-based baselines for recommender systems. The implementation code is available at https://github.com/AnonyFeb/dsml.",
        "translated": "度量学习因其能够基于观测到的用户-物品交互来提供个性化推荐而受到了广泛关注。当前的度量学习方法旨在通过一个绝对的几何距离间隔将负样本物品推离相应的用户和正样本物品。然而，物品可能来自不平衡的类别，并具有不同的类内变异性。因此，绝对距离间隔可能不适合估计用户对不平衡物品的偏好差异。为此，我们提出了一种新方法，名为离散尺度不变度量学习（DSIML），该方法通过对用户和物品添加二值约束，将它们映射到共享汉明子空间的二值编码中，以加速在线推荐。具体而言，我们首先在共享汉明子空间中，基于负样本物品点处的角度，提出了一个尺度不变的间隔。然后，我们基于该间隔推导出了一个尺度不变的三元合页损失。为了捕获更多的偏好差异信息，我们在所提出的模型中，将一个成对排序损失整合到尺度不变损失中。由于直接优化使用 \\textit{log-sum-exp} 函数表述的混合整数优化问题存在困难，我们寻求优化其变分二次上界，并采用交替优化策略来学习哈希码。在基准数据集上的实验清楚地表明，我们提出的方法在推荐系统方面优于具有竞争力的度量学习和基于哈希的基线方法。实施代码已在 https://github.com/AnonyFeb/dsml 发布。"
    },
    {
        "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data\n  Augmentation Strategies for Knowledge Graph Question Answering",
        "url": "http://arxiv.org/abs/2506.09414v1",
        "pub_date": "2025-06-11",
        "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural language processing that requires reasoning over knowledge graphs (KGs) to answer natural language questions. Recent methods utilizing large language models (LLMs) have shown remarkable semantic parsing capabilities but are limited by the scarcity of diverse annotated data and multi-hop reasoning samples. Traditional data augmentation approaches are focus mainly on single-hop questions and prone to semantic distortion, while LLM-based methods primarily address semantic distortion but usually neglect multi-hop reasoning, thus limiting data diversity. The scarcity of multi-hop samples further weakens models' generalization. To address these issues, we propose PGDA-KGQA, a prompt-guided generative framework with multiple data augmentation strategies for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by crafting meticulously engineered prompts that integrate the provided textual content, it leverages LLMs to generate large-scale (question, logical form) pairs for model training. Specifically, PGDA-KGQA enriches its training set by: (1) generating single-hop pseudo questions to improve the alignment of question semantics with KG relations; (2) applying semantic-preserving question rewriting to improve robustness against linguistic variations; (3) employing answer-guided reverse path exploration to create realistic multi-hop questions. By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA utilizes the augmented data to enhance the accuracy of logical form generation and thus improve answer retrieval performance. Experiments demonstrate that outperforms state-of-the-art methods on standard KGQA datasets, achieving improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by 1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.",
        "translated": "知识图谱问答（KGQA）是自然语言处理中的一项关键任务，它需要对知识图谱（KGs）进行推理以回答自然语言问题。近期利用大语言模型（LLMs）的方法已展现出卓越的语义解析能力，但受限于多样化标注数据和多跳推理样本的稀缺性。传统数据增强方法主要侧重于单跳问题且易产生语义失真，而基于LLM的方法主要解决语义失真但通常忽略多跳推理，从而限制了数据多样性。多跳样本的稀缺性进一步削弱了模型的泛化能力。\n\n为了解决这些问题，我们提出了PGDA-KGQA，这是一个提示引导的生成框架，其中包含多种KGQA数据增强策略。其核心是，PGDA-KGQA采用统一的提示设计范式：通过精心设计整合了所提供文本内容的提示，它利用LLMs生成大规模的（问题，逻辑形式）对以用于模型训练。具体而言，PGDA-KGQA通过以下方式丰富其训练集：(1) 生成单跳伪问题，以提高问题语义与知识图谱关系的对齐度；(2) 应用语义保持的问题重写，以提高对语言变体的鲁棒性；(3) 采用答案引导的逆向路径探索，以创建真实的多跳问题。通过采用增强-生成-检索的语义解析流水线，PGDA-KGQA利用增强后的数据提高逻辑形式生成的准确性，从而提升答案检索性能。实验表明，PGDA-KGQA在标准KGQA数据集上优于最先进的方法，在WebQSP数据集上F1、Hits@1和Accuracy分别提升了2.8%、1.2%和3.1%，在ComplexWebQuestions数据集上则分别提升了1.8%、1.1%和2.4%。"
    },
    {
        "title": "MAGMaR Shared Task System Description: Video Retrieval with OmniEmbed",
        "url": "http://arxiv.org/abs/2506.09409v1",
        "pub_date": "2025-06-11",
        "summary": "Effective video retrieval remains challenging due to the complexity of integrating visual, auditory, and textual modalities. In this paper, we explore unified retrieval methods using OmniEmbed, a powerful multimodal embedding model from the Tevatron 2.0 toolkit, in the context of the MAGMaR shared task. Evaluated on the comprehensive MultiVENT 2.0 dataset, OmniEmbed generates unified embeddings for text, images, audio, and video, enabling robust multimodal retrieval. By finetuning OmniEmbed with the combined multimodal data--visual frames, audio tracks, and textual descriptions provided in MultiVENT 2.0, we achieve substantial improvements in complex, multilingual video retrieval tasks. Our submission achieved the highest score on the MAGMaR shared task leaderboard among public submissions as of May 20th, 2025, highlighting the practical effectiveness of our unified multimodal retrieval approach. Model checkpoint in this work is opensourced.",
        "translated": "由于视觉、听觉和文本模态整合的复杂性，有效的视频检索仍然面临挑战。本文探讨了在 MAGMaR 共享任务背景下，使用 Tevatron 2.0 工具包中强大的多模态嵌入模型 OmniEmbed 来实现统一检索的方法。\n\nOmniEmbed 在综合性的 MultiVENT 2.0 数据集上进行评估，能够为文本、图像、音频和视频生成统一的嵌入表示，从而实现鲁棒的多模态检索。通过利用 MultiVENT 2.0 中提供的视觉帧、音频轨和文本描述等组合多模态数据对 OmniEmbed 进行微调，我们在复杂的、多语言视频检索任务中取得了显著提升。\n\n截至 2025 年 5 月 20 日，我们的提交方案在 MAGMaR 共享任务排行榜的公开提交中获得了最高分，这凸显了我们统一多模态检索方法的实用有效性。本文使用的模型检查点已开源。"
    },
    {
        "title": "ThinkQE: Query Expansion via an Evolving Thinking Process",
        "url": "http://arxiv.org/abs/2506.09260v1",
        "pub_date": "2025-06-10",
        "summary": "Effective query expansion for web search benefits from promoting both exploration and result diversity to capture multiple interpretations and facets of a query. While recent LLM-based methods have improved retrieval performance and demonstrate strong domain generalization without additional training, they often generate narrowly focused expansions that overlook these desiderata. We propose ThinkQE, a test-time query expansion framework addressing this limitation through two key components: a thinking-based expansion process that encourages deeper and comprehensive semantic exploration, and a corpus-interaction strategy that iteratively refines expansions using retrieval feedback from the corpus. Experiments on diverse web search benchmarks (DL19, DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches, including training-intensive dense retrievers and rerankers.",
        "translated": "网络搜索中有效的查询扩展，得益于促进探索性和结果多样性，以便捕获查询的多重释义和多重维度。尽管近期基于大语言模型（LLM）的方法在无需额外训练的情况下，提升了检索性能并展现出强大的领域泛化能力，但它们通常会生成范围过于狭窄的扩展，从而忽略了上述理想特性。\n\n为此，我们提出了ThinkQE，一个测试时（test-time）查询扩展框架，旨在解决这一局限性。该框架通过两个关键组件来实现：一是基于思考的扩展过程，旨在促进更深层次、更全面的语义探索；二是语料库交互策略，该策略利用来自语料库的检索反馈迭代地完善查询扩展。在多样化的网络搜索基准测试集（DL19、DL20和BRIGHT）上的实验表明，ThinkQE持续优于现有方法，包括那些需要大量训练的稠密检索器和重排序器。"
    },
    {
        "title": "In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation",
        "url": "http://arxiv.org/abs/2506.09221v1",
        "pub_date": "2025-06-10",
        "summary": "The spread of online misinformation poses serious threats to democratic societies. Traditionally, expert fact-checkers verify the truthfulness of information through investigative processes. However, the volume and immediacy of online content present major scalability challenges. Crowdsourcing offers a promising alternative by leveraging non-expert judgments, but it introduces concerns about bias, accuracy, and interpretability. This thesis investigates how human intelligence can be harnessed to assess the truthfulness of online information, focusing on three areas: misinformation assessment, cognitive biases, and automated fact-checking systems. Through large-scale crowdsourcing experiments and statistical modeling, it identifies key factors influencing human judgments and introduces a model for the joint prediction and explanation of truthfulness. The findings show that non-expert judgments often align with expert assessments, particularly when factors such as timing and experience are considered. By deepening our understanding of human judgment and bias in truthfulness assessment, this thesis contributes to the development of more transparent, trustworthy, and interpretable systems for combating misinformation.",
        "translated": "网络虚假信息的传播对民主社会构成严重威胁。传统上，专家事实核查员通过调查流程核实信息的真实性。然而，网络内容的海量体量和即时性带来了重大的可扩展性挑战。众包通过利用非专业人士的判断，提供了一种有前景的替代方案，但它也引发了对偏见、准确性和可解释性的担忧。\n\n本论文研究了如何利用人类智能来评估网络信息的真实性，侧重于三个领域：虚假信息评估、认知偏差和自动化事实核查系统。通过大规模众包实验和统计建模，本论文识别出影响人类判断的关键因素，并引入了一个用于真实性联合预测和解释的模型。研究结果表明，非专业人士的判断往往与专家评估一致，尤其是在考虑时效性和经验等因素时。通过深化我们对真实性评估中人类判断和偏见的理解，本论文为开发更透明、更值得信赖、更可解释的打击虚假信息系统做出了贡献。"
    },
    {
        "title": "Revisiting Graph Projections for Effective Complementary Product\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.09209v1",
        "pub_date": "2025-06-10",
        "summary": "Complementary product recommendation is a powerful strategy to improve customer experience and retail sales. However, recommending the right product is not a simple task because of the noisy and sparse nature of user-item interactions. In this work, we propose a simple yet effective method to predict a list of complementary products given a query item, based on the structure of a directed weighted graph projected from the user-item bipartite graph. We revisit bipartite graph projections for recommender systems and propose a novel approach for inferring complementarity relationships from historical user-item interactions. We compare our model with recent methods from the literature and show, despite the simplicity of our approach, an average improvement of +43% and +38% over sequential and graph-based recommenders, respectively, over different benchmarks.",
        "translated": "互补商品推荐是提升客户体验和零售额的强有力策略。然而，由于用户-商品交互中存在的噪声和稀疏性，推荐合适的产品并非易事。在本文中，我们提出了一种简单而有效的方法，用于在给定一个查询商品的情况下，基于从用户-商品二分图（user-item bipartite graph）投影得到的有向加权图（directed weighted graph）结构来预测互补商品列表。我们重新审视了用于推荐系统的二分图投影，并提出了一种从历史用户-商品交互中推断互补关系的新颖方法。我们将我们的模型与文献中最新的方法进行比较，结果表明，尽管我们的方法很简单，但在不同的基准数据集上，相较于序列推荐器（sequential recommenders）和基于图的推荐器（graph-based recommenders），我们的方法平均分别取得了43%和38%的提升。"
    },
    {
        "title": "Multimodal Representation Alignment for Cross-modal Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.08774v1",
        "pub_date": "2025-06-10",
        "summary": "Different machine learning models can represent the same underlying concept in different ways. This variability is particularly valuable for in-the-wild multimodal retrieval, where the objective is to identify the corresponding representation in one modality given another modality as input. This challenge can be effectively framed as a feature alignment problem. For example, given a sentence encoded by a language model, retrieve the most semantically aligned image based on features produced by an image encoder, or vice versa. In this work, we first investigate the geometric relationships between visual and textual embeddings derived from both vision-language models and combined unimodal models. We then align these representations using four standard similarity metrics as well as two learned ones, implemented via neural networks. Our findings indicate that the Wasserstein distance can serve as an informative measure of the modality gap, while cosine similarity consistently outperforms alternative metrics in feature alignment tasks. Furthermore, we observe that conventional architectures such as multilayer perceptrons are insufficient for capturing the complex interactions between image and text representations. Our study offers novel insights and practical considerations for researchers working in multimodal information retrieval, particularly in real-world, cross-modal applications.",
        "translated": "不同的机器学习模型能够以不同的方式表征相同的底层概念。这种多样性对于实际应用场景下的多模态检索尤为重要，其目标是在给定一种模态作为输入时，识别出另一种模态中对应的表征。这一挑战可以被有效地建模为一个特征对齐问题。例如，给定一个由语言模型编码的句子，基于图像编码器生成的特征检索语义上最对齐的图像，反之亦然。\n\n在这项工作中，我们首先探究了分别从视觉-语言模型和组合式单模态模型中获得的视觉嵌入和文本嵌入之间的几何关系。随后，我们使用四种标准相似性度量以及两种通过神经网络实现的学习型度量对这些表征进行对齐。我们的研究结果表明，Wasserstein距离可以作为模态间隙的有效衡量标准，而余弦相似度在特征对齐任务中始终优于其他备选度量。此外，我们观察到多层感知机（MLP）等传统架构不足以捕获图像和文本表征之间复杂的交互作用。我们的研究为多模态信息检索领域的研究人员提供了新颖的见解和实用性考量，尤其是在真实世界的跨模态应用中。"
    },
    {
        "title": "Paths to Causality: Finding Informative Subgraphs Within Knowledge\n  Graphs for Knowledge-Based Causal Discovery",
        "url": "http://arxiv.org/abs/2506.08771v1",
        "pub_date": "2025-06-10",
        "summary": "Inferring causal relationships between variable pairs is crucial for understanding multivariate interactions in complex systems. Knowledge-based causal discovery -- which involves inferring causal relationships by reasoning over the metadata of variables (e.g., names or textual context) -- offers a compelling alternative to traditional methods that rely on observational data. However, existing methods using Large Language Models (LLMs) often produce unstable and inconsistent results, compromising their reliability for causal inference. To address this, we introduce a novel approach that integrates Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery. Our approach identifies informative metapath-based subgraphs within KGs and further refines the selection of these subgraphs using Learning-to-Rank-based models. The top-ranked subgraphs are then incorporated into zero-shot prompts, improving the effectiveness of LLMs in inferring the causal relationship. Extensive experiments on biomedical and open-domain datasets demonstrate that our method outperforms most baselines by up to 44.4 points in F1 scores, evaluated across diverse LLMs and KGs. Our code and datasets are available on GitHub: https://github.com/susantiyuni/path-to-causality",
        "translated": "推断变量对之间的因果关系对于理解复杂系统中的多变量交互至关重要。基于知识的因果发现——通过对变量的元数据（例如，名称或文本上下文）进行推理来推断因果关系——为依赖观测数据的传统方法提供了一种引人注目的替代方法。然而，现有使用大型语言模型（LLM）的方法经常产生不稳定和不一致的结果，损害了它们进行因果推断的可靠性。\n\n为此，我们提出了一种新颖的方法，将知识图谱（KG）与LLM相结合，以增强基于知识的因果发现。我们的方法识别KG中信息丰富的元路径子图，并使用基于学习排序（Learning-to-Rank）的模型进一步优化这些子图的选择。排名靠前的子图随后被纳入零样本提示中，从而提高了LLM在推断因果关系方面的有效性。在生物医学和开放域数据集上进行的大量实验表明，我们的方法在F1分数上最高可优于大多数基线44.4点，并在多种LLM和KG上进行了评估。我们的代码和数据集可在GitHub上获取：https://github.com/susantiyuni/path-to-causality"
    },
    {
        "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and\n  Re-ranking",
        "url": "http://arxiv.org/abs/2506.09944v1",
        "pub_date": "2025-06-11",
        "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. We identify QRHEAD by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). We further introduce QR- RETRIEVER, an efficient and effective retriever that uses the accumulated attention mass of QRHEAD as retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. We also evaluate QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the querycontext attention scoring and task selection are crucial for identifying QRHEAD with strong downstream utility. Overall, our work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs.",
        "translated": "近期研究（Wu 等人，2025b）已识别出“检索头”（retrieval heads），它们是注意力头的一个子集，负责在长上下文语言模型（LM）中检索关键信息，并通过其在“大海捞针”任务中的复制粘贴行为进行衡量。在本文中，我们引入了 QRHEAD（查询聚焦检索头），这是一组改进的注意力头，能够增强从长上下文中的信息检索能力。我们通过结合输入查询聚合注意力分数来识别 QRHEAD，并利用少量真实世界任务（例如长上下文问答）的示例。我们进一步引入了 QR-RETRIEVER，这是一种高效且有效的检索器，它使用 QRHEAD 的累积注意力权重作为检索分数。我们将 QR-RETRIEVER 用于长上下文推理，通过选择检索分数最高的、最相关的部分。在多跳推理任务 LongMemEval 和 CLIPPER 上，这使得性能相较于完整上下文提升了 10% 以上，并优于强大的稠密检索器。我们还将 QR-RETRIEVER 作为重排器在 BEIR 基准测试上进行了评估，发现它实现了强大的零样本性能，优于其他基于大型语言模型（LLM）的重排器，例如 RankGPT。进一步分析表明，查询-上下文注意力评分和任务选择对于识别具有强大下游效用的 QRHEAD 都至关重要。总而言之，我们的工作贡献了一个通用检索器，并为语言模型的长上下文能力提供了可解释性见解。"
    },
    {
        "title": "Aspect-Based Opinion Summarization with Argumentation Schemes",
        "url": "http://arxiv.org/abs/2506.09917v1",
        "pub_date": "2025-06-11",
        "summary": "Reviews are valuable resources for customers making purchase decisions in online shopping. However, it is impractical for customers to go over the vast number of reviews and manually conclude the prominent opinions, which prompts the need for automated opinion summarization systems. Previous approaches, either extractive or abstractive, face challenges in automatically producing grounded aspect-centric summaries. In this paper, we propose a novel summarization system that not only captures predominant opinions from an aspect perspective with supporting evidence, but also adapts to varying domains without relying on a pre-defined set of aspects. Our proposed framework, ASESUM, summarizes viewpoints relevant to the critical aspects of a product by extracting aspect-centric arguments and measuring their salience and validity. We conduct experiments on a real-world dataset to demonstrate the superiority of our approach in capturing diverse perspectives of the original reviews compared to new and existing methods.",
        "translated": "在线购物中，评论是顾客做出购买决策的宝贵资源。然而，顾客逐一查阅海量评论并从中人工归纳出主要观点是不切实际的，这促使了对自动化观点摘要系统的需求。现有的抽取式或生成式方法在自动生成有事实依据的方面中心摘要方面面临挑战。在本文中，我们提出了一种新颖的摘要系统，它不仅能够从方面角度捕获主要观点并附带支持证据，而且无需依赖预定义的方面集合即可适应不同领域。我们提出的框架ASESUM通过提取方面中心论据并衡量其显著性和有效性，从而总结出与产品关键方面相关的观点。我们在真实世界数据集上进行了实验，结果表明，与新方法和现有方法相比，我们的方法在捕获原始评论多样化视角方面表现出优越性。"
    },
    {
        "title": "Precise Zero-Shot Pointwise Ranking with LLMs through Post-Aggregated\n  Global Context Information",
        "url": "http://arxiv.org/abs/2506.10859v1",
        "pub_date": "2025-06-12",
        "summary": "Recent advancements have successfully harnessed the power of Large Language Models (LLMs) for zero-shot document ranking, exploring a variety of prompting strategies. Comparative approaches like pairwise and listwise achieve high effectiveness but are computationally intensive and thus less practical for larger-scale applications. Scoring-based pointwise approaches exhibit superior efficiency by independently and simultaneously generating the relevance scores for each candidate document. However, this independence ignores critical comparative insights between documents, resulting in inconsistent scoring and suboptimal performance. In this paper, we aim to improve the effectiveness of pointwise methods while preserving their efficiency through two key innovations: (1) We propose a novel Global-Consistent Comparative Pointwise Ranking (GCCP) strategy that incorporates global reference comparisons between each candidate and an anchor document to generate contrastive relevance scores. We strategically design the anchor document as a query-focused summary of pseudo-relevant candidates, which serves as an effective reference point by capturing the global context for document comparison. (2) These contrastive relevance scores can be efficiently Post-Aggregated with existing pointwise methods, seamlessly integrating essential Global Context information in a training-free manner (PAGC). Extensive experiments on the TREC DL and BEIR benchmark demonstrate that our approach significantly outperforms previous pointwise methods while maintaining comparable efficiency. Our method also achieves competitive performance against comparative methods that require substantially more computational resources. More analyses further validate the efficacy of our anchor construction strategy.",
        "translated": "近期进展已成功驾驭大语言模型（LLM）进行零样本文档排序，并探索了多种提示策略。成对比较和列表比较等对比方法虽然能取得高有效性，但其计算开销大，因此不适用于大规模应用。基于评分的逐点方法通过独立且并行地生成每个候选文档的相关性分数，展现出卓越的效率。然而，这种独立性忽略了文档间关键的比较信息，从而导致评分不一致和次优性能。\n\n在本文中，我们旨在通过两项关键创新，在保持效率的同时提升逐点方法的有效性：(1) 我们提出了一种新颖的全局一致比较逐点排序（Global-Consistent Comparative Pointwise Ranking, GCCP）策略，该策略引入了每个候选文档与一个锚点文档之间的全局参考比较，以生成对比相关性分数。我们策略性地将锚点文档设计为伪相关候选文档的查询中心摘要，通过捕获文档比较的全局上下文，它可作为一个有效的参考点。(2) 这些对比相关性分数可以与现有逐点方法进行高效的后聚合（Post-Aggregated with Global Context, PAGC），以免训练的方式无缝集成关键的全局上下文信息。\n\n在TREC DL和BEIR基准数据集上的大量实验表明，我们的方法显著超越了以往的逐点方法，同时保持了可比的效率。此外，我们的方法与需要大量计算资源的对比方法相比也达到了具有竞争力的性能。进一步的分析也验证了我们锚点构建策略的有效性。"
    },
    {
        "title": "CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation\n  through Self-Training",
        "url": "http://arxiv.org/abs/2506.10844v1",
        "pub_date": "2025-06-12",
        "summary": "This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG) framework composed of specialized agents for subtasks such as planning, searching, reasoning, and coordination. Our system uses a self-training paradigm with reward-guided trajectory sampling to optimize inter-agent collaboration and enhance response generation. Evaluated on DataMorgana-derived datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms conventional RAG baselines. We further analyze competition outcomes and showcase the framework's strengths with case studies, demonstrating its efficacy for complex, real-world RAG tasks.",
        "translated": "本文提出了mRAG，这是一个多智能体检索增强生成（RAG）框架，它由专门用于规划、搜索、推理和协调等子任务的智能体组成。我们的系统采用自训练范式，通过奖励引导的轨迹采样来优化智能体间的协作并提升响应生成能力。在SIGIR 2025 LiveRAG 竞赛期间，mRAG 在源自DataMorgana的数据集上进行评估，其性能超越了传统的RAG基线。我们进一步分析了竞赛结果，并通过案例研究展示了该框架的优势，证明了其在复杂、真实世界的RAG任务中的有效性。"
    },
    {
        "title": "Constructing and Evaluating Declarative RAG Pipelines in PyTerrier",
        "url": "http://arxiv.org/abs/2506.10802v1",
        "pub_date": "2025-06-12",
        "summary": "Search engines often follow a pipeline architecture, where complex but effective reranking components are used to refine the results of an initial retrieval. Retrieval augmented generation (RAG) is an exciting application of the pipeline architecture, where the final component generates a coherent answer for the users from the retrieved documents. In this demo paper, we describe how such RAG pipelines can be formulated in the declarative PyTerrier architecture, and the advantages of doing so. Our PyTerrier-RAG extension for PyTerrier provides easy access to standard RAG datasets and evaluation measures, state-of-the-art LLM readers, and using PyTerrier's unique operator notation, easy-to-build pipelines. We demonstrate the succinctness of indexing and RAG pipelines on standard datasets (including Natural Questions) and how to build on the larger PyTerrier ecosystem with state-of-the-art sparse, learned-sparse, and dense retrievers, and other neural rankers.",
        "translated": "搜索引擎通常遵循流水线架构，其中使用复杂但有效的重排序组件来优化初始检索的结果。检索增强生成（RAG）是流水线架构的一种令人兴奋的应用，其最终组件从检索到的文档中为用户生成连贯的答案。在这篇演示论文中，我们描述了如何在声明式 PyTerrier 架构中构建此类 RAG 流水线，并阐述了这样做的好处。\n\n我们的 PyTerrier-RAG 扩展为 PyTerrier 提供了便捷的访问方式，可用于标准 RAG 数据集和评估指标、最先进的 LLM 阅读器，并且利用 PyTerrier 独特的运算符表示法，可以轻松构建流水线。我们展示了在标准数据集（包括 Natural Questions）上构建索引和 RAG 流水线的简洁性，以及如何利用更大的 PyTerrier 生态系统，结合最先进的稀疏、学习型稀疏和稠密检索器以及其他神经网络排序器。"
    },
    {
        "title": "TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to\n  Evolving Research Corpora",
        "url": "http://arxiv.org/abs/2506.10737v1",
        "pub_date": "2025-06-12",
        "summary": "The rapid evolution of scientific fields introduces challenges in organizing and retrieving scientific literature. While expert-curated taxonomies have traditionally addressed this need, the process is time-consuming and expensive. Furthermore, recent automatic taxonomy construction methods either (1) over-rely on a specific corpus, sacrificing generalizability, or (2) depend heavily on the general knowledge of large language models (LLMs) contained within their pre-training datasets, often overlooking the dynamic nature of evolving scientific domains. Additionally, these approaches fail to account for the multi-faceted nature of scientific literature, where a single research paper may contribute to multiple dimensions (e.g., methodology, new tasks, evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a framework that dynamically adapts an LLM-generated taxonomy to a given corpus across multiple dimensions. TaxoAdapt performs iterative hierarchical classification, expanding both the taxonomy width and depth based on corpus' topical distribution. We demonstrate its state-of-the-art performance across a diverse set of computer science conferences over the years to showcase its ability to structure and capture the evolution of scientific fields. As a multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more granularity-preserving and 50.41% more coherent than the most competitive baselines judged by LLMs.",
        "translated": "科学领域的快速演进给科学文献的组织和检索带来了挑战。尽管专家人工构建的分类体系（taxonomies）传统上能满足这一需求，但其过程耗时且昂贵。此外，近期自动分类体系构建方法存在两类问题：(1) 过度依赖特定语料库，牺牲了泛化能力；(2) 严重依赖大型语言模型（LLM）预训练数据中包含的通用知识，常常忽视不断演进的科学领域的动态特性。此外，这些方法未能考虑到科学文献的多维度特性，即一篇研究论文可能在多个维度上有所贡献（例如，方法论、新任务、评估指标、基准）。\n\n为了弥补这些不足，我们提出了TaxoAdapt，一个能将LLM生成的分类体系动态适应到给定语料库的跨维度框架。TaxoAdapt执行迭代分层分类，基于语料库的主题分布扩展分类体系的广度和深度。我们通过在多年间各种计算机科学会议上的实验，展示了其最先进的性能，以彰显其构建结构和捕捉科学领域演进的能力。作为一种多维度方法，TaxoAdapt生成的分类体系在LLM评判下，比最具竞争力的基线方法在粒度保持性上高出26.51%，在连贯性上高出50.41%。"
    },
    {
        "title": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of\n  Nuanced Claims",
        "url": "http://arxiv.org/abs/2506.10728v1",
        "pub_date": "2025-06-12",
        "summary": "Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely \"true\" or \"false\" -- as is frequently the case with scientific and political claims. However, a claim (e.g., \"vaccine A is better than vaccine B\") can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., \"how many biomedical papers believe vaccine A is more transportable than B?\"). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines.",
        "translated": "个人或实体提出的主张往往细致入微，无法被明确地标记为完全“真”或“假”——科学和政治领域的主张尤其如此。然而，一项主张（例如，“疫苗A优于疫苗B”）可以被剖析为其组成方面和子方面（例如，功效、安全性、分发），这些细分方面更容易单独验证。这种方法能够促成更全面、结构化的回应，为特定问题提供多维度的视角，同时允许读者优先关注主张中感兴趣的特定角度（例如，对儿童的安全性）。\n\n为此，我们提出ClaimSpect，一个基于检索增强生成（RAG）的框架，旨在自动构建处理主张时通常会考虑的方面层级结构，并利用特定语料库的视角来丰富这些方面。该结构能够对输入语料库进行分层分区，以检索相关片段，从而有助于发现新的子方面。此外，这些片段还能揭示对主张某个方面的不同观点（例如，支持、中立或反对），以及这些观点的普遍程度（例如，“有多少生物医学论文认为疫苗A比疫苗B更易于运输？”）。我们将ClaimSpect应用于我们所构建数据集中涵盖的各类真实世界科学和政治主张，展示了其在解构细致入微的主张和表示语料库内观点方面的鲁棒性和准确性。通过真实世界案例研究和人工评估，我们验证了其相对于多个基线的有效性。"
    },
    {
        "title": "Contrastive Matrix Completion with Denoising and Augmented Graph Views\n  for Robust Recommendation",
        "url": "http://arxiv.org/abs/2506.10658v1",
        "pub_date": "2025-06-12",
        "summary": "Matrix completion is a widely adopted framework in recommender systems, as predicting the missing entries in the user-item rating matrix enables a comprehensive understanding of user preferences. However, current graph neural network (GNN)-based approaches are highly sensitive to noisy or irrelevant edges--due to their inherent message-passing mechanisms--and are prone to overfitting, which limits their generalizability. To overcome these challenges, we propose a novel method called Matrix Completion using Contrastive Learning (MCCL). Our approach begins by extracting local neighborhood subgraphs for each interaction and subsequently generates two distinct graph representations. The first representation emphasizes denoising by integrating GNN layers with an attention mechanism, while the second is obtained via a graph variational autoencoder that aligns the feature distribution with a standard prior. A mutual learning loss function is employed during training to gradually harmonize these representations, enabling the model to capture common patterns and significantly enhance its generalizability. Extensive experiments on several real-world datasets demonstrate that our approach not only improves the numerical accuracy of the predicted scores--achieving up to a 0.8% improvement in RMSE--but also produces superior rankings with improvements of up to 36% in ranking metrics.",
        "translated": "矩阵补全作为推荐系统中广泛采用的框架，通过预测用户-物品评分矩阵中的缺失项，能够全面理解用户偏好。然而，当前基于图神经网络（GNN）的方法由于其固有的消息传递机制，对噪声或不相关边高度敏感，且容易过拟合，这限制了它们的泛化能力。为了克服这些挑战，我们提出了一种名为“使用对比学习的矩阵补全”（MCCL）的新颖方法。我们的方法首先为每个交互提取局部邻域子图，随后生成两种不同的图表示。第一个表示通过集成GNN层和注意力机制来侧重于去噪，而第二个则通过图变分自编码器获得，该编码器将特征分布与标准先验对齐。在训练过程中，我们采用互学习损失函数来逐步协调这些表示，使模型能够捕获共同模式并显著增强其泛化能力。在多个真实世界数据集上进行的广泛实验表明，我们的方法不仅提高了预测分数的数值精度——在RMSE上实现了高达0.8%的改进——而且在排名指标上提升高达36%，产生了更优越的排名。"
    },
    {
        "title": "Conversational Search: From Fundamentals to Frontiers in the LLM Era",
        "url": "http://arxiv.org/abs/2506.10635v1",
        "pub_date": "2025-06-12",
        "summary": "Conversational search enables multi-turn interactions between users and systems to fulfill users' complex information needs. During this interaction, the system should understand the users' search intent within the conversational context and then return the relevant information through a flexible, dialogue-based interface. The recent powerful large language models (LLMs) with capacities of instruction following, content generation, and reasoning, attract significant attention and advancements, providing new opportunities and challenges for building up intelligent conversational search systems. This tutorial aims to introduce the connection between fundamentals and the emerging topics revolutionized by LLMs in the context of conversational search. It is designed for students, researchers, and practitioners from both academia and industry. Participants will gain a comprehensive understanding of both the core principles and cutting-edge developments driven by LLMs in conversational search, equipping them with the knowledge needed to contribute to the development of next-generation conversational search systems.",
        "translated": "对话式搜索使得用户与系统之间能够进行多轮交互，以满足用户复杂的信息需求。在此交互过程中，系统应理解用户在对话上下文中的搜索意图，然后通过灵活的、基于对话的界面返回相关信息。\n\n近期强大的大型语言模型（LLMs），具备指令遵循、内容生成和推理的能力，吸引了广泛关注并取得了显著进展，为构建智能对话式搜索系统提供了新的机遇和挑战。本教程旨在介绍在对话式搜索领域中，基础知识与由LLMs带来变革的新兴主题之间的联系。\n\n本教程面向来自学术界和工业界的学生、研究人员和实践者。参与者将获得对对话式搜索中由LLMs驱动的核心原则和前沿发展的全面理解，从而掌握所需的知识，以助力下一代对话式搜索系统的发展。"
    },
    {
        "title": "Macro Graph of Experts for Billion-Scale Multi-Task Recommendation",
        "url": "http://arxiv.org/abs/2506.10520v1",
        "pub_date": "2025-06-12",
        "summary": "Graph-based multi-task learning at billion-scale presents a significant challenge, as different tasks correspond to distinct billion-scale graphs. Traditional multi-task learning methods often neglect these graph structures, relying solely on individual user and item embeddings. However, disregarding graph structures overlooks substantial potential for improving performance. In this paper, we introduce the Macro Graph of Expert (MGOE) framework, the first approach capable of leveraging macro graph embeddings to capture task-specific macro features while modeling the correlations between task-specific experts. Specifically, we propose the concept of a Macro Graph Bottom, which, for the first time, enables multi-task learning models to incorporate graph information effectively. We design the Macro Prediction Tower to dynamically integrate macro knowledge across tasks. MGOE has been deployed at scale, powering multi-task learning for the homepage of a leading billion-scale recommender system. Extensive offline experiments conducted on three public benchmark datasets demonstrate its superiority over state-of-the-art multi-task learning methods, establishing MGOE as a breakthrough in multi-task graph-based recommendation. Furthermore, online A/B tests confirm the superiority of MGOE in billion-scale recommender systems.",
        "translated": "在十亿级规模下进行基于图的多任务学习面临巨大挑战，因为不同的任务对应着各自独立的十亿级图。传统的的多任务学习方法常忽略这些图结构，仅依赖于独立的用户和物品嵌入。然而，忽视图结构会错失巨大的性能提升潜力。本文提出宏观专家图（Macro Graph of Expert, MGOE）框架，这是首个能够利用宏观图嵌入捕获任务特定宏观特征，并同时建模任务特定专家之间关联性的方法。具体而言，我们提出了宏观图底部（Macro Graph Bottom）的概念，这首次使多任务学习模型能够有效地融入图信息。我们设计了宏观预测塔（Macro Prediction Tower）来动态整合跨任务的宏观知识。MGOE已实现大规模部署，驱动着某领先十亿级推荐系统首页的多任务学习。在三个公开基准数据集上进行的广泛离线实验证明了MGOE优于最先进的多任务学习方法，使其成为基于图的多任务推荐领域的突破性进展。此外，在线A/B测试进一步证实了MGOE在十亿级推荐系统中的优越性。"
    },
    {
        "title": "Generative Representational Learning of Foundation Models for\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.11999v1",
        "pub_date": "2025-06-13",
        "summary": "Developing a single foundation model with the capability to excel across diverse tasks has been a long-standing objective in the field of artificial intelligence. As the wave of general-purpose foundation models sweeps across various domains, their influence has significantly extended to the field of recommendation systems. While recent efforts have explored recommendation foundation models for various generative tasks, they often overlook crucial embedding tasks and struggle with the complexities of multi-task learning, including knowledge sharing &amp; conflict resolution, and convergence speed inconsistencies. To address these limitations, we introduce RecFound, a generative representational learning framework for recommendation foundation models. We construct the first comprehensive dataset for recommendation foundation models covering both generative and embedding tasks across diverse scenarios. Based on this dataset, we propose a novel multi-task training scheme featuring a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge sharing &amp; conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched) to address inconsistent convergence, and a Model Merge module to balance the performance across tasks. Experiments demonstrate that RecFound achieves state-of-the-art performance across various recommendation tasks, outperforming existing baselines.",
        "translated": "在人工智能领域，开发一个能够胜任多样化任务的统一基础模型，一直是一个长期以来的目标。随着通用基础模型浪潮席卷各个领域，其影响力已深刻延伸至推荐系统领域。尽管近期研究已探索推荐基础模型在多种生成任务上的应用，但它们却往往忽视了关键的嵌入任务，并且在多任务学习的复杂性方面表现不足，例如知识共享与冲突消解、以及收敛速度不一致性等问题。\n\n为解决这些局限，我们提出了RecFound，一种用于推荐基础模型的生成式表征学习框架。我们构建了首个面向推荐基础模型的综合性数据集，该数据集同时涵盖了多种场景下的生成任务和嵌入任务。基于此数据集，我们提出了一种新颖的多任务训练方案，该方案包含一个任务级低秩专家混合（Task-wise Mixture of Low-rank Experts, TMoLE）模块以处理知识共享与冲突，一个分步式收敛导向采样调度器（Step-wise Convergence-oriented Sample Scheduler, S2Sched）以解决收敛不一致的问题，以及一个模型合并模块（Model Merge）以平衡跨任务的性能。实验结果表明，RecFound在各种推荐任务中均达到了最先进的水平，并优于现有基线。"
    },
    {
        "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
        "url": "http://arxiv.org/abs/2506.11763v1",
        "pub_date": "2025-06-13",
        "summary": "Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.",
        "translated": "深度研究智能体是基于大型语言模型（LLM）的智能体中的一个突出类别。通过自主编排多步骤网络探索、定向检索和高阶综合，它们能够将海量在线信息转化为分析师级别的、富含引用的报告——将数小时的人工桌面研究压缩至数分钟。然而，目前仍然缺乏一个能够系统性评估这些智能体能力的综合基准。\n\n为了弥补这一空白，我们提出了DeepResearch Bench，这是一个包含100个博士级研究任务的基准，每个任务均由22个不同领域的领域专家精心设计。评估深度研究智能体（DRA）本质上是复杂且劳动密集型的。因此，我们提出了两种新颖的方法，它们与人类判断具有高度一致性。第一种是基于参考的评估方法，采用自适应标准来评估生成研究报告的质量。另一个框架旨在通过评估其有效引用数量和整体引用准确性来评估DRA的信息检索和收集能力。我们已将DeepResearch Bench和这些框架的关键组件在https://github.com/Ayanami0730/deep_research_bench上开源，以加速实用的大型语言模型智能体的开发。"
    },
    {
        "title": "Forgetful by Design? A Critical Audit of YouTube's Search API for\n  Academic Research",
        "url": "http://arxiv.org/abs/2506.11727v1",
        "pub_date": "2025-06-13",
        "summary": "This paper critically audits the search endpoint of YouTube's Data API (v3), a common tool for academic research. Through systematic weekly searches over six months using eleven queries, we identify major limitations regarding completeness, representativeness, consistency, and bias. Our findings reveal substantial differences between ranking parameters like relevance and date in terms of video recall and precision, with relevance often retrieving numerous off-topic videos. We also find severe temporal decay, as the number of findable videos for a specific period dramatically decreases after just 20-60 days from the publication date, potentially hampering many different research designs. Furthermore, search results lack consistency, with identical queries yielding different video sets over time, compromising replicability. A case study on the European Parliament elections highlights how these issues impact research outcomes. While the paper offers several mitigation strategies, it concludes that the API's search function, potentially prioritizing \"freshness\" over comprehensive retrieval, is not adequate for robust academic research, especially concerning Digital Services Act requirements.",
        "translated": "本文批判性地审查了YouTube Data API (v3) 的搜索端点，该API是学术研究中常用的工具。通过系统地进行为期六个月的每周搜索，并使用十一个查询，我们发现了其在完整性、代表性、一致性和偏差方面的主要局限性。我们的研究结果揭示了相关性（relevance）和日期（date）等排序参数在视频召回率和精确率方面存在显著差异，其中相关性排序经常检索到大量偏离主题的视频。我们还发现严重的时效性衰减，即特定时期内可被找到的视频数量在发布日期后仅仅20-60天就急剧减少，这可能会阻碍许多不同的研究设计。此外，搜索结果缺乏一致性，相同的查询在不同时间会产生不同的视频集合，从而损害了研究的可复现性。一项关于欧洲议会选举的案例研究强调了这些问题如何影响研究成果。尽管本文提出了一些缓解策略，但它总结认为，该API的搜索功能（可能优先考虑“新鲜度”而非全面的检索）不足以支持严谨的学术研究，尤其是在满足《数字服务法案》要求方面。"
    },
    {
        "title": "TongSearch-QR: Reinforced Query Reasoning for Retrieval",
        "url": "http://arxiv.org/abs/2506.11603v1",
        "pub_date": "2025-06-13",
        "summary": "Traditional information retrieval (IR) methods excel at textual and semantic matching but struggle in reasoning-intensive retrieval tasks that require multi-hop inference or complex semantic understanding between queries and documents. One promising solution is to explicitly rewrite or augment queries using large language models (LLMs) to elicit reasoning-relevant content prior to retrieval. However, the widespread use of large-scale language models like GPT-4 or LLaMA3-70B remains impractical due to their high inference cost and limited deployability in real-world systems. In this work, we introduce TongSearch QR (Previously Known as \"TongSearch Reasoner\"), a family of small-scale language models for query reasoning and rewriting in reasoning-intensive retrieval. With a novel semi-rule-based reward function, we employ reinforcement learning approaches enabling smaller language models, e,g, Qwen2.5-7B-Instruct and Qwen2.5-1.5B-Instruct, to achieve query reasoning performance rivaling large-scale language models without their prohibitive inference costs. Experiment results on BRIGHT benchmark show that with BM25 as retrievers, both TongSearch QR-7B and TongSearch QR-1.5B models significantly outperform existing baselines, including prompt-based query reasoners and some latest dense retrievers trained for reasoning-intensive retrieval tasks, offering superior adaptability for real-world deployment.",
        "translated": "传统信息检索 (IR) 方法擅长文本和语义匹配，但在需要多跳推理或查询与文档之间复杂语义理解的推理密集型检索任务中表现不佳。一个有前景的解决方案是，在检索之前使用大型语言模型 (LLM) 显式地重写或增强查询，以引出与推理相关的内容。然而，由于像 GPT-4 或 LLaMA3-70B 这样的大型语言模型具有高昂的推理成本和在实际系统中有限的部署能力，它们的广泛应用仍然不切实际。\n\n在这项工作中，我们引入了 TongSearch QR（原名“TongSearch Reasoner”），这是一系列用于推理密集型检索中查询推理和重写的小型语言模型。借助一种新颖的半基于规则的奖励函数，我们采用了强化学习方法，使得像 Qwen2.5-7B-Instruct 和 Qwen2.5-1.5B-Instruct 这样的小型语言模型能够实现与大型语言模型相媲美的查询推理性能，而无需承担其高昂的推理成本。BRIGHT 基准测试上的实验结果表明，以 BM25 作为检索器时，TongSearch QR-7B 和 TongSearch QR-1.5B 模型均显著优于现有基线，包括基于提示的查询推理器以及一些为推理密集型检索任务训练的最新密集检索器，从而为实际部署提供了卓越的适应性。"
    },
    {
        "title": "GraphRAG-Causal: A novel graph-augmented framework for causal reasoning\n  and annotation in news",
        "url": "http://arxiv.org/abs/2506.11600v1",
        "pub_date": "2025-06-13",
        "summary": "GraphRAG-Causal introduces an innovative framework that combines graph-based retrieval with large language models to enhance causal reasoning in news analysis. Traditional NLP approaches often struggle with identifying complex, implicit causal links, especially in low-data scenarios. Our approach addresses these challenges by transforming annotated news headlines into structured causal knowledge graphs. It then employs a hybrid retrieval system that merges semantic embeddings with graph-based structural cues leveraging Neo4j to accurately match and retrieve relevant events. The framework is built on a three-stage pipeline: First, during Data Preparation, news sentences are meticulously annotated and converted into causal graphs capturing cause, effect, and trigger relationships. Next, the Graph Retrieval stage stores these graphs along with their embeddings in a Neo4j database and utilizes hybrid Cypher queries to efficiently identify events that share both semantic and structural similarities with a given query. Finally, the LLM Inference stage utilizes these retrieved causal graphs in a few-shot learning setup with XML-based prompting, enabling robust classification and tagging of causal relationships. Experimental evaluations demonstrate that GraphRAG-Causal achieves an impressive F1-score of 82.1% on causal classification using just 20 few-shot examples. This approach significantly boosts accuracy and consistency, making it highly suitable for real-time applications in news reliability assessment, misinformation detection, and policy analysis.",
        "translated": "GraphRAG-Causal 引入了一个创新的框架，该框架结合了图基检索与大型语言模型，旨在增强新闻分析中的因果推理能力。传统的自然语言处理（NLP）方法在识别复杂、隐式的因果关系方面常常面临挑战，特别是在数据稀缺的场景下。我们的方法通过将标注的新闻标题转化为结构化的因果知识图谱，解决了这些挑战。它随后采用了一种混合检索系统，该系统将语义嵌入与图基结构线索相结合，并利用 Neo4j 精确匹配和检索相关事件。\n\n该框架基于一个三阶段的流水线构建：\n\n1.  **数据准备**：新闻语句被精心标注并转换为因果图，捕获原因、结果和触发关系。\n2.  **图检索**：将这些图及其嵌入存储在 Neo4j 数据库中，并利用混合 Cypher 查询高效地识别与给定查询同时具有语义和结构相似性的事件。\n3.  **LLM 推理**：在基于 XML 提示的少样本学习设置中利用这些检索到的因果图，从而实现因果关系的鲁棒分类和标记。\n\n实验评估表明，GraphRAG-Causal 在仅使用 20 个少样本示例的情况下，在因果分类上取得了高达 82.1% 的 F1 分数。这种方法显著提升了准确性和一致性，使其非常适用于新闻可靠性评估、虚假信息检测和政策分析等实时应用。"
    },
    {
        "title": "Dual-View Disentangled Multi-Intent Learning for Enhanced Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2506.11538v1",
        "pub_date": "2025-06-13",
        "summary": "Disentangling user intentions from implicit feedback has become a promising strategy to enhance recommendation accuracy and interpretability. Prior methods often model intentions independently and lack explicit supervision, thus failing to capture the joint semantics that drive user-item interactions. To address these limitations, we propose DMICF, a unified framework that explicitly models interaction-level intent alignment while leveraging structural signals from both user and item perspectives. DMICF adopts a dual-view architecture that jointly encodes user-item interaction graphs from both sides, enabling bidirectional information fusion. This design enhances robustness under data sparsity by allowing the structural redundancy of one view to compensate for the limitations of the other. To model fine-grained user-item compatibility, DMICF introduces an intent interaction encoder that performs sub-intent alignment within each view, uncovering shared semantic structures that underlie user decisions. This localized alignment enables adaptive refinement of intent embeddings based on interaction context, thus improving the model's generalization and expressiveness, particularly in long-tail scenarios. Furthermore, DMICF integrates an intent-aware scoring mechanism that aggregates compatibility signals from matched intent pairs across user and item subspaces, enabling personalized prediction grounded in semantic congruence rather than entangled representations. To facilitate semantic disentanglement, we design a discriminative training signal via multi-negative sampling and softmax normalization, which pulls together semantically aligned intent pairs while pushing apart irrelevant or noisy ones. Extensive experiments demonstrate that DMICF consistently delivers robust performance across datasets with diverse interaction distributions.",
        "translated": "将用户意图从隐式反馈中解耦，已成为提升推荐准确性和可解释性的一种有前景的策略。以往的方法通常独立建模意图，且缺乏显式监督，因此未能捕捉驱动用户-物品交互的联合语义。为解决这些局限性，我们提出了DMICF，一个统一框架，它显式建模交互层面的意图对齐，同时利用用户和物品两个视角的结构信号。\n\nDMICF采用双视图架构，从用户和物品两侧共同编码用户-物品交互图，实现双向信息融合。这种设计通过允许一个视图的结构冗余来弥补另一个视图的局限性，从而增强了数据稀疏性下的鲁棒性。为了建模细粒度的用户-物品兼容性，DMICF引入了一个意图交互编码器，它在每个视图内部执行子意图对齐，从而揭示用户决策背后的共享语义结构。这种局部对齐能够基于交互上下文对意图嵌入进行自适应细化，进而提升模型的泛化能力和表达性，尤其在长尾场景中表现突出。\n\n此外，DMICF集成了意图感知评分机制，它聚合了用户和物品子空间中匹配意图对的兼容性信号，从而实现了基于语义一致性而非纠缠表示的个性化预测。为了促进语义解耦，我们通过多负采样和softmax归一化设计了一种判别性训练信号，该信号能拉近语义对齐的意图对，同时推开不相关或带有噪声的意图对。大量实验表明，DMICF在具有多样交互分布的数据集上始终展现出强大的鲁棒性能。"
    },
    {
        "title": "Leveraging Reference Documents for Zero-Shot Ranking via Large Language\n  Models",
        "url": "http://arxiv.org/abs/2506.11452v1",
        "pub_date": "2025-06-13",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in the task of text ranking for information retrieval. While Pointwise ranking approaches offer computational efficiency by scoring documents independently, they often yield biased relevance estimates due to the lack of inter-document comparisons. In contrast, Pairwise methods improve ranking accuracy by explicitly comparing document pairs, but suffer from substantial computational overhead with quadratic complexity ($O(n^2)$). To address this tradeoff, we propose \\textbf{RefRank}, a simple and effective comparative ranking method based on a fixed reference document. Instead of comparing all document pairs, RefRank prompts the LLM to evaluate each candidate relative to a shared reference anchor. By selecting the reference anchor that encapsulates the core query intent, RefRank implicitly captures relevance cues, enabling indirect comparison between documents via this common anchor. This reduces computational cost to linear time ($O(n)$) while importantly, preserving the advantages of comparative evaluation. To further enhance robustness, we aggregate multiple RefRank outputs using a weighted averaging scheme across different reference choices. Experiments on several benchmark datasets and with various LLMs show that RefRank significantly outperforms Pointwise baselines and could achieve performance at least on par with Pairwise approaches with a significantly lower computational cost.",
        "translated": "大型语言模型（LLM）在信息检索的文本排序任务中展现出卓越的性能。尽管逐点排序（Pointwise）方法通过独立地为文档评分来提供计算效率，但由于缺乏文档间比较，它们通常会产生有偏的相关性估计。相比之下，成对（Pairwise）方法通过显式比较文档对来提高排序准确性，但会带来巨大的计算开销，具有二次复杂度（$O(n^2)$）。为了解决这种权衡，我们提出了 \\textbf{RefRank}，一种基于固定参考文档的简单有效比较排序方法。RefRank 不再比较所有文档对，而是提示LLM评估每个候选文档相对于一个共享的参考锚点。通过选择能封装核心查询意图的参考锚点，RefRank 隐式地捕获相关性线索，从而通过这个共同锚点实现文档间的间接比较。这使得计算成本降低到线性时间（$O(n)$），同时重要的是，保留了比较评估的优势。为了进一步增强鲁棒性，我们采用加权平均方案，聚合了在不同参考选择下的多个 RefRank 输出。在多个基准数据集和各种LLM上的实验表明，RefRank 显著优于逐点基线方法，并且在计算成本显著降低的情况下，其性能至少与成对方法持平。"
    },
    {
        "title": "Deep Learning Model Acceleration and Optimization Strategies for\n  Real-Time Recommendation Systems",
        "url": "http://arxiv.org/abs/2506.11421v1",
        "pub_date": "2025-06-13",
        "summary": "With the rapid growth of Internet services, recommendation systems play a central role in delivering personalized content. Faced with massive user requests and complex model architectures, the key challenge for real-time recommendation systems is how to reduce inference latency and increase system throughput without sacrificing recommendation quality. This paper addresses the high computational cost and resource bottlenecks of deep learning models in real-time settings by proposing a combined set of modeling- and system-level acceleration and optimization strategies. At the model level, we dramatically reduce parameter counts and compute requirements through lightweight network design, structured pruning, and weight quantization. At the system level, we integrate multiple heterogeneous compute platforms and high-performance inference libraries, and we design elastic inference scheduling and load-balancing mechanisms based on real-time load characteristics. Experiments show that, while maintaining the original recommendation accuracy, our methods cut latency to less than 30% of the baseline and more than double system throughput, offering a practical solution for deploying large-scale online recommendation services.",
        "translated": "随着互联网服务的快速发展，推荐系统在提供个性化内容方面扮演着核心角色。面对海量用户请求和复杂的模型架构，实时推荐系统的关键挑战在于如何在不牺牲推荐质量的前提下，降低推理延迟并提高系统吞吐量。本文针对深度学习模型在实时场景中的高计算开销和资源瓶颈问题，提出了一套结合模型级和系统级的加速与优化策略。在模型层面，我们通过轻量级网络设计、结构化剪枝和权重量化等技术，显著减少了模型参数量和计算需求。在系统层面，我们整合了多个异构计算平台和高性能推理库，并基于实时负载特性设计了弹性推理调度和负载均衡机制。实验结果表明，在保持原有推荐精度的同时，我们的方法将延迟降低到基线的30%以下，并将系统吞吐量提升了一倍以上，为部署大规模在线推荐服务提供了一种实用的解决方案。"
    },
    {
        "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
        "url": "http://arxiv.org/abs/2506.12015v1",
        "pub_date": "2025-06-13",
        "summary": "Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.",
        "translated": "开源基础模型已得到快速采用和发展，在多样化领域实现了强大的通用能力。然而，由于微调大型基础模型所需的内存开销远超推理，对于大多数用户而言，将其微调用于领域特定或个性化任务仍然成本过高，令人望而却步。我们引入EMLoC（带有LoRA校正的基于模拟器的内存高效微调框架），它使得模型微调能够在与推理所需的相同内存预算内进行。\n\nEMLoC通过在小型下游校准集上应用激活感知奇异值分解（SVD），构建了一个任务特定的轻量级模拟器。随后，通过LoRA在该轻量级模拟器上执行微调。为解决原始模型与压缩模拟器之间的失配问题，我们提出了一种新颖的补偿算法来校正微调后的LoRA模块，从而可以将该模块合并到原始模型中进行推理。EMLoC支持灵活的压缩比和标准训练流程，使其能够适应广泛的应用。\n\n大量实验表明，EMLoC在多个数据集和模态上均优于其他基线方法。此外，在无需量化的前提下，EMLoC实现了在单一24GB消费级GPU上微调380亿参数模型，为个人用户带来了高效实用的模型适应。"
    },
    {
        "title": "Generative Representational Learning of Foundation Models for\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.11999v1",
        "pub_date": "2025-06-13",
        "summary": "Developing a single foundation model with the capability to excel across diverse tasks has been a long-standing objective in the field of artificial intelligence. As the wave of general-purpose foundation models sweeps across various domains, their influence has significantly extended to the field of recommendation systems. While recent efforts have explored recommendation foundation models for various generative tasks, they often overlook crucial embedding tasks and struggle with the complexities of multi-task learning, including knowledge sharing &amp; conflict resolution, and convergence speed inconsistencies. To address these limitations, we introduce RecFound, a generative representational learning framework for recommendation foundation models. We construct the first comprehensive dataset for recommendation foundation models covering both generative and embedding tasks across diverse scenarios. Based on this dataset, we propose a novel multi-task training scheme featuring a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge sharing &amp; conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched) to address inconsistent convergence, and a Model Merge module to balance the performance across tasks. Experiments demonstrate that RecFound achieves state-of-the-art performance across various recommendation tasks, outperforming existing baselines.",
        "translated": "人工智能领域的一个长期目标是开发能够胜任各种任务的单一基础模型。随着通用型基础模型浪潮席卷各个领域，它们的影响力已显著扩展到推荐系统领域。尽管最近的研究探索了用于各种生成任务的推荐基础模型，但它们常常忽视关键的嵌入任务，并且难以应对多任务学习的复杂性，包括知识共享与冲突解决，以及收敛速度不一致性。\n\n为了解决这些局限性，我们引入了RecFound，一个专为推荐基础模型设计的生成式表示学习框架。我们构建了首个综合性数据集，专用于推荐基础模型，涵盖了跨越不同场景的生成任务和嵌入任务。基于该数据集，我们提出了一种新颖的多任务训练方案，其特点包括：任务级低秩专家混合（TMoLE）以处理知识共享与冲突、逐步收敛导向型样本调度器（S2Sched）以解决不一致的收敛问题，以及一个模型合并模块以平衡各项任务的性能。实验表明，RecFound在各种推荐任务中均达到了最先进的性能，超越了现有基线。"
    },
    {
        "title": "LTRR: Learning To Rank Retrievers for LLMs",
        "url": "http://arxiv.org/abs/2506.13743v1",
        "pub_date": "2025-06-16",
        "summary": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed retriever, despite growing evidence that no single retriever performs optimally across all query types. In this paper, we explore a query routing approach that dynamically selects from a pool of retrievers based on the query, using both train-free heuristics and learned routing models. We frame routing as a learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to rank retrievers by their expected utility gain to downstream LLM performance. Our experiments, conducted on synthetic QA data with controlled query type variations, show that routing-based RAG systems can outperform the best single-retriever-based systems. Performance gains are especially pronounced in models trained with the Answer Correctness (AC) metric and with pairwise learning approaches, especially with XGBoost. We also observe improvements in generalization to out-of-distribution queries. As part of the SIGIR 2025 LiveRAG challenge, our submitted system demonstrated the practical viability of our approach, achieving competitive performance in both answer correctness and faithfulness. These findings highlight the importance of both training methodology and metric selection in query routing for RAG systems.",
        "translated": "检索增强生成（RAG）系统通常依赖于单个固定的检索器，尽管越来越多的证据表明没有哪个单一检索器能在所有查询类型上都达到最佳性能。在本文中，我们探索了一种查询路由方法，该方法能够根据查询，结合使用免训练启发式方法和学习型路由模型，动态地从检索器池中选择检索器。我们将路由问题构建为一个学习排序（LTR）问题，并引入了LTRR，这是一个学习根据检索器对下游大型语言模型（LLM）性能的预期效用增益对其进行排序的框架。\n\n我们的实验在具有受控查询类型变化的合成问答数据上进行，结果表明基于路由的RAG系统能够超越性能最佳的单一检索器系统。性能提升在使用答案正确性（AC）指标训练的模型以及采用成对学习方法时尤其显著，特别是使用XGBoost时。我们还观察到在对分布外查询的泛化能力方面有所改进。作为SIGIR 2025 LiveRAG挑战赛的一部分，我们提交的系统展示了我们方法的实际可行性，在答案正确性和忠实性方面均取得了有竞争力的性能。这些发现强调了训练方法和度量指标选择在RAG系统查询路由中的重要性。"
    },
    {
        "title": "OneRec Technical Report",
        "url": "http://arxiv.org/abs/2506.13695v1",
        "pub_date": "2025-06-16",
        "summary": "Recommender systems have been widely used in various large-scale user-oriented platforms for many years. However, compared to the rapid developments in the AI community, recommendation systems have not achieved a breakthrough in recent years. For instance, they still rely on a multi-stage cascaded architecture rather than an end-to-end approach, leading to computational fragmentation and optimization inconsistencies, and hindering the effective application of key breakthrough technologies from the AI community in recommendation scenarios.   To address these issues, we propose OneRec, which reshapes the recommendation system through an end-to-end generative approach and achieves promising results. Firstly, we have enhanced the computational FLOPs of the current recommendation model by 10 $\\times$ and have identified the scaling laws for recommendations within certain boundaries. Secondly, reinforcement learning techniques, previously difficult to apply for optimizing recommendations, show significant potential in this framework. Lastly, through infrastructure optimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU) on flagship GPUs during training and inference, respectively, aligning closely with the LLM community. This architecture significantly reduces communication and storage overhead, resulting in operating expense that is only 10.6% of traditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP, it handles 25% of total queries per second, enhancing overall App Stay Time by 0.54% and 1.24%, respectively. Additionally, we have observed significant increases in metrics such as 7-day Lifetime, which is a crucial indicator of recommendation experience. We also provide practical lessons and insights derived from developing, optimizing, and maintaining a production-scale recommendation system with significant real-world impact.",
        "translated": "推荐系统多年来已广泛应用于各种大规模面向用户的平台。然而，与人工智能领域的快速发展相比，推荐系统近年来并未取得突破性进展。例如，它们仍依赖于多阶段级联架构而非端到端方法，这导致了计算碎片化和优化不一致性，阻碍了人工智能领域关键突破性技术在推荐场景中的有效应用。\n\n为解决这些问题，我们提出了OneRec，它通过端到端生成式方法重塑了推荐系统，并取得了喜人的成果。首先，我们将当前推荐模型的计算浮点运算次数（FLOPs）提升了10倍，并在特定范围内发现了推荐的缩放定律。其次，此前难以应用于推荐优化的强化学习技术，在此框架中展现出巨大潜力。最后，通过基础设施优化，我们在旗舰级GPU上实现了训练和推理过程中23.7%和28.8%的模型浮点运算利用率（MFU），这与大语言模型（LLM）领域高度一致。该架构显著降低了通信和存储开销，使得运营支出（OpEx）仅为传统推荐流水线的10.6%。它已部署于快手/快手极速版APP，处理了总查询量（QPS）的25%，分别将总App停留时长提升了0.54%和1.24%。此外，我们观察到7日留存等指标也显著增长，这是衡量推荐体验的关键指标。我们还分享了在开发、优化和维护具有重大实际影响的生产规模推荐系统过程中获得的实践经验和见解。"
    },
    {
        "title": "Tree-Based Text Retrieval via Hierarchical Clustering in RAGFrameworks:\n  Application on Taiwanese Regulations",
        "url": "http://arxiv.org/abs/2506.13607v1",
        "pub_date": "2025-06-16",
        "summary": "Traditional Retrieval-Augmented Generation (RAG) systems employ brute-force inner product search to retrieve the top-k most similar documents, then combined with the user query and passed to a language model. This allows the model to access external knowledge and reduce hallucinations. However, selecting an appropriate k value remains a significant challenge in practical applications: a small k may fail to retrieve sufficient information, while a large k can introduce excessive and irrelevant content. To address this, we propose a hierarchical clustering-based retrieval method that eliminates the need to predefine k. Our approach maintains the accuracy and relevance of system responses while adaptively selecting semantically relevant content. In the experiment stage, we applied our method to a Taiwanese legal dataset with expert-graded queries. The results show that our approach achieves superior performance in expert evaluations and maintains high precision while eliminating the need to predefine k, demonstrating improved accuracy and interpretability in legal text retrieval tasks. Our framework is simple to implement and easily integrates with existing RAG pipelines, making it a practical solution for real-world applications under limited resources.",
        "translated": "传统的检索增强生成（RAG）系统采用暴力内积搜索来检索最相似的top-k个文档，随后将这些文档与用户查询结合并输入给语言模型。这使得模型能够访问外部知识并减少幻觉（hallucinations）。然而，在实际应用中，选择一个合适的k值仍然是一个重大挑战：k值过小可能无法检索到足够的信息，而k值过大则可能引入过多无关内容。为解决此问题，我们提出了一种基于层次聚类的检索方法，该方法无需预定义k值。我们的方法在保持系统响应的准确性和相关性的同时，自适应地选择语义相关内容。\n\n在实验阶段，我们将该方法应用于一个包含专家评级查询的台湾法律数据集。结果表明，我们的方法在专家评估中表现出卓越的性能，并在无需预定义k值的情况下保持了高精度，从而证明了其在法律文本检索任务中提高了准确性和可解释性。我们的框架易于实现，并且可以轻松集成到现有的RAG管道中，使其成为在有限资源下实际应用的实用解决方案。"
    },
    {
        "title": "Hierarchical Multi-Positive Contrastive Learning for Patent Image\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.13496v1",
        "pub_date": "2025-06-16",
        "summary": "Patent images are technical drawings that convey information about a patent's innovation. Patent image retrieval systems aim to search in vast collections and retrieve the most relevant images. Despite recent advances in information retrieval, patent images still pose significant challenges due to their technical intricacies and complex semantic information, requiring efficient fine-tuning for domain adaptation. Current methods neglect patents' hierarchical relationships, such as those defined by the Locarno International Classification (LIC) system, which groups broad categories (e.g., \"furnishing\") into subclasses (e.g., \"seats\" and \"beds\") and further into specific patent designs. In this work, we introduce a hierarchical multi-positive contrastive loss that leverages the LIC's taxonomy to induce such relations in the retrieval process. Our approach assigns multiple positive pairs to each patent image within a batch, with varying similarity scores based on the hierarchical taxonomy. Our experimental analysis with various vision and multimodal models on the DeepPatent2 dataset shows that the proposed method enhances the retrieval results. Notably, our method is effective with low-parameter models, which require fewer computational resources and can be deployed on environments with limited hardware.",
        "translated": "专利图像是技术图纸，用于传达专利创新的信息。专利图像检索系统旨在海量图像集合中搜索并检索出最相关的图像。尽管信息检索领域取得了最新进展，但专利图像因其技术复杂性和复杂的语义信息，仍然带来了巨大挑战，需要高效的领域适应性微调。\n\n现有方法忽略了专利的层级关系，例如由洛迦诺国际分类（LIC）系统定义的层级关系，该系统将大类（例如“家具”）分为子类（例如“座椅”和“床”），并进一步细分为具体的专利设计。在这项工作中，我们引入了一种层级多正例对比损失（hierarchical multi-positive contrastive loss），它利用LIC的分类体系在检索过程中引入此类关系。我们的方法在一个批次内为每个专利图像分配多个正例对，并根据层级分类体系赋予不同的相似度分数。\n\n我们在DeepPatent2数据集上使用各种视觉和多模态模型进行的实验分析表明，所提出的方法显著提升了检索结果。值得注意的是，我们的方法对低参数模型同样有效，这使得它所需的计算资源更少，并能够部署在硬件资源有限的环境中。"
    },
    {
        "title": "Beyond One-Size-Fits-All: A Study of Neural and Behavioural Variability\n  Across Different Recommendation Categories",
        "url": "http://arxiv.org/abs/2506.13409v1",
        "pub_date": "2025-06-16",
        "summary": "Traditionally, Recommender Systems (RS) have primarily measured performance based on the accuracy and relevance of their recommendations. However, this algorithmic-centric approach overlooks how different types of recommendations impact user engagement and shape the overall quality of experience. In this paper, we shift the focus to the user and address for the first time the challenge of decoding the neural and behavioural variability across distinct recommendation categories, considering more than just relevance. Specifically, we conducted a controlled study using a comprehensive e-commerce dataset containing various recommendation types, and collected Electroencephalography and behavioural data. We analysed both neural and behavioural responses to recommendations that were categorised as Exact, Substitute, Complement, or Irrelevant products within search query results. Our findings offer novel insights into user preferences and decision-making processes, revealing meaningful relationships between behavioural and neural patterns for each category, but also indicate inter-subject variability.",
        "translated": "传统上，推荐系统（RS）主要根据推荐的准确性和相关性来衡量其性能。然而，这种以算法为中心的方法忽视了不同类型的推荐如何影响用户参与度并塑造整体用户体验质量。在本文中，我们将重点转向用户，并首次解决了在考虑相关性之外，解码不同推荐类别中神经和行为变异性的挑战。具体而言，我们利用一个包含各种推荐类型的综合电商数据集，进行了一项对照研究，并收集了脑电图（Electroencephalography）和行为数据。我们分析了在搜索查询结果中被归类为精确（Exact）、替代（Substitute）、互补（Complement）或不相关（Irrelevant）产品的推荐所引起的神经和行为反应。我们的研究结果为用户偏好和决策过程提供了新颖的见解，揭示了每种类别中行为和神经模式之间有意义的关系，但也表明了主体间的变异性。"
    },
    {
        "title": "Decompositional Reasoning for Graph Retrieval with Large Language Models",
        "url": "http://arxiv.org/abs/2506.13380v1",
        "pub_date": "2025-06-16",
        "summary": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with multi-hop reasoning and factual consistency, limiting their effectiveness on knowledge-intensive tasks like complex question answering (QA). Linking Knowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally lack the ability to reason efficiently over graph-structured information. To tackle this problem, we propose a novel retrieval approach that integrates textual knowledge graphs into the LLM reasoning process via query decomposition. Our method decomposes complex questions into sub-questions, retrieves relevant textual subgraphs, and composes a question-specific knowledge graph to guide answer generation. For that, we use a weighted similarity function that focuses on both the complex question and the generated subquestions to extract a relevant subgraph, which allows efficient and precise retrieval for complex questions and improves the performance of LLMs on multi-hop QA tasks. This structured reasoning pipeline enhances factual grounding and interpretability while leveraging the generative strengths of LLMs. We evaluate our method on standard multi-hop QA benchmarks and show that it achieves comparable or superior performance to competitive existing methods, using smaller models and fewer LLM calls.",
        "translated": "**大型语言模型（LLMs）** 在众多自然语言处理（NLP）任务中表现出色，但在**多跳推理**和**事实一致性**方面存在困难，从而限制了它们在复杂问答（QA）等**知识密集型任务**上的有效性。将**知识图谱（KG）** 与LLMs结合已展现出有前景的结果，但LLMs通常缺乏对图结构信息进行高效推理的能力。\n\n为解决此问题，我们提出了一种新颖的**检索方法**，通过**查询分解**将文本知识图谱整合到LLM的推理过程中。我们的方法将复杂问题分解为**子问题**，检索相关的**文本子图**，并构建一个与问题相关的知识图谱以指导**答案生成**。为此，我们使用一个**加权相似性函数**，该函数同时关注复杂问题和生成的子问题，以提取相关子图，从而实现了复杂问题的高效精确检索，并提高了LLMs在多跳问答（QA）任务上的性能。这种**结构化推理流程**增强了**事实基础**和**可解释性**，同时充分利用了LLMs的生成能力。我们在标准多跳问答（QA）基准上评估了所提出的方法，结果表明，与现有有竞争力的SOTA方法相比，该方法实现了可比或更优的性能，且使用的模型更小，LLM调用次数更少。"
    },
    {
        "title": "Gated Rotary-Enhanced Linear Attention for Long-term Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.13315v1",
        "pub_date": "2025-06-16",
        "summary": "In Sequential Recommendation Systems (SRSs), Transformer models show remarkable performance but face computation cost challenges when modeling long-term user behavior sequences due to the quadratic complexity of the dot-product attention mechanism. By approximating the dot-product attention, linear attention provides an efficient option with linear complexity. However, existing linear attention methods face two limitations: 1) they often use learnable position encodings, which incur extra computational costs in long-term sequence scenarios, and 2) they may not consider the user's fine-grained local preferences and confuse these with the actual change of long-term interests. To remedy these drawbacks, we propose a long-term sequential Recommendation model with Gated Rotary Enhanced Linear Attention (RecGRELA). Specifically, we first propose a Rotary-Enhanced Linear Attention (RELA) module to model long-range dependency within the user's historical information using rotary position encodings. We then introduce a local short operation to incorporate local preferences and demonstrate the theoretical insight. We further introduce a SiLU-based Gated mechanism for RELA (GRELA) to help the model determine whether a user's behavior indicates local interest or a genuine shift in long-term preferences. Experimental results on four public datasets demonstrate that our RecGRELA achieves state-of-the-art performance compared to existing SRSs while maintaining low memory overhead.",
        "translated": "在序列推荐系统（SRSs）中，Transformer模型展现出卓越性能，但在建模长期用户行为序列时面临计算成本挑战，这源于点积注意力机制的二次复杂度。通过近似点积注意力，线性注意力提供了一种具有线性复杂度的高效选择。然而，现有的线性注意力方法面临两个局限性：1) 它们通常使用可学习的位置编码，这在长期序列场景中会带来额外的计算成本；2) 它们可能未考虑用户的细粒度局部偏好，并将其与长期兴趣的实际变化混淆。\n\n为了弥补这些不足，我们提出了一种带有门控旋转增强线性注意力的长期序列推荐模型（RecGRELA）。具体而言，我们首先提出了一个旋转增强线性注意力（RELA）模块，该模块使用旋转位置编码来建模用户历史信息中的长距离依赖关系。接着，我们引入了一个局部短操作来纳入局部偏好，并阐明了其理论洞察。此外，我们还为RELA引入了一个基于SiLU的门控机制（GRELA），以帮助模型判断用户的行为是表明局部兴趣还是长期偏好的真正转变。在四个公共数据集上的实验结果表明，与现有SRSs相比，我们的RecGRELA实现了最先进的性能，同时保持了低内存开销。"
    },
    {
        "title": "Vector Ontologies as an LLM world view extraction method",
        "url": "http://arxiv.org/abs/2506.13252v1",
        "pub_date": "2025-06-16",
        "summary": "Large Language Models (LLMs) possess intricate internal representations of the world, yet these latent structures are notoriously difficult to interpret or repurpose beyond the original prediction task. Building on our earlier work (Rothenfusser, 2025), which introduced the concept of vector ontologies as a framework for translating high-dimensional neural representations into interpretable geometric structures, this paper provides the first empirical validation of that approach. A vector ontology defines a domain-specific vector space spanned by ontologically meaningful dimensions, allowing geometric analysis of concepts and relationships within a domain. We construct an 8-dimensional vector ontology of musical genres based on Spotify audio features and test whether an LLM's internal world model of music can be consistently and accurately projected into this space. Using GPT-4o-mini, we extract genre representations through multiple natural language prompts and analyze the consistency of these projections across linguistic variations and their alignment with ground-truth data. Our results show (1) high spatial consistency of genre projections across 47 query formulations, (2) strong alignment between LLM-inferred genre locations and real-world audio feature distributions, and (3) evidence of a direct relationship between prompt phrasing and spatial shifts in the LLM's inferred vector ontology. These findings demonstrate that LLMs internalize structured, repurposable knowledge and that vector ontologies offer a promising method for extracting and analyzing this knowledge in a transparent and verifiable way.",
        "translated": "大语言模型（LLM）拥有复杂精密的内部世界表征，然而，这些潜在结构在原始预测任务之外却难以解释或复用。本文基于我们之前的研究（Rothenfusser, 2025），该研究引入了向量本体论（vector ontologies）的概念，将其作为一种将高维神经表征转换为可解释几何结构的框架。在此基础上，本文首次提供了对该方法的实证验证。向量本体论定义了一个领域特定的向量空间，该空间由本体论意义上的维度所张成，从而能够对领域内的概念和关系进行几何分析。\n\n我们基于Spotify音频特征构建了一个8维的音乐流派向量本体论，并测试了大语言模型的内部音乐世界模型是否能被一致且准确地投影到这个空间中。我们使用GPT-4o-mini，通过多个自然语言提示提取流派表征，并分析这些投影在语言变体之间的一致性及其与真实数据的对齐情况。我们的结果表明：（1）流派投影在47种查询表述中表现出高度的空间一致性；（2）大语言模型推断的流派位置与真实世界的音频特征分布之间存在很强的对齐；以及（3）提示措辞与大语言模型推断的向量本体论中空间偏移之间存在直接关系的证据。这些发现表明，大语言模型内化了结构化、可复用的知识，并且向量本体论为以透明和可验证的方式提取和分析这些知识提供了一种有前景的方法。"
    },
    {
        "title": "PB$^2$: Preference Space Exploration via Population-Based Methods in\n  Preference-Based Reinforcement Learning",
        "url": "http://arxiv.org/abs/2506.13741v1",
        "pub_date": "2025-06-16",
        "summary": "Preference-based reinforcement learning (PbRL) has emerged as a promising approach for learning behaviors from human feedback without predefined reward functions. However, current PbRL methods face a critical challenge in effectively exploring the preference space, often converging prematurely to suboptimal policies that satisfy only a narrow subset of human preferences. In this work, we identify and address this preference exploration problem through population-based methods. We demonstrate that maintaining a diverse population of agents enables more comprehensive exploration of the preference landscape compared to single-agent approaches. Crucially, this diversity improves reward model learning by generating preference queries with clearly distinguishable behaviors, a key factor in real-world scenarios where humans must easily differentiate between options to provide meaningful feedback. Our experiments reveal that current methods may fail by getting stuck in local optima, requiring excessive feedback, or degrading significantly when human evaluators make errors on similar trajectories, a realistic scenario often overlooked by methods relying on perfect oracle teachers. Our population-based approach demonstrates robust performance when teachers mislabel similar trajectory segments and shows significantly enhanced preference exploration capabilities,particularly in environments with complex reward landscapes.",
        "translated": "Preference-based reinforcement learning (PbRL) has emerged as a promising approach for learning behaviors from human feedback without predefined reward functions. However, current PbRL methods face a critical challenge in effectively exploring the preference space, often converging prematurely to suboptimal policies that satisfy only a narrow subset of human preferences. In this work, we identify and address this preference exploration problem through population-based methods. We demonstrate that maintaining a diverse population of agents enables more comprehensive exploration of the preference landscape compared to single-agent approaches. Crucially, this diversity improves reward model learning by generating preference queries with clearly distinguishable behaviors, a key factor in real-world scenarios where humans must easily differentiate between options to provide meaningful feedback. Our experiments reveal that current methods may fail by getting stuck in local optima, requiring excessive feedback, or degrading significantly when human evaluators make errors on similar trajectories, a realistic scenario often overlooked by methods relying on perfect oracle teachers. Our population-based approach demonstrates robust performance when teachers mislabel similar trajectory segments and shows significantly enhanced preference exploration capabilities, particularly in environments with complex reward landscapes.\n\n---\n\n**中文翻译：**\n\n基于偏好的强化学习（PbRL）已成为一种有前景的方法，用于在没有预定义奖励函数的情况下从人类反馈中学习行为。然而，当前的PbRL方法在有效探索偏好空间方面面临一个严峻挑战，它们经常过早地收敛到次优策略，这些策略仅能满足人类偏好的狭隘子集。在本工作中，我们通过基于种群的方法识别并解决了这一偏好探索问题。我们证明，与单智能体方法相比，维护多样化的智能体种群能够实现对偏好景观更全面的探索。至关重要的是，这种多样性通过生成具有易于区分行为的偏好查询来改善奖励模型学习，这在人类必须轻松区分不同选项才能提供有意义反馈的现实场景中是一个关键因素。我们的实验表明，当前方法可能会陷入局部最优、需要过多反馈，或者当人类评估者对相似轨迹进行误判时性能显著下降。这种情况是一个现实场景，但往往被那些依赖完美预言机教师的方法所忽视。我们的基于种群的方法在教师误标记相似轨迹片段时表现出鲁棒的性能，并显示出显著增强的偏好探索能力，尤其是在奖励景观复杂的环境中。"
    },
    {
        "title": "BanditWare: A Contextual Bandit-based Framework for Hardware Prediction",
        "url": "http://arxiv.org/abs/2506.13730v1",
        "pub_date": "2025-06-16",
        "summary": "Distributed computing systems are essential for meeting the demands of modern applications, yet transitioning from single-system to distributed environments presents significant challenges. Misallocating resources in shared systems can lead to resource contention, system instability, degraded performance, priority inversion, inefficient utilization, increased latency, and environmental impact.   We present BanditWare, an online recommendation system that dynamically selects the most suitable hardware for applications using a contextual multi-armed bandit algorithm. BanditWare balances exploration and exploitation, gradually refining its hardware recommendations based on observed application performance while continuing to explore potentially better options. Unlike traditional statistical and machine learning approaches that rely heavily on large historical datasets, BanditWare operates online, learning and adapting in real-time as new workloads arrive.   We evaluated BanditWare on three workflow applications: Cycles (an agricultural science scientific workflow) BurnPro3D (a web-based platform for fire science) and a matrix multiplication application. Designed for seamless integration with the National Data Platform (NDP), BanditWare enables users of all experience levels to optimize resource allocation efficiently.",
        "translated": "分布式计算系统对于满足现代应用的需求至关重要，然而，从单系统环境向分布式环境的转变带来了巨大的挑战。在共享系统中资源分配不当可能导致资源争用、系统不稳定、性能下降、优先级反转、资源利用效率低下、延迟增加以及环境影响。\n\n我们提出了 BanditWare，一个在线推荐系统，它采用上下文多臂老虎机算法，为应用动态选择最合适的硬件。BanditWare 平衡了探索与利用，基于观察到的应用性能逐步优化其硬件推荐，同时继续探索潜在更优的选择。与严重依赖大量历史数据集的传统统计和机器学习方法不同，BanditWare 在线运行，随着新工作负载的到来，实时学习和适应。\n\n我们在三个工作流应用上评估了 BanditWare：Cycles（一个农业科学科研工作流）、BurnPro3D（一个基于网络的火灾科学平台）以及一个矩阵乘法应用。BanditWare 旨在与国家数据平台（NDP）无缝集成，使各种经验水平的用户能够高效优化资源分配。"
    },
    {
        "title": "LTRR: Learning To Rank Retrievers for LLMs",
        "url": "http://arxiv.org/abs/2506.13743v1",
        "pub_date": "2025-06-16",
        "summary": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed retriever, despite growing evidence that no single retriever performs optimally across all query types. In this paper, we explore a query routing approach that dynamically selects from a pool of retrievers based on the query, using both train-free heuristics and learned routing models. We frame routing as a learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to rank retrievers by their expected utility gain to downstream LLM performance. Our experiments, conducted on synthetic QA data with controlled query type variations, show that routing-based RAG systems can outperform the best single-retriever-based systems. Performance gains are especially pronounced in models trained with the Answer Correctness (AC) metric and with pairwise learning approaches, especially with XGBoost. We also observe improvements in generalization to out-of-distribution queries. As part of the SIGIR 2025 LiveRAG challenge, our submitted system demonstrated the practical viability of our approach, achieving competitive performance in both answer correctness and faithfulness. These findings highlight the importance of both training methodology and metric selection in query routing for RAG systems.",
        "translated": "检索增强生成 (RAG) 系统通常依赖于单个固定检索器，尽管越来越多的证据表明，没有一个检索器能够在所有查询类型上都达到最佳性能。在本文中，我们探索了一种查询路由方法，该方法利用免训练启发式方法和学习型路由模型，根据查询动态地从检索器池中选择检索器。我们将路由框定为一个排序学习 (LTR) 问题，并引入了 LTRR 框架，该框架学习根据检索器对下游大型语言模型 (LLM) 性能的预期效用增益来对其进行排序。\n\n我们的实验在具有受控查询类型变体的合成问答数据上进行，结果表明基于路由的 RAG 系统能够超越表现最佳的单检索器系统。性能提升在使用答案正确性 (AC) 指标和成对学习方法（特别是 XGBoost）训练的模型中尤为显著。我们还观察到系统对分布外 (OOD) 查询的泛化能力有所提高。作为 SIGIR 2025 LiveRAG 挑战的一部分，我们提交的系统展示了所提方法的实际可行性，在答案正确性和忠实性方面均取得了有竞争力的性能。这些发现强调了训练方法和指标选择在 RAG 系统查询路由中的重要性。"
    },
    {
        "title": "A Systematic Replicability and Comparative Study of BSARec and SASRec\n  for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2506.14692v1",
        "pub_date": "2025-06-17",
        "summary": "This study aims at comparing two sequential recommender systems: Self-Attention based Sequential Recommendation (SASRec), and Beyond Self-Attention based Sequential Recommendation (BSARec) in order to check the improvement frequency enhancement - the added element in BSARec - has on recommendations. The models in the study, have been re-implemented with a common base-structure from EasyRec, with the aim of obtaining a fair and reproducible comparison. The results obtained displayed how BSARec, by including bias terms for frequency enhancement, does indeed outperform SASRec, although the increases in performance obtained, are not as high as those presented by the authors. This work aims at offering an overview on existing methods, and most importantly at underlying the importance of implementation details for performance comparison.",
        "translated": "本研究旨在比较两种序列推荐系统：基于自注意力机制的序列推荐（SASRec）和超越自注意力机制的序列推荐（BSARec），以验证频率增强（BSARec中新增的元素）对推荐效果的提升作用。研究中的模型基于EasyRec的通用基础架构进行了重新实现，旨在实现公平且可复现的比较。所获得的结果表明，BSARec通过引入用于频率增强的偏置项，性能确实优于SASRec，尽管所实现的性能提升并未达到原作者所报告的水平。本研究旨在对现有方法进行概述，更重要的是强调实现细节对于性能比较的重要性。"
    },
    {
        "title": "Refining music sample identification with a self-supervised graph neural\n  network",
        "url": "http://arxiv.org/abs/2506.14684v1",
        "pub_date": "2025-06-17",
        "summary": "Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.   In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.   To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.",
        "translated": "自动采样识别（ASID），即检测并识别在新音乐作品中被重用的音频片段，是音频查询检索领域一项重要但充满挑战的任务。尽管一项相关任务——音频指纹识别，已在“真实世界”（例如噪声、混响）条件下准确检索音乐内容方面取得了显著进展，但ASID系统在识别经过音乐修改的采样时仍面临困难。因此，开发一个能够抵抗常见音乐制作变换（如时间拉伸、音高转换、效果处理以及底层或叠加音乐）的鲁棒系统，是一个重要的开放性挑战。\n\n在这项工作中，我们提出了一种轻量级且可扩展的编码架构，它在对比学习框架内采用了图神经网络（GNN）。与当前最先进的系统相比，我们的模型仅使用 9% 的可训练参数，同时实现了可比的性能，达到了 44.2% 的平均精度（mAP）。\n\n为了提高检索质量，我们引入了一种两阶段方法：首先进行初步的粗粒度相似性搜索以进行候选选择，随后是一个交叉注意力分类器，用于拒绝不相关的匹配并优化检索到候选的排序——这是现有模型所缺乏的关键能力。此外，由于现实世界应用中的查询通常持续时间较短，我们使用为 Sample100 数据集新增的细粒度标注，对系统进行了短查询的基准测试，这些标注也作为我们工作的一部分公开发布。"
    },
    {
        "title": "RMIT-ADM+S at the SIGIR 2025 LiveRAG Challenge",
        "url": "http://arxiv.org/abs/2506.14516v1",
        "pub_date": "2025-06-17",
        "summary": "This paper presents the RMIT--ADM+S participation in the SIGIR 2025 LiveRAG Challenge. Our Generation-Retrieval-Augmented Generation (GRAG) approach relies on generating a hypothetical answer that is used in the retrieval phase, alongside the original question. GRAG also incorporates a pointwise large language model (LLM)-based re-ranking step prior to final answer generation. We describe the system architecture and the rationale behind our design choices. In particular, a systematic evaluation using the Grid of Points (GoP) framework and N-way ANOVA enabled comparison across multiple configurations, including query variant generation, question decomposition, rank fusion strategies, and prompting techniques for answer generation. Our system achieved a Relevance score of 1.199 and a Faithfulness score of 0.477 on the private leaderboard, placing among the top four finalists in the LiveRAG 2025 Challenge.",
        "translated": "以下是论文摘要的中文翻译：\n\n本文介绍了 RMIT-ADM+S 团队在 SIGIR 2025 LiveRAG 挑战赛中的参与情况。我们提出的生成-检索增强生成（GRAG）方法，基于生成一个假设性答案，该答案与原始问题一同用于检索阶段。GRAG 还结合了一个基于逐点式大语言模型（LLM）的重排序步骤，用于最终答案生成之前。我们描述了系统架构以及我们设计选择背后的原理。特别是，一项利用网格点（GoP）框架和 N 向方差分析的系统性评估，实现了多种配置间的比较，包括查询变体生成、问题分解、排序融合策略以及答案生成的提示技术。我们的系统在私人排行榜上取得了 1.199 的相关性得分和 0.477 的忠实性得分，在 LiveRAG 2025 挑战赛中跻身前四名决赛队伍。"
    },
    {
        "title": "Vela: Scalable Embeddings with Voice Large Language Models for\n  Multimodal Retrieval",
        "url": "http://arxiv.org/abs/2506.14445v1",
        "pub_date": "2025-06-17",
        "summary": "Multimodal large language models (MLLMs) have seen substantial progress in recent years. However, their ability to represent multimodal information in the acoustic domain remains underexplored. In this work, we introduce Vela, a novel framework designed to adapt MLLMs for the generation of universal multimodal embeddings. By leveraging MLLMs with specially crafted prompts and selected in-context learning examples, Vela effectively bridges the modality gap across various modalities. We then propose a single-modality training approach, where the model is trained exclusively on text pairs. Our experiments show that Vela outperforms traditional CLAP models in standard text-audio retrieval tasks. Furthermore, we introduce new benchmarks that expose CLAP models' limitations in handling long texts and complex retrieval tasks. In contrast, Vela, by harnessing the capabilities of MLLMs, demonstrates robust performance in these scenarios. Our code will soon be available.",
        "translated": "多模态大型语言模型（MLLMs）近年来取得了显著进展。然而，它们在音频领域表示多模态信息的能力仍未得到充分探索。本工作引入了Vela，一个新颖的框架，旨在通过适应多模态大型语言模型（MLLMs）来生成通用多模态嵌入。Vela通过利用MLLM，并结合精心设计的提示（prompts）和选定的上下文学习（in-context learning）示例，有效地弥合了不同模态间的模态鸿沟。接着，我们提出了一种单模态训练方法，其中模型仅通过文本对进行训练。我们的实验表明，在标准文本-音频检索任务中，Vela的表现优于传统的CLAP模型。此外，我们还引入了新的基准，揭示了CLAP模型在处理长文本和复杂检索任务时的局限性。相比之下，Vela凭借MLLM的强大能力，在这些场景中展现出稳健的性能。我们的代码即将开源。"
    },
    {
        "title": "Similarity = Value? Consultation Value Assessment and Alignment for\n  Personalized Search",
        "url": "http://arxiv.org/abs/2506.14437v1",
        "pub_date": "2025-06-17",
        "summary": "Personalized search systems in e-commerce platforms increasingly involve user interactions with AI assistants, where users consult about products, usage scenarios, and more. Leveraging consultation to personalize search services is trending. Existing methods typically rely on semantic similarity to align historical consultations with current queries due to the absence of 'value' labels, but we observe that semantic similarity alone often fails to capture the true value of consultation for personalization. To address this, we propose a consultation value assessment framework that evaluates historical consultations from three novel perspectives: (1) Scenario Scope Value, (2) Posterior Action Value, and (3) Time Decay Value. Based on this, we introduce VAPS, a value-aware personalized search model that selectively incorporates high-value consultations through a consultation-user action interaction module and an explicit objective that aligns consultations with user actions. Experiments on both public and commercial datasets show that VAPS consistently outperforms baselines in both retrieval and ranking tasks.",
        "translated": "电商平台中的个性化搜索系统越来越多地涉及用户与AI助手的交互，用户在其中咨询产品、使用场景等。利用咨询内容来个性化搜索服务正呈现上升趋势。现有方法由于缺乏“价值”标签，通常依赖语义相似性将历史咨询与当前查询对齐。但我们观察到，仅凭语义相似性往往无法捕获咨询内容对于个性化搜索的真正价值。为解决这一问题，我们提出了一个咨询价值评估框架，从三个新颖的视角评估历史咨询内容：(1) 场景范围价值，(2) 后续行动价值，和 (3) 时间衰减价值。在此基础上，我们引入了VAPS，一个价值感知个性化搜索模型。VAPS通过一个咨询-用户行为交互模块以及一个将咨询内容与用户行为对齐的明确目标，选择性地整合高价值咨询。在公开和商业数据集上的实验表明，VAPS在检索和排序任务中均始终优于基线模型。"
    },
    {
        "title": "RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG\n  Systems for the SIGIR LiveRAG Competition",
        "url": "http://arxiv.org/abs/2506.14412v1",
        "pub_date": "2025-06-17",
        "summary": "Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR 2025 LiveRAG Challenge.",
        "translated": "检索增强生成 (RAG) 通过结合大型语言模型 (LLMs) 内部的参数化知识与外部的非参数化来源，旨在提高事实准确性并最大程度地减少幻觉。LiveRAG 2025 挑战赛旨在探索 RAG 解决方案，以最大限度地提高在 DataMorgana 问答对上的准确性，这些问答对由单跳和多跳问题组成。该挑战赛提供了 Fineweb 10BT 数据集的稀疏 OpenSearch 索引和稠密 Pinecone 索引的访问权限。它限制模型使用参数量不超过 100 亿的 LLM，并且最终答案的生成必须使用 Falcon-3-10B 模型。提交的答案由一个判官 LLM 和人类评估者共同评估。通过在挑战赛条件下探索不同的检索器组合和 RAG 解决方案，我们最终的解决方案是采用了 InstructRAG，并结合了 Pinecone 检索器和 BGE 重排序器。我们的解决方案取得了 1.13 的正确性分数和 0.55 的忠实度分数，最终在 SIGIR 2025 LiveRAG 挑战赛中位列第四。"
    },
    {
        "title": "hyperFA*IR: A hypergeometric approach to fair rankings with finite\n  candidate pool",
        "url": "http://arxiv.org/abs/2506.14349v1",
        "pub_date": "2025-06-17",
        "summary": "Ranking algorithms play a pivotal role in decision-making processes across diverse domains, from search engines to job applications. When rankings directly impact individuals, ensuring fairness becomes essential, particularly for groups that are marginalised or misrepresented in the data. Most of the existing group fairness frameworks often rely on ensuring proportional representation of protected groups. However, these approaches face limitations in accounting for the stochastic nature of ranking processes or the finite size of candidate pools. To this end, we present hyperFA*IR, a framework for assessing and enforcing fairness in rankings drawn from a finite set of candidates. It relies on a generative process based on the hypergeometric distribution, which models real-world scenarios by sampling without replacement from fixed group sizes. This approach improves fairness assessment when top-$k$ selections are large relative to the pool or when protected groups are small. We compare our approach to the widely used binomial model, which treats each draw as independent with fixed probability, and demonstrate$-$both analytically and empirically$-$that our method more accurately reproduces the statistical properties of sampling from a finite population. To operationalise this framework, we propose a Monte Carlo-based algorithm that efficiently detects unfair rankings by avoiding computationally expensive parameter tuning. Finally, we adapt our generative approach to define affirmative action policies by introducing weights into the sampling process.",
        "translated": "排名算法在从搜索引擎到求职申请等各种决策过程中发挥着举足轻重的作用。当排名直接影响个人时，确保公平性至关重要，特别是对于那些在数据中被边缘化或代表性不足的群体。大多数现有的群体公平性框架通常依赖于确保受保护群体的比例代表性。然而，这些方法在考虑排名过程的随机性或候选池的有限大小时面临局限性。\n\n为此，我们提出了 hyperFA*IR，一个用于评估和保证从有限候选集中生成的排名公平性的框架。它依赖于一个基于超几何分布的生成过程，该过程通过从固定群体规模中进行不放回抽样来模拟现实世界场景。这种方法在top-k选择相对于整个候选池较大或受保护群体规模较小时，改进了公平性评估。我们将我们的方法与广泛使用的二项式模型进行比较，后者将每次抽取视为具有固定概率的独立事件，并从理论和经验上证明，我们的方法更准确地再现了从有限总体中抽样的统计特性。为了将该框架付诸实践，我们提出了一种基于蒙特卡洛的算法，该算法通过避免计算成本高昂的参数调优来有效地检测不公平排名。最后，我们调整了我们的生成方法，通过在抽样过程中引入权重来定义平权行动政策。"
    },
    {
        "title": "A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive,\n  Transparent, and Reproducible Geo-Temporal Information Synthesis",
        "url": "http://arxiv.org/abs/2506.14345v1",
        "pub_date": "2025-06-17",
        "summary": "The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.",
        "translated": "大型语言模型（LLM）的兴起已彻底改变了信息获取方式。当前，LLM还驱动着深度研究系统，这些系统能够通过规划的迭代搜索、检索和推理，生成全面、报告式的答案。然而，当前的深度研究系统仍缺乏时空能力，而这种能力对于回答涉及地理和/或时间约束的上下文丰富问题至关重要，这些问题常见于公共卫生、环境科学或社会经济分析等领域。\n\n本文提出了我们对下一代系统的愿景，明确了将时空推理整合到深度研究流程中的重要技术、基础设施和评估挑战。我们主张，应通过开放和可复现的基础设施以及严格的评估协议，增强检索和合成过程处理时空约束的能力。我们的愿景为构建更先进、更具时空感知能力的深度研究系统指明了一条路径，有望对人工智能驱动的信息获取的未来产生深远影响。"
    },
    {
        "title": "ImpReSS: Implicit Recommender System for Support Conversations",
        "url": "http://arxiv.org/abs/2506.14231v1",
        "pub_date": "2025-06-17",
        "summary": "Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.",
        "translated": "随着大语言模型（LLM）的最新进展，基于LLM的聊天机器人通过自动化交互并提供一致、可扩展的服务，彻底改变了客户支持。虽然基于LLM的对话推荐系统（CRS）因其提升推荐质量的能力而备受关注，但针对推荐在客户支持交互中隐式集成的研究却十分有限。在本文中，我们提出了ImpReSS，一个专为客户支持对话设计的隐式推荐系统。ImpReSS与现有支持聊天机器人并行运行，在用户报告问题、聊天机器人提供解决方案的场景中发挥作用。基于客户支持对话，ImpReSS识别出推荐相关解决方案产品类别（SPC）的机会，这些类别有助于解决问题或预防问题再次发生，从而也支持业务增长。与传统CRS不同，ImpReSS完全隐式地运作，不依赖于用户购买意图的任何假设。我们对ImpReSS在支持对话中推荐有助于解决所提出问题的相关SPC的能力进行了实证评估，结果显示出可喜的成果，包括：在通用问题解决方面，MRR@1（以及recall@3）分别为0.72（0.89）；在信息安全支持方面为0.82（0.83）；在网络安全故障排除方面为0.85（0.67）。为了支持未来的研究，我们的数据和代码将应要求共享。"
    },
    {
        "title": "InsertRank: LLMs can reason over BM25 scores to Improve Listwise\n  Reranking",
        "url": "http://arxiv.org/abs/2506.14086v1",
        "pub_date": "2025-06-17",
        "summary": "Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.",
        "translated": "大语言模型（LLMs）在各类信息检索任务中展现出显著进展，尤其是在作为重排序器（reranker）方面，这得益于它们通过大规模预训练获得的强大泛化能力和知识迁移能力。与此同时，基于LLM的聊天界面的兴起提升了用户预期，促使用户提出更复杂的查询，这些查询需要通过对文档进行“推理”来检索，而非通过简单的关键词匹配或语义相似性。尽管一些近期的研究工作已利用LLM的推理能力来对这类查询进行重排序，但仍有相当大的改进空间。\n\n为此，我们引入了InsertRank，这是一种基于LLM的重排序器，它在重排序过程中利用了BM25分数等词法信号，以进一步提升检索性能。InsertRank在BRIGHT（一个涵盖12个不同领域的推理基准）和R2MED（一个涵盖8个不同任务的专用医学推理检索基准）上均展现出更高的检索有效性。我们进行了详尽的评估和多项消融研究，并证明InsertRank在包括GPT、Gemini和Deepseek模型在内的多种LLM家族中持续提升了检索有效性。此外，我们还对通过改变BM25分数尺度进行的归一化（normalization）以及通过打乱文档顺序引起的位置偏差（positional bias）进行了消融研究。使用Deepseek-R1模型，InsertRank在BRIGHT基准上获得了37.5分，在R2MED基准上获得了51.1分，超越了以往的方法。"
    },
    {
        "title": "Refining music sample identification with a self-supervised graph neural\n  network",
        "url": "http://arxiv.org/abs/2506.14684v1",
        "pub_date": "2025-06-17",
        "summary": "Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.   In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.   To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.",
        "translated": "自动样本识别（ASID）旨在检测和识别在新音乐作品中被重用的音频片段，是音频查询检索领域中一项重要但具有挑战性的任务。尽管相关任务“音频指纹”在“真实世界”（例如，有噪声、混响）条件下准确检索音乐内容方面取得了显著进展，但ASID系统在识别经过音乐修改的样本时面临困难。因此，开发一个对时间拉伸、音高偏移、效果处理以及伴奏或叠加音乐等常见音乐制作变换具有鲁棒性的系统，是一个重要的开放性挑战。\n\n在这项工作中，我们提出了一种轻量级且可扩展的编码架构，该架构在对比学习框架内采用图神经网络。与当前最先进的系统相比，我们的模型仅使用了9%的可训练参数，同时达到了可比的性能，平均精度（mAP）达到44.2%。\n\n为了提升检索质量，我们引入了一种两阶段方法：首先进行用于候选选择的初步粗粒度相似性搜索，然后是一个交叉注意力分类器，用于拒绝不相关的匹配并细化检索到的候选排名——这是以往模型所缺乏的关键能力。此外，由于实际应用中的查询通常持续时间较短，我们利用为Sample100数据集制作的新的细粒度标注，对系统在短查询场景下的性能进行了基准测试，这些标注也作为我们工作的一部分公开发布。"
    },
    {
        "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets",
        "url": "http://arxiv.org/abs/2506.14761v1",
        "pub_date": "2025-06-17",
        "summary": "Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.",
        "translated": "分词（Tokenization）为输入文本强制设定了固定的粒度，这限制了语言模型对数据的操作方式以及其未来预测的范围。字节对编码（Byte Pair Encoding, BPE）及类似方案对文本进行一次性分割，构建静态词表，这使得模型受限于该选择。我们通过引入一个自回归 U-Net 来打破了这种僵化，该网络在训练过程中学习嵌入自身的标记。该网络读取原始字节，将其汇聚成单词，然后是单词对，再到最多4个单词，从而为其提供了序列的多尺度视图。在更深的阶段，模型必须预测更远的未来——即预测接下来的几个单词而非下一个字节——因此更深的阶段侧重于更广泛的语义模式，而较早的阶段则处理细粒度细节。在仔细调整并控制预训练计算量的情况下，浅层层次结构的模型能与强大的 BPE 基线模型性能持平，而更深层次结构则展现出有前景的趋势。由于分词现在内嵌于模型中，相同的系统能够处理字符级任务，并能在低资源语言之间传递知识。"
    },
    {
        "title": "DiscRec: Disentangled Semantic-Collaborative Modeling for Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.15576v1",
        "pub_date": "2025-06-18",
        "summary": "Generative recommendation is emerging as a powerful paradigm that directly generates item predictions, moving beyond traditional matching-based approaches. However, current methods face two key challenges: token-item misalignment, where uniform token-level modeling ignores item-level granularity that is critical for collaborative signal learning, and semantic-collaborative signal entanglement, where collaborative and semantic signals exhibit distinct distributions yet are fused in a unified embedding space, leading to conflicting optimization objectives that limit the recommendation performance.   To address these issues, we propose DiscRec, a novel framework that enables Disentangled Semantic-Collaborative signal modeling with flexible fusion for generative Recommendation.First, DiscRec introduces item-level position embeddings, assigned based on indices within each semantic ID, enabling explicit modeling of item structure in input token sequences.Second, DiscRec employs a dual-branch module to disentangle the two signals at the embedding layer: a semantic branch encodes semantic signals using original token embeddings, while a collaborative branch applies localized attention restricted to tokens within the same item to effectively capture collaborative signals. A gating mechanism subsequently fuses both branches while preserving the model's ability to model sequential dependencies. Extensive experiments on four real-world datasets demonstrate that DiscRec effectively decouples these signals and consistently outperforms state-of-the-art baselines. Our codes are available on https://github.com/Ten-Mao/DiscRec.",
        "translated": "生成式推荐正成为一种强大的范式，它直接生成物品预测，超越了传统的基于匹配的方法。然而，当前方法面临两个关键挑战：一是词元-物品错位，即统一的词元级建模忽略了物品级粒度，而这对于协同信号学习至关重要；二是语义-协同信号纠缠，即协同信号和语义信号表现出不同的分布，却在一个统一的嵌入空间中融合，这导致了冲突的优化目标，从而限制了推荐性能。\n\n为解决这些问题，我们提出了DiscRec，这是一个新颖的框架，旨在实现生成式推荐中语义-协同信号的解耦建模与灵活融合。首先，DiscRec引入了物品级位置嵌入，这些嵌入根据每个语义ID内的索引进行分配，从而能够在输入词元序列中显式地建模物品结构。其次，DiscRec采用一个双分支模块，在嵌入层解耦这两种信号：一个语义分支使用原始词元嵌入来编码语义信号，而一个协同分支则对同一物品内的词元应用局部注意力，以有效捕获协同信号。一个门控机制随后融合了两个分支，同时保留了模型建模序列依赖性的能力。在四个真实世界数据集上进行的大量实验表明，DiscRec有效地解耦了这些信号，并且持续优于最先进的基线方法。我们的代码可在 https://github.com/Ten-Mao/DiscRec 获取。"
    },
    {
        "title": "Multi-Interest Recommendation: A Survey",
        "url": "http://arxiv.org/abs/2506.15284v1",
        "pub_date": "2025-06-18",
        "summary": "Existing recommendation methods often struggle to model users' multifaceted preferences due to the diversity and volatility of user behavior, as well as the inherent uncertainty and ambiguity of item attributes in practical scenarios. Multi-interest recommendation addresses this challenge by extracting multiple interest representations from users' historical interactions, enabling fine-grained preference modeling and more accurate recommendations. It has drawn broad interest in recommendation research. However, current recommendation surveys have either specialized in frontier recommendation methods or delved into specific tasks and downstream applications. In this work, we systematically review the progress, solutions, challenges, and future directions of multi-interest recommendation by answering the following three questions: (1) Why is multi-interest modeling significantly important for recommendation? (2) What aspects are focused on by multi-interest modeling in recommendation? and (3) How can multi-interest modeling be applied, along with the technical details of the representative modules? We hope that this survey establishes a fundamental framework and delivers a preliminary overview for researchers interested in this field and committed to further exploration. The implementation of multi-interest recommendation summarized in this survey is maintained at https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey.",
        "translated": "现有推荐方法在实际应用中，由于用户行为的多样性和易变性，以及物品属性固有的不确定性和模糊性，往往难以建模用户多兴趣偏好。多兴趣推荐（Multi-interest recommendation）通过从用户历史交互中提取多个兴趣表示，解决了这一挑战，从而实现细粒度偏好建模和更准确的推荐。它在推荐研究领域引起了广泛关注。然而，当前的推荐综述要么专注于前沿推荐方法，要么深入探讨特定任务和下游应用。\n\n在这项工作中，我们通过回答以下三个问题，系统地综述了多兴趣推荐的进展、解决方案、挑战和未来方向：\n(1) 多兴趣建模对推荐为何如此重要？\n(2) 推荐中的多兴趣建模关注哪些方面？\n(3) 多兴趣建模如何应用，以及代表性模块的技术细节？\n\n我们希望这篇综述能为对该领域感兴趣并致力于深入探索的研究人员建立一个基本框架，并提供初步概述。本综述所总结的多兴趣推荐相关实现维护在：https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey。"
    },
    {
        "title": "Next-User Retrieval: Enhancing Cold-Start Recommendations via Generative\n  Next-User Modeling",
        "url": "http://arxiv.org/abs/2506.15267v1",
        "pub_date": "2025-06-18",
        "summary": "The item cold-start problem is critical for online recommendation systems, as the success of this phase determines whether high-quality new items can transition to popular ones, receive essential feedback to inspire creators, and thus lead to the long-term retention of creators. However, modern recommendation systems still struggle to address item cold-start challenges due to the heavy reliance on item and historical interactions, which are non-trivial for cold-start items lacking sufficient exposure and feedback. Lookalike algorithms provide a promising solution by extending feedback for new items based on lookalike users. Traditional lookalike algorithms face such limitations: (1) failing to effectively model the lookalike users and further improve recommendations with the existing rule- or model-based methods; and (2) struggling to utilize the interaction signals and incorporate diverse features in modern recommendation systems.   Inspired by lookalike algorithms, we propose Next-User Retrieval, a novel framework for enhancing cold-start recommendations via generative next-user modeling. Specifically, we employ a transformer-based model to capture the unidirectional relationships among recently interacted users and utilize these sequences to generate the next potential user who is most likely to interact with the item. The additional item features are also integrated as prefix prompt embeddings to assist the next-user generation. The effectiveness of Next-User Retrieval is evaluated through both offline experiments and online A/B tests. Our method achieves significant improvements with increases of 0.0142% in daily active users and +0.1144% in publications in Douyin, showcasing its practical applicability and scalability.",
        "translated": "物品冷启动问题对在线推荐系统至关重要，因为这一阶段的成功决定了高质量的新物品能否发展为热门物品，能否获得关键反馈以激励创作者，从而实现创作者的长期留存。然而，现代推荐系统仍难以有效解决物品冷启动挑战，因为它们严重依赖物品和历史交互信息，而对于缺乏足够曝光和反馈的冷启动物品而言，这些信息是难以获取的。相似用户算法（Lookalike algorithms）通过基于相似用户为新物品扩展反馈，提供了一种有前景的解决方案。传统的相似用户算法面临以下局限性：(1) 无法有效建模相似用户，也无法利用现有基于规则或基于模型的方法进一步改进推荐；(2) 难以在现代推荐系统中有效利用交互信号并整合多样化特征。\n\n受相似用户算法启发，我们提出了“下一位用户检索”（Next-User Retrieval），这是一个新颖的框架，旨在通过生成式下一位用户建模来增强冷启动推荐。具体而言，我们采用基于Transformer的模型来捕捉最近交互用户之间的单向关系，并利用这些序列生成最有可能与该物品交互的下一位潜在用户。额外的物品特征也被整合为前缀提示嵌入（prefix prompt embeddings），以辅助下一位用户的生成。“下一位用户检索”的有效性通过离线实验和在线A/B测试进行了评估。我们的方法取得了显著改进，使抖音的日活跃用户（DAU）增加了0.0142%，发布量增加了0.1144%，充分展示了其在实际应用中的可行性和可扩展性。"
    },
    {
        "title": "Advancing Loss Functions in Recommender Systems: A Comparative Study\n  with a Rényi Divergence-Based Solution",
        "url": "http://arxiv.org/abs/2506.15120v1",
        "pub_date": "2025-06-18",
        "summary": "Loss functions play a pivotal role in optimizing recommendation models. Among various loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are particularly effective. Their theoretical connections and differences warrant in-depth exploration. This work conducts comprehensive analyses of these losses, yielding significant insights: 1) Common strengths -- both can be viewed as augmentations of traditional losses with Distributional Robust Optimization (DRO), enhancing robustness to distributional shifts; 2) Respective limitations -- stemming from their use of different distribution distance metrics in DRO optimization, SL exhibits high sensitivity to false negative instances, whereas CCL suffers from low data utilization. To address these limitations, this work proposes a new loss function, DrRL, which generalizes SL and CCL by leveraging R\\'enyi-divergence in DRO optimization. DrRL incorporates the advantageous structures of both SL and CCL, and can be demonstrated to effectively mitigate their limitations. Extensive experiments have been conducted to validate the superiority of DrRL on both recommendation accuracy and robustness.",
        "translated": "损失函数在优化推荐模型中扮演着关键角色。在各种损失函数中，Softmax损失 (SL) 和余弦对比损失 (CCL) 尤为有效。它们的理论联系和区别值得深入探讨。本文对这些损失函数进行了全面分析，获得了重要的见解：1) 共同优点——两者都可以被视为结合了分布鲁棒优化 (DRO) 的传统损失函数的增强，从而增强了对分布偏移的鲁棒性；2) 各自的局限性——源于它们在DRO优化中使用了不同的分布距离度量，SL对假阴性实例表现出高敏感性，而CCL则存在数据利用率低的问题。为解决这些局限性，本文提出了一种新的损失函数DrRL，它通过在DRO优化中利用Rényi散度，泛化了SL和CCL。DrRL融合了SL和CCL的优势结构，并且可以证明能够有效缓解它们的局限性。广泛的实验验证了DrRL在推荐准确性和鲁棒性两方面的优越性。"
    },
    {
        "title": "Dense SAE Latents Are Features, Not Bugs",
        "url": "http://arxiv.org/abs/2506.15679v1",
        "pub_date": "2025-06-18",
        "summary": "Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \\emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.",
        "translated": "稀疏自编码器（SAE）旨在通过施加稀疏性约束，从语言模型中提取可解释的特征。理想情况下，SAE的训练应生成既稀疏又语义有意义的隐变量。然而，许多SAE隐变量频繁激活（即是“密集”的），这引发了它们可能是训练过程中不良产物的担忧。\n\n本研究系统地调查了密集隐变量的几何、功能和起源，并表明它们不仅持久存在，而且通常反映了有意义的模型表示。我们首先证明，密集隐变量倾向于形成对立对，这些对立对能够重构残差流中的特定方向。并且，消融其子空间会抑制在重新训练的SAE中出现新的密集特征——这表明高密度特征是残差空间的一种内在属性。随后，我们引入了密集隐变量的分类法，识别出与位置跟踪、上下文绑定、熵调节、特定字母输出信号、词性以及主成分重构相关的类别。最后，我们分析了这些特征在不同层级间的演变，揭示了从早期层级的结构特征，到中间层级的语义特征，再到模型最后层级面向输出的信号的转变。\n\n我们的发现表明，密集隐变量在语言模型计算中扮演着功能性角色，不应被视为训练噪声而忽视。"
    },
    {
        "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards\n  Improve Preference Learning",
        "url": "http://arxiv.org/abs/2506.15651v1",
        "pub_date": "2025-06-18",
        "summary": "Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. We present AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, we employ language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. Our analysis confirms that the extracted rules exhibit good agreement with dataset preference. We find that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, our case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix, and the code is open-sourced at https://github.com/cxcscmu/AutoRule.",
        "translated": "基于规则的奖励为改进人类反馈强化学习（RLHF）提供了一种有前景的策略，但当前方法通常依赖手动规则工程。我们提出了 AutoRule，一种从偏好反馈中提取规则并将其转化为基于规则的奖励的全自动方法。AutoRule 的提取过程分为三个阶段：它利用推理模型解释用户偏好，从这些解释的推理链中识别候选规则，并将它们综合成一个统一的规则集。利用最终确定的规则集，我们采用语言模型验证器来计算每个输出满足规则的比例，并在策略优化期间将此指标作为辅助奖励，与学习到的奖励模型并行使用。与使用相同学习奖励模型但没有基于规则辅助奖励进行训练的 GRPO 基线模型相比，使用 AutoRule 训练 Llama-3-8B 模型，在 AlpacaEval2.0 上实现了长度受控胜率 28.6% 的相对提升，并在一个保留的 MT-Bench 子集上实现了第二轮性能 6.1% 的相对提升。我们的分析证实，提取的规则与数据集偏好表现出良好的一致性。我们发现，在两轮运行中，AutoRule 相比于学习到的奖励模型，展现出更低的奖励作弊（reward hacking）现象。最后，我们的案例研究表明，提取的规则捕捉到了不同数据集中所重视的独特质量。提取的规则已在附录中提供，代码已在 https://github.com/cxcscmu/AutoRule 开源。"
    },
    {
        "title": "Towards AI Search Paradigm",
        "url": "http://arxiv.org/abs/2506.17188v1",
        "pub_date": "2025-06-20",
        "summary": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.",
        "translated": "本论文引入了AI搜索范式，这是一个全面的蓝图，旨在构建能够模拟人类信息处理和决策的下一代搜索系统。该范式采用了一种由四个LLM（大型语言模型）驱动的智能体（Master、Planner、Executor和Writer）组成的模块化架构，这些智能体能够动态适应从简单事实查询到复杂多阶段推理任务的各种信息需求。这些智能体通过协调的工作流动态协作，以评估查询复杂性、将问题分解为可执行计划，并协调工具使用、任务执行和内容合成。我们系统地介绍了实现这一范式的关键方法，包括任务规划和工具集成、执行策略、对齐且鲁棒的检索增强生成，以及高效的LLM推理，涵盖了算法技术和基础设施层面的优化。通过为这些基础组件提供深入指导，本工作旨在为开发可信赖、自适应和可扩展的AI搜索系统提供参考。"
    },
    {
        "title": "PersonalAI: Towards digital twins in the graph form",
        "url": "http://arxiv.org/abs/2506.17001v1",
        "pub_date": "2025-06-20",
        "summary": "The challenge of personalizing language models, specifically the ability to account for a user's history during interactions, is of significant interest. Despite recent advancements in large language models (LLMs) and Retrieval Augmented Generation that have enhanced the factual base of LLMs, the task of retaining extensive personal information and using it to generate personalized responses remains pertinent. To address this, we propose utilizing external memory in the form of knowledge graphs, which are constructed and updated by the LLM itself. We have expanded upon ideas of AriGraph architecture and for the first time introduced a combined graph featuring both standard edges and two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and DiaASQ benchmarks indicates that this approach aids in making the process of graph construction and knowledge extraction unified and robust. Furthermore, we augmented the DiaASQ benchmark by incorporating parameters such as time into dialogues and introducing contradictory statements made by the same speaker at different times. Despite these modifications, the performance of the question-answering system remained robust, demonstrating the proposed architecture's ability to maintain and utilize temporal dependencies.",
        "translated": "个性化语言模型，特别是如何在交互过程中考虑用户历史信息的能力，备受关注。尽管大语言模型（LLMs）和检索增强生成（Retrieval Augmented Generation，RAG）的最新进展已显著提升了LLMs的事实基础，但保留大量个人信息并利用其生成个性化回复的任务仍然具有重要意义。\n\n为解决这一问题，我们提出利用外部记忆，具体形式为由LLM自身构建和更新的知识图谱。我们在AriGraph架构思想的基础上进行了扩展，并首次引入了一种结合了标准边和两种类型超边的组合图。在TriviaQA、HotpotQA和DiaASQ基准测试集上进行的实验表明，该方法有助于使图谱构建和知识提取过程更加统一和鲁棒。此外，我们通过在对话中引入时间等参数，并加入同一说话者在不同时间做出的矛盾性陈述，对DiaASQ基准测试集进行了增强。尽管进行了这些修改，问答系统的性能依然保持鲁棒，这证明了所提出的架构能够维护和利用时间依赖性。"
    },
    {
        "title": "RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed\n  Question Answering",
        "url": "http://arxiv.org/abs/2506.16988v1",
        "pub_date": "2025-06-20",
        "summary": "We present RAGentA, a multi-agent retrieval-augmented generation (RAG) framework for attributed question answering (QA). With the goal of trustworthy answer generation, RAGentA focuses on optimizing answer correctness, defined by coverage and relevance to the question and faithfulness, which measures the extent to which answers are grounded in retrieved documents. RAGentA uses a multi-agent architecture that iteratively filters retrieved documents, generates attributed answers with in-line citations, and verifies completeness through dynamic refinement. Central to the framework is a hybrid retrieval strategy that combines sparse and dense methods, improving Recall@20 by 12.5% compared to the best single retrieval model, resulting in more correct and well-supported answers. Evaluated on a synthetic QA dataset derived from the FineWeb index, RAGentA outperforms standard RAG baselines, achieving gains of 1.09% in correctness and 10.72% in faithfulness. These results demonstrate the effectiveness of the multi-agent architecture and hybrid retrieval in advancing trustworthy QA.",
        "translated": "我们提出了 RAGentA，一个用于归因问答（QA）的多智能体检索增强生成（RAG）框架。RAGentA 以生成可信答案为目标，致力于优化答案的正确性（由覆盖度和与问题的相关性定义）和忠实性（衡量答案在多大程度上基于检索到的文档）。RAGentA 采用多智能体架构，该架构迭代地过滤检索到的文档，生成带有行内引用的归因答案，并通过动态优化验证其完整性。该框架的核心是一种混合检索策略，它结合了稀疏和密集方法，与最佳单一检索模型相比，将 Recall@20 提高了 12.5%，从而生成了更正确且有充分支撑的答案。在源自 FineWeb 索引的合成问答数据集上进行评估，RAGentA 优于标准 RAG 基线，在正确性方面提升了 1.09%，在忠实性方面提升了 10.72%。这些结果证明了多智能体架构和混合检索在推动可信问答方面的有效性。"
    },
    {
        "title": "Pyramid Mixer: Multi-dimensional Multi-period Interest Modeling for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2506.16942v1",
        "pub_date": "2025-06-20",
        "summary": "Sequential recommendation, a critical task in recommendation systems, predicts the next user action based on the understanding of the user's historical behaviors. Conventional studies mainly focus on cross-behavior modeling with self-attention based methods while neglecting comprehensive user interest modeling for more dimensions. In this study, we propose a novel sequential recommendation model, Pyramid Mixer, which leverages the MLP-Mixer architecture to achieve efficient and complete modeling of user interests. Our method learns comprehensive user interests via cross-behavior and cross-feature user sequence modeling. The mixer layers are stacked in a pyramid way for cross-period user temporal interest learning. Through extensive offline and online experiments, we demonstrate the effectiveness and efficiency of our method, and we obtain a +0.106% improvement in user stay duration and a +0.0113% increase in user active days in the online A/B test. The Pyramid Mixer has been successfully deployed on the industrial platform, demonstrating its scalability and impact in real-world applications.",
        "translated": "序列推荐是推荐系统中的一项关键任务，它基于对用户历史行为的理解来预测用户的下一个行为。传统研究主要关注使用基于自注意力的方法进行跨行为建模，却忽略了从更多维度进行综合用户兴趣建模。在这项研究中，我们提出了一个新颖的序列推荐模型——Pyramid Mixer，它利用MLP-Mixer架构来实现对用户兴趣的有效且完整的建模。我们的方法通过跨行为和跨特征的用户序列建模来学习综合用户兴趣。混合器层以金字塔方式堆叠，用于学习跨周期的用户时间兴趣。通过大量的离线和在线实验，我们证明了我们方法的有效性和效率，并且在在线A/B测试中，我们获得了用户停留时长0.106%的提升以及用户活跃天数0.0113%的增长。Pyramid Mixer已成功部署到工业平台，证明了其在实际应用中的可扩展性和影响力。"
    },
    {
        "title": "Multi-Objective Recommendation in the Era of Generative AI: A Survey of\n  Recent Progress and Future Prospects",
        "url": "http://arxiv.org/abs/2506.16893v1",
        "pub_date": "2025-06-20",
        "summary": "With the recent progress in generative artificial intelligence (Generative AI), particularly in the development of large language models, recommendation systems are evolving to become more versatile. Unlike traditional techniques, generative AI not only learns patterns and representations from complex data but also enables content generation, data synthesis, and personalized experiences. This generative capability plays a crucial role in the field of recommendation systems, helping to address the issue of data sparsity and improving the overall performance of recommendation systems. Numerous studies on generative AI have already emerged in the field of recommendation systems. Meanwhile, the current requirements for recommendation systems have surpassed the single utility of accuracy, leading to a proliferation of multi-objective research that considers various goals in recommendation systems. However, to the best of our knowledge, there remains a lack of comprehensive studies on multi-objective recommendation systems based on generative AI technologies, leaving a significant gap in the literature. Therefore, we investigate the existing research on multi-objective recommendation systems involving generative AI to bridge this gap. We compile current research on multi-objective recommendation systems based on generative techniques, categorizing them by objectives. Additionally, we summarize relevant evaluation metrics and commonly used datasets, concluding with an analysis of the challenges and future directions in this domain.",
        "translated": "随着生成式人工智能（Generative AI）的最新进展，特别是在大语言模型（LLMs）的开发方面，推荐系统正在演进得更加多功能。与传统技术不同，生成式AI不仅能从复杂数据中学习模式和表征，还能实现内容生成、数据合成和个性化体验。这种生成能力在推荐系统领域发挥着关键作用，有助于解决数据稀疏性问题，并提升推荐系统的整体性能。目前，推荐系统领域已经涌现出大量关于生成式AI的研究。同时，当前对推荐系统的要求已超越了单一的准确性效用，这导致了考虑推荐系统中多种目标的多目标研究激增。然而，据我们所知，目前仍缺乏对基于生成式AI技术的多目标推荐系统的全面研究，这在现有文献中留下了一个显著的空白。因此，我们旨在调研涉及生成式AI的多目标推荐系统领域的现有研究，以弥补这一空白。我们整理了基于生成式技术的多目标推荐系统的现有研究，并根据其目标进行分类。此外，我们总结了相关的评估指标和常用数据集，最后分析了该领域的挑战和未来方向。"
    },
    {
        "title": "eSapiens: A Real-World NLP Framework for Multimodal Document\n  Understanding and Enterprise Knowledge Processing",
        "url": "http://arxiv.org/abs/2506.16768v1",
        "pub_date": "2025-06-20",
        "summary": "We introduce eSapiens, a unified question-answering system designed for enterprise settings, which bridges structured databases and unstructured textual corpora via a dual-module architecture. The system combines a Text-to-SQL planner with a hybrid Retrieval-Augmented Generation (RAG) pipeline, enabling natural language access to both relational data and free-form documents. To enhance answer faithfulness, the RAG module integrates dense and sparse retrieval, commercial reranking, and a citation verification loop that ensures grounding consistency. We evaluate eSapiens on the RAGTruth benchmark across five leading large language models (LLMs), analyzing performance across key dimensions such as completeness, hallucination, and context utilization. Results demonstrate that eSapiens outperforms a FAISS baseline in contextual relevance and generation quality, with optional strict-grounding controls for high-stakes scenarios. This work provides a deployable framework for robust, citation-aware question answering in real-world enterprise applications.",
        "translated": "本文推出eSapiens，一个专为企业环境设计的统一问答系统，它通过双模块架构桥接了结构化数据库和非结构化文本语料库。该系统整合了Text-to-SQL规划器和混合式检索增强生成（RAG）流水线，从而实现了对关系数据和自由格式文档的自然语言访问。为提升回答忠实性，RAG模块融合了稠密和稀疏检索、商用重排序以及一个引用验证循环，旨在确保接地一致性。我们使用五个领先的大型语言模型（LLMs），在RAGTruth基准上对eSapiens进行了评估，并在完整性、幻觉和上下文利用率等关键维度上分析了其性能。结果表明，eSapiens在上下文相关性和生成质量方面超越了FAISS基线，并针对高风险场景提供了可选的严格接地控制。本研究为实际企业应用中鲁棒、引用感知的问答提供了一个可部署的框架。"
    },
    {
        "title": "A Simple Contrastive Framework Of Item Tokenization For Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.16683v1",
        "pub_date": "2025-06-20",
        "summary": "Generative retrieval-based recommendation has emerged as a promising paradigm aiming at directly generating the identifiers of the target candidates. However, in large-scale recommendation systems, this approach becomes increasingly cumbersome due to the redundancy and sheer scale of the token space. To overcome these limitations, recent research has explored the use of semantic tokens as an alternative to ID tokens, which typically leveraged reconstruction-based strategies, like RQ-VAE, to quantize content embeddings and significantly reduce the embedding size. However, reconstructive quantization aims for the precise reconstruction of each item embedding independently, which conflicts with the goal of generative retrieval tasks focusing more on differentiating among items. Moreover, multi-modal side information of items, such as descriptive text and images, geographical knowledge in location-based recommendation services, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Nevertheless, effectively integrating such complementary knowledge into existing generative recommendation frameworks remains challenging. To overcome these challenges, we propose a novel unsupervised deep quantization exclusively based on contrastive learning, named SimCIT (a Simple Contrastive Item Tokenization framework). Specifically, different from existing reconstruction-based strategies, SimCIT propose to use a learnable residual quantization module to align with the signals from different modalities of the items, which combines multi-modal knowledge alignment and semantic tokenization in a mutually beneficial contrastive learning framework. Extensive experiments across public datasets and a large-scale industrial dataset from various domains demonstrate SimCIT's effectiveness in LLM-based generative recommendation.",
        "translated": "基于生成式检索的推荐作为一种有前景的范式崭露头角，旨在直接生成目标候选项目的标识符。然而，在大规模推荐系统中，由于令牌空间的冗余和庞大规模，这种方法变得越来越繁琐。为了克服这些局限性，近期研究探索了使用语义令牌作为ID令牌的替代，这些方法通常利用基于重建的策略（如RQ-VAE）来量化内容嵌入并显著减小嵌入大小。然而，重建式量化旨在独立精确重建每个项目嵌入，这与生成式检索任务更侧重于区分项目的目标相冲突。此外，项目的多模态辅助信息，例如描述性文本和图像，以及基于位置的推荐服务中的地理知识，已被证明通过为交互提供更丰富的上下文，能有效改善推荐效果。尽管如此，将此类互补知识有效整合到现有生成式推荐框架中仍然具有挑战性。\n\n为了克服这些挑战，我们提出了一种完全基于对比学习的新颖无监督深度量化方法，命名为SimCIT（一个简单的对比项目令牌化框架）。具体来说，SimCIT不同于现有基于重建的策略，提出使用一个可学习的残差量化模块来对齐来自项目不同模态的信号，这在一个互利共赢的对比学习框架中结合了多模态知识对齐和语义令牌化。在来自不同领域的大量公开数据集和大规模工业数据集上的广泛实验表明，SimCIT在基于大语言模型的生成式推荐中表现出卓越的有效性。"
    },
    {
        "title": "Semantic Outlier Removal with Embedding Models and LLMs",
        "url": "http://arxiv.org/abs/2506.16644v1",
        "pub_date": "2025-06-19",
        "summary": "Modern text processing pipelines demand robust methods to remove extraneous content while preserving a document's core message. Traditional approaches such as HTML boilerplate extraction or keyword filters often fail in multilingual settings and struggle with context-sensitive nuances, whereas Large Language Models (LLMs) offer improved quality at high computational cost. We introduce SORE (Semantic Outlier Removal), a cost-effective, transparent method that leverages multilingual sentence embeddings and approximate nearest-neighbor search to identify and excise unwanted text segments. By first identifying core content via metadata embedding and then flagging segments that either closely match predefined outlier groups or deviate significantly from the core, SORE achieves near-LLM extraction precision at a fraction of the cost. Experiments on HTML datasets demonstrate that SORE outperforms structural methods and yield high precision in diverse scenarios. Our system is currently deployed in production, processing millions of documents daily across multiple languages while maintaining both efficiency and accuracy. To facilitate reproducibility and further research, we release our implementation and evaluation datasets.",
        "translated": "现代文本处理流水线需要鲁棒的方法来移除冗余内容，同时保留文档的核心信息。传统的如HTML样板内容提取或关键词过滤方法在多语言环境下往往失效，并且难以处理上下文敏感的细微差别；而大语言模型（LLM）虽然能提供更高的质量，但计算成本高昂。\n\n我们引入了SORE（语义离群点剔除），这是一种经济高效且透明的方法，它利用多语言句子嵌入和近似最近邻搜索来识别并剔除不需要的文本片段。SORE首先通过元数据嵌入识别核心内容，然后标记那些与预定义离群点组高度匹配或显著偏离核心内容的片段，从而以极低的成本实现了接近大语言模型的提取精度。在HTML数据集上的实验表明，SORE优于结构化方法，并在多样化场景中展现出高精度。\n\n我们的系统目前已投入生产使用，每天处理数百万份跨多种语言的文档，同时保持了高效性和准确性。为了促进可复现性和进一步研究，我们发布了我们的实现代码和评估数据集。"
    },
    {
        "title": "Revela: Dense Retriever Learning via Language Modeling",
        "url": "http://arxiv.org/abs/2506.16552v1",
        "pub_date": "2025-06-19",
        "summary": "Dense retrievers play a vital role in accessing external and specialized knowledge to augment language models (LMs). Training dense retrievers typically requires annotated query-document pairs, which are costly and hard to obtain in specialized domains such as code-motivating growing interest in self-supervised retriever learning. Since LMs are trained to capture token-level dependencies through a self-supervised learning objective (i.e., next-token prediction), we can analogously cast retrieval as learning dependencies among chunks of tokens. This analogy naturally leads to the question: How can we adapt self-supervised learning objectives in the spirit of language modeling to train retrievers?   To answer this question, we introduce Revela, a unified and scalable training framework for self-supervised retriever learning via language modeling. Revela models semantic dependencies among documents by conditioning next-token prediction on both local and cross-document context through an in-batch attention mechanism. This attention is weighted by retriever-computed similarity scores, enabling the retriever to be optimized as part of language modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific (CoIR) benchmarks across various retriever backbones. At a comparable parameter scale, Revela outperforms the previous best method with absolute improvements of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10, respectively, underscoring its effectiveness. Performance increases with model size, highlighting both the scalability of our approach and its promise for self-supervised retriever learning.",
        "translated": "稠密检索器在使语言模型（LMs）获取外部和专业知识以增强其能力方面发挥着至关重要的作用。训练稠密检索器通常需要标注的查询-文档对，这在代码等专业领域获取成本高昂且难度大，因此促使人们对自监督检索器学习的兴趣日益增长。鉴于语言模型通过自监督学习目标（即下词元预测）训练以捕获词元级依赖，我们可以类比地将检索视为学习词元块之间的依赖关系。这种类比自然引出了一个问题：我们如何将沿用语言建模思路的自监督学习目标应用于检索器训练？\n\n为了回答这个问题，我们引入了Revela，这是一个基于语言建模的自监督检索器学习的统一且可扩展的训练框架。Revela通过批内注意力机制，以局部和跨文档上下文为条件进行下词元预测，从而建模文档间的语义依赖。这种注意力由检索器计算的相似度分数加权，使得检索器能够作为语言建模的一部分进行优化。我们使用通用领域（BEIR）和领域特定（CoIR）基准，针对多种检索器骨干网络对Revela进行了评估。在可比较的参数规模下，Revela在NDCG@10上分别以5.2%（相对18.3%）和5.6%（相对14.4%）的绝对提升，超越了此前表现最佳的方法，强调了其有效性。性能随模型大小的增加而提升，突显了我们方法的可扩展性及其在自监督检索器学习方面的广阔前景。"
    },
    {
        "title": "Towards AI Search Paradigm",
        "url": "http://arxiv.org/abs/2506.17188v1",
        "pub_date": "2025-06-20",
        "summary": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.",
        "translated": "本文中，我们提出了**人工智能搜索范式**（AI Search Paradigm），这是一个全面的蓝图，旨在构建能够模拟人类信息处理和决策的下一代搜索系统。该范式采用模块化架构，由四个LLM驱动的智能体（Master、Planner、Executor和Writer）构成，这些智能体能够动态适应从简单事实查询到复杂多阶段推理任务的各类信息需求。这些智能体通过协调的工作流动态协作，以评估查询复杂度，将问题分解为可执行计划，并编排工具使用、任务执行和内容合成。\n\n我们系统性地介绍了实现这一范式的关键方法，包括任务规划与工具集成、执行策略、对齐且鲁棒的检索增强生成，以及高效的LLM推理，涵盖了算法技术和基础设施层面的优化。通过为这些基础组件提供深入指导，本工作旨在为开发可信赖、自适应和可扩展的AI搜索系统提供参考。"
    },
    {
        "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.18902v2",
        "pub_date": "2025-06-23",
        "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.",
        "translated": "我们推出了 jina-embeddings-v4，这是一个38亿参数的多模态嵌入模型，它通过一种新颖的架构，以后期交互（late interaction）风格统一了文本和图像表示，支持单向量和多向量嵌入。该模型集成了任务特定的低秩适应（LoRA）适配器，以优化在各种检索场景中的性能，包括查询-文档检索、语义文本相似度和代码搜索。综合评估结果显示，jina-embeddings-v4在单模态和跨模态检索任务上均达到了最先进的性能，尤其擅长处理表格、图表、示意图和混合媒体格式等视觉丰富内容。为便于评估其能力，我们还推出了Jina-VDR，这是一个专门为视觉丰富图像检索设计的新颖基准。"
    },
    {
        "title": "An Audio-centric Multi-task Learning Framework for Streaming Ads\n  Targeting on Spotify",
        "url": "http://arxiv.org/abs/2506.18735v1",
        "pub_date": "2025-06-23",
        "summary": "Spotify, a large-scale multimedia platform, attracts over 675 million monthly active users who collectively consume millions of hours of music, podcasts, audiobooks, and video content. This diverse content consumption pattern introduces unique challenges for computational advertising, which must effectively integrate a variety of ad modalities, including audio, video, and display, within a single user experience. Traditional ad recommendation models, primarily designed for foregrounded experiences, often struggle to reconcile the platform's inherent audio-centrality with the demands of optimizing ad performance across multiple formats and modalities. To overcome these challenges, we introduce Cross-modal Adaptive Mixture-of-Experts (CAMoE), a novel framework for optimizing click-through rate (CTR) prediction in both audio-centric and multi-modal settings. CAMoE enhances traditional mixture-of-experts models by incorporating modality-aware task grouping, adaptive loss masking, and deep-cross networks (DCN) to capture complex feature interactions within a multi-modal ad ecosystem. Through extensive ablation studies, we demonstrate that this approach achieves near Pareto-optimal performance across audio, video, and display ad formats, significantly improving AUC-PR compared to conventional single-task and content-based multi-task learning baselines. When deployed at scale on Spotify's ad serving platform, CAMoE delivered substantial gains, yielding a 14.5% increase in CTR for audio ads, a 1.3% increase for video ads, and a 4.8% reduction in expected cost-per-click (eCPC) for audio slots.",
        "translated": "Spotify作为一个大规模多媒体平台，吸引了超过6.75亿月活跃用户，他们共同消费了数百万小时的音乐、播客、有声读物和视频内容。这种多样化的内容消费模式为计算广告带来了独特的挑战，计算广告必须在单一用户体验中有效整合包括音频、视频和展示广告在内的多种广告模态。传统的广告推荐模型主要为前台体验设计，常常难以协调该平台固有的音频中心性与优化跨多种格式和模态的广告性能的需求。\n\n为了克服这些挑战，我们引入了跨模态自适应混合专家模型（CAMoE），这是一种旨在优化以音频为中心和多模态设置中点击率（CTR）预测的新颖框架。CAMoE通过整合模态感知任务分组、自适应损失掩码和深度交叉网络（DCN），增强了传统的混合专家模型，以捕获多模态广告生态系统中的复杂特征交互。通过广泛的消融研究，我们证明该方法在音频、视频和展示广告格式上实现了接近帕累托最优的性能，与传统的单任务和基于内容的多任务学习基线相比，显著提高了AUC-PR。当在Spotify的广告服务平台大规模部署时，CAMoE带来了显著的收益，使音频广告的CTR提升了14.5%，视频广告提升了1.3%，并使音频广告位的预期每点击成本（eCPC）降低了4.8%。"
    },
    {
        "title": "Harnessing the Power of Reinforcement Learning for Language-Model-Based\n  Information Retriever via Query-Document Co-Augmentation",
        "url": "http://arxiv.org/abs/2506.18670v1",
        "pub_date": "2025-06-23",
        "summary": "Recent studies have proposed leveraging Large Language Models (LLMs) as information retrievers through query rewriting. However, for challenging corpora, we argue that enhancing queries alone is insufficient for robust semantic matching; the LLM should also have sufficient understanding of the corpus by directly handling and augmenting the documents themselves. To this end, we present an LLM-based retriever empowered to augment both user queries and corpus documents, with its policy fully explored via reinforcement learning (RL) and minimal human inductive bias. Notably, we find that simply allowing the LLM to modify documents yields little benefit unless paired with our carefully designed bidirectional RL framework, which enables the LLM to simultaneously learn and collaborate on both query and document augmentation policies. A key technical challenge in realizing such a framework lies in jointly updating both policies during training, where the rewards for the two directions depend on each other, making their entangled reward intractable. Our approach addresses this by introducing a reward sampling strategy and a specifically designed RL algorithm that enables effective training with these sampled rewards. Experimental results demonstrate that our approach significantly enhances LLM-based retrieval performance in both sparse and dense settings, particularly in difficult retrieval domains, and achieves strong cross-benchmark generalization. Our code is released at https://github.com/liujm2001/CoAugRetriever.",
        "translated": "近期研究提出通过查询重写，将大语言模型（LLM）用作信息检索器。然而，我们认为，对于复杂的语料库，仅增强查询不足以实现鲁棒的语义匹配；LLM还应通过直接处理和增强文档本身，对语料库有充分的理解。为此，我们提出了一种基于LLM的检索器，它能够同时增强用户查询和语料库文档，其策略通过强化学习（RL）充分探索，且人工归纳偏置极小。值得注意的是，我们发现，仅允许LLM修改文档效果甚微，除非与我们精心设计的双向强化学习（RL）框架相结合，该框架使LLM能够同时学习并协同制定查询和文档的增强策略。实现这一框架的关键技术挑战在于训练期间如何联合更新两种策略，因为两个方向的奖励相互依赖，使得其纠缠奖励难以处理。我们的方法通过引入一种奖励采样策略和一种专门设计的强化学习算法来解决此问题，该算法能够利用这些采样奖励进行有效训练。实验结果表明，我们的方法显著提升了基于LLM的检索性能，无论是在稀疏还是密集设置下，尤其是在困难的检索领域，并展现出强大的跨基准泛化能力。我们的代码已在 https://github.com/liujm2001/CoAugRetriever 发布。"
    },
    {
        "title": "Rethinking Click Models in Light of Carousel Interfaces: Theory-Based\n  Categorization and Design of Click Models",
        "url": "http://arxiv.org/abs/2506.18548v1",
        "pub_date": "2025-06-23",
        "summary": "Click models are a well-established for modeling user interactions with web interfaces. Previous work has mainly focused on traditional single-list web search settings; this includes existing surveys that introduced categorizations based on the first generation of probabilistic graphical model (PGM) click models that have become standard. However, these categorizations have become outdated, as their conceptualizations are unable to meaningfully compare PGM with neural network (NN) click models nor generalize to newer interfaces, such as carousel interfaces. We argue that this outdated view fails to adequately explain the fundamentals of click model designs, thus hindering the development of novel click models.   This work reconsiders what should be the fundamental concepts in click model design, grounding them - unlike previous approaches - in their mathematical properties. We propose three fundamental key-design choices that explain what statistical patterns a click model can capture, and thus indirectly, what user behaviors they can capture. Based on these choices, we create a novel click model taxonomy that allows a meaningful comparison of all existing click models; this is the first taxonomy of single-list, grid and carousel click models that includes PGMs and NNs. Finally, we show how our conceptualization provides a foundation for future click model design by an example derivation of a novel design for carousel interfaces.",
        "translated": "点击模型是用于建模用户与网页界面交互的一种成熟方法。以往的工作主要集中在传统的单列表网页搜索设置中；这包括现有综述，它们基于已成为标准的第一代概率图模型（PGM）点击模型引入了分类方法。然而，这些分类方法已经过时，因为其概念框架无法有意义地比较 PGM 与神经网络（NN）点击模型，也无法泛化到轮播界面等新型界面。我们认为，这种过时的视角未能充分解释点击模型设计的基本原理，从而阻碍了新型点击模型的发展。\n\n本研究重新审视了点击模型设计中应有的基本概念，并与以往方法不同，将它们根植于其数学特性。我们提出了三个基本的关键设计选择，它们解释了点击模型能够捕捉哪些统计模式，从而间接解释了它们能够捕捉哪些用户行为。基于这些选择，我们创建了一种新颖的点击模型分类体系，它允许对所有现有点击模型进行有意义的比较；这是第一个涵盖 PGM 和 NN 的单列表、网格和轮播点击模型分类体系。最后，我们通过一个针对轮播界面的新颖设计示例推导，展示了我们的概念化如何为未来的点击模型设计提供了基础。"
    },
    {
        "title": "When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking",
        "url": "http://arxiv.org/abs/2506.18535v1",
        "pub_date": "2025-06-23",
        "summary": "This paper investigates the counterintuitive phenomenon where fine-tuning pre-trained transformer models degrades performance on the MS MARCO passage ranking task. Through comprehensive experiments involving five model variants-including full parameter fine-tuning and parameter efficient LoRA adaptations-we demonstrate that all fine-tuning approaches underperform the base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our analysis reveals that fine-tuning disrupts the optimal embedding space structure learned during the base model's extensive pre-training on 1 billion sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations show progressive embedding space flattening, while training dynamics analysis and computational efficiency metrics further support our findings. These results challenge conventional wisdom about transfer learning effectiveness on saturated benchmarks and suggest architectural innovations may be necessary for meaningful improvements.",
        "translated": "本文研究了一个反直觉现象：预训练Transformer模型在MS MARCO段落排序任务上进行微调后，性能反而下降。通过对五种模型变体（包括全参数微调和参数高效的LoRA适应）进行全面实验，我们证明所有微调方法均劣于基准的`sentence-transformers/all-MiniLM-L6-v2`模型（MRR@10: 0.3026）。我们的分析表明，微调破坏了基准模型在对10亿句子对（其中包括910万MS MARCO样本）进行广泛预训练期间学习到的最优嵌入空间结构。UMAP可视化显示嵌入空间逐渐扁平化，同时训练动态分析和计算效率指标进一步支持了我们的发现。这些结果挑战了关于迁移学习在饱和基准测试上有效性的传统认知，并表明为了实现有意义的改进，可能需要架构创新。"
    },
    {
        "title": "PERSCEN: Learning Personalized Interaction Pattern and Scenario\n  Preference for Multi-Scenario Matching",
        "url": "http://arxiv.org/abs/2506.18382v1",
        "pub_date": "2025-06-23",
        "summary": "With the expansion of business scales and scopes on online platforms, multi-scenario matching has become a mainstream solution to reduce maintenance costs and alleviate data sparsity. The key to effective multi-scenario recommendation lies in capturing both user preferences shared across all scenarios and scenario-aware preferences specific to each scenario. However, existing methods often overlook user-specific modeling, limiting the generation of personalized user representations. To address this, we propose PERSCEN, an innovative approach that incorporates user-specific modeling into multi-scenario matching. PERSCEN constructs a user-specific feature graph based on user characteristics and employs a lightweight graph neural network to capture higher-order interaction patterns, enabling personalized extraction of preferences shared across scenarios. Additionally, we leverage vector quantization techniques to distil scenario-aware preferences from users' behavior sequence within individual scenarios, facilitating user-specific and scenario-aware preference modeling. To enhance efficient and flexible information transfer, we introduce a progressive scenario-aware gated linear unit that allows fine-grained, low-latency fusion. Extensive experiments demonstrate that PERSCEN outperforms existing methods. Further efficiency analysis confirms that PERSCEN effectively balances performance with computational cost, ensuring its practicality for real-world industrial systems.",
        "translated": "随着在线平台业务规模和范围的不断扩大，多场景匹配已成为降低维护成本、缓解数据稀疏性的主流解决方案。有效多场景推荐的关键在于同时捕获跨场景共享的用户偏好和各场景特有的场景感知偏好。然而，现有方法常常忽视用户专属建模，这限制了个性化用户表示的生成。\n\n为解决此问题，我们提出了一种名为PERSCEN的创新方法，该方法将用户专属建模融入多场景匹配中。PERSCEN基于用户特征构建用户专属特征图，并采用轻量级图神经网络来捕获高阶交互模式，从而实现跨场景共享偏好的个性化提取。此外，我们利用矢量量化技术从用户在各个场景内的行为序列中提炼场景感知偏好，以促进用户专属且场景感知的偏好建模。为增强高效灵活的信息传输，我们引入了渐进式场景感知门控线性单元，该单元支持细粒度、低延迟的融合。大量实验表明，PERSCEN的性能优于现有方法。进一步的效率分析证实，PERSCEN在性能与计算成本之间取得了有效平衡，确保了其在真实工业系统中的实用性。"
    },
    {
        "title": "Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems",
        "url": "http://arxiv.org/abs/2506.18327v1",
        "pub_date": "2025-06-23",
        "summary": "Recommendation systems play a crucial role in our daily lives by impacting user experience across various domains, including e-commerce, job advertisements, entertainment, etc. Given the vital role of such systems in our lives, practitioners must ensure they do not produce unfair and imbalanced recommendations. Previous work addressing bias in recommendations overlooked bias in certain item categories, potentially leaving some biases unaddressed. Additionally, most previous work on fair re-ranking focused on binary-sensitive attributes. In this paper, we address these issues by proposing a fairness-aware re-ranking approach that helps mitigate bias in different categories of items. This re-ranking approach leverages existing biases to correct disparities in recommendations across various demographic groups. We show how our approach can mitigate bias on multiple sensitive attributes, including gender, age, and occupation. We experimented on three real-world datasets to evaluate the effectiveness of our re-ranking scheme in mitigating bias in recommendations. Our results show how this approach helps mitigate social bias with little to no degradation in performance.",
        "translated": "推荐系统通过影响电子商务、招聘广告、娱乐等各个领域的用户体验，在我们的日常生活中扮演着至关重要的角色。鉴于此类系统在我们生活中的重要作用，从业者必须确保它们不会产生不公平和不平衡的推荐。先前旨在解决推荐中偏见的工作忽略了某些物品类别中的偏见，可能导致一些偏见未能得到妥善处理。此外，大多数先前关于公平重排序的工作都侧重于二元敏感属性。\n\n在本文中，我们通过提出一种公平感知重排序方法来解决这些问题，该方法有助于减轻不同类别物品中的偏见。这种重排序方法利用现有偏见，以纠正在不同人口统计学群体之间推荐结果的差异。我们展示了我们的方法如何能够减轻在多个敏感属性（包括性别、年龄和职业）上的偏见。我们在三个真实世界数据集上进行了实验，以评估我们的重排序方案在减轻推荐偏见方面的有效性。我们的结果表明，该方法在性能几乎没有下降的情况下有助于减轻社会偏见。"
    },
    {
        "title": "Team LA at SCIDOCA shared task 2025: Citation Discovery via\n  relation-based zero-shot retrieval",
        "url": "http://arxiv.org/abs/2506.18316v1",
        "pub_date": "2025-06-23",
        "summary": "The Citation Discovery Shared Task focuses on predicting the correct citation from a given candidate pool for a given paragraph. The main challenges stem from the length of the abstract paragraphs and the high similarity among candidate abstracts, making it difficult to determine the exact paper to cite. To address this, we develop a system that first retrieves the top-k most similar abstracts based on extracted relational features from the given paragraph. From this subset, we leverage a Large Language Model (LLM) to accurately identify the most relevant citation. We evaluate our framework on the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its effectiveness in citation prediction.",
        "translated": "**引用发现共享任务**旨在针对给定段落，从给定的候选池中预测正确的引用文献。主要挑战源于摘要段落的长度以及候选摘要之间的高度相似性，这使得确定确切的引用论文变得困难。为此，我们开发了一个系统，该系统首先根据从给定段落中提取的关系特征，检索出top-k个最相似的摘要。从这个子集中，我们利用大型语言模型（LLM）来精确识别最相关的引用文献。我们在SCIDOCA 2025组织者提供的训练数据集上评估了我们的框架，证明了其在引用预测方面的有效性。"
    },
    {
        "title": "Enhancing Document Retrieval in COVID-19 Research: Leveraging Large\n  Language Models for Hidden Relation Extraction",
        "url": "http://arxiv.org/abs/2506.18311v1",
        "pub_date": "2025-06-23",
        "summary": "In recent years, with the appearance of the COVID-19 pandemic, numerous publications relevant to this disease have been issued. Because of the massive volume of publications, an efficient retrieval system is necessary to provide researchers with useful information if an unexpected pandemic happens so suddenly, like COVID-19. In this work, we present a method to help the retrieval system, the Covrelex-SE system, to provide more high-quality search results. We exploited the power of the large language models (LLMs) to extract the hidden relationships inside the unlabeled publication that cannot be found by the current parsing tools that the system is using. Since then, help the system to have more useful information during retrieval progress.",
        "translated": "近年来，随着COVID-19大流行的爆发，大量与该疾病相关的出版物被发布。鉴于出版物数量庞大，如果像COVID-19这样突如其来的大流行再次发生，一个高效的检索系统对于为研究人员提供有用信息至关重要。在这项工作中，我们提出了一种方法，旨在助力Covrelex-SE检索系统提供更高质量的搜索结果。我们利用大型语言模型（LLMs）的强大能力，从当前系统所用解析工具无法发现的未标注出版物中提取出隐藏关系。从而帮助系统在检索过程中获取更丰富的有用信息。"
    },
    {
        "title": "LettinGo: Explore User Profile Generation for Recommendation System",
        "url": "http://arxiv.org/abs/2506.18309v1",
        "pub_date": "2025-06-23",
        "summary": "User profiling is pivotal for recommendation systems, as it transforms raw user interaction data into concise and structured representations that drive personalized recommendations. While traditional embedding-based profiles lack interpretability and adaptability, recent advances with large language models (LLMs) enable text-based profiles that are semantically richer and more transparent. However, existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors. In this paper, we introduce LettinGo, a novel framework for generating diverse and adaptive user profiles. By leveraging the expressive power of LLMs and incorporating direct feedback from downstream recommendation tasks, our approach avoids the rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ Direct Preference Optimization (DPO) to align the profile generator with task-specific performance, ensuring that the profiles remain adaptive and effective. LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance. Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness. This work enhances profile generation as a key innovation for next-generation recommendation systems.",
        "translated": "用户画像对于推荐系统至关重要，因为它将原始用户交互数据转化为简洁且结构化的表示，从而驱动个性化推荐。传统的基于嵌入（embedding）的用户画像缺乏可解释性和适应性，而大型语言模型（LLMs）的最新进展使得基于文本的用户画像语义更丰富、更具透明度。然而，现有方法往往固守固定格式，这限制了它们捕捉用户行为全部多样性的能力。\n\n在本文中，我们提出LettinGo，一个用于生成多样化且自适应用户画像的新颖框架。通过利用LLM的强大表达能力并融合来自下游推荐任务的直接反馈，我们的方法避免了监督微调（SFT）所施加的僵化约束。相反，我们采用直接偏好优化（DPO）来使画像生成器与任务特定性能对齐，从而确保画像保持自适应性和有效性。LettinGo分为三个阶段运作：(1) 通过多个LLM探索多样化的用户画像；(2) 基于画像在推荐系统中的影响来评估其质量；(3) 通过源自任务性能的成对偏好数据来对齐画像生成。实验结果表明，我们的框架显著提升了推荐准确性、灵活性和上下文感知能力。本工作将用户画像生成提升为下一代推荐系统的一项关键创新。"
    },
    {
        "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.18902v2",
        "pub_date": "2025-06-23",
        "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.",
        "translated": "我们推出了jina-embeddings-v4，这是一个38亿参数的多模态嵌入模型。它通过一种支持晚期交互模式下单向量和多向量嵌入的新颖架构，统一了文本和图像的表示。该模型集成了任务专用的低秩适应（LoRA）适配器，以优化在各种检索场景中的性能，包括查询-文档检索、语义文本相似性以及代码搜索。全面评估表明，jina-embeddings-v4在单模态和跨模态检索任务上均实现了最先进的性能，尤其擅长处理表格、图表、示意图和混合媒体格式等视觉丰富的内容。为了促进对此能力的评估，我们还推出了Jina-VDR，这是一个专门为视觉丰富的图像检索设计的新颖基准。"
    },
    {
        "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual\n  Retrieval",
        "url": "http://arxiv.org/abs/2506.18902v2",
        "pub_date": "2025-06-23",
        "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.",
        "translated": "我们推出了 jina-embeddings-v4，这是一个拥有38亿参数的多模态嵌入模型。该模型通过一种新颖的架构，统一了文本和图像表示，并支持后期交互（late interaction）风格下的单向量和多向量嵌入。模型集成了任务专用的低秩适应（LoRA）适配器，旨在优化其在各种检索场景中的性能，包括查询-文档检索、语义文本相似度以及代码搜索。\n\n全面评估表明，jina-embeddings-v4 在单模态和跨模态检索任务上均实现了最先进的性能，尤其擅长处理视觉丰富的内容，例如表格、图表、示意图和混合媒体格式。为了便于评估这一能力，我们还推出了 Jina-VDR，这是一个专门为视觉丰富的图像检索设计的新颖基准。"
    },
    {
        "title": "Unidentified and Confounded? Understanding Two-Tower Models for Unbiased\n  Learning to Rank",
        "url": "http://arxiv.org/abs/2506.20501v1",
        "pub_date": "2025-06-25",
        "summary": "Additive two-tower models are popular learning-to-rank methods for handling biased user feedback in industry settings. Recent studies, however, report a concerning phenomenon: training two-tower models on clicks collected by well-performing production systems leads to decreased ranking performance. This paper investigates two recent explanations for this observation: confounding effects from logging policies and model identifiability issues. We theoretically analyze the identifiability conditions of two-tower models, showing that either document swaps across positions or overlapping feature distributions are required to recover model parameters from clicks. We also investigate the effect of logging policies on two-tower models, finding that they introduce no bias when models perfectly capture user behavior. However, logging policies can amplify biases when models imperfectly capture user behavior, particularly when prediction errors correlate with document placement across positions. We propose a sample weighting technique to mitigate these effects and provide actionable insights for researchers and practitioners using two-tower models.",
        "translated": "叠加式双塔模型（Additive two-tower models）是工业环境中用于处理有偏用户反馈的流行学习排序方法。然而，近期研究报告了一个令人担忧的现象：使用由性能良好的生产系统收集的点击数据来训练双塔模型，反而会导致排序性能下降。本文研究了对这一观察结果的两种近期解释：记录策略（logging policies）带来的混淆效应以及模型可识别性问题。\n\n我们从理论上分析了双塔模型的可识别性条件，表明需要文档在位置间的互换（document swaps across positions）或重叠的特征分布（overlapping feature distributions）才能从点击数据中恢复模型参数。我们还研究了记录策略对双塔模型的影响，发现当模型完美捕捉用户行为时，它们不会引入偏差。然而，当模型不完美捕捉用户行为时，记录策略会放大偏差，特别是当预测误差与文档在位置间的放置存在相关性时。我们提出了一种样本加权技术以减轻这些影响，并为使用双塔模型的研究人员和从业者提供可操作的见解。"
    },
    {
        "title": "Knowledge-Aware Diverse Reranking for Cross-Source Question Answering",
        "url": "http://arxiv.org/abs/2506.20476v1",
        "pub_date": "2025-06-25",
        "summary": "This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG competition. The competition's evaluation set, automatically generated by DataMorgana from internet corpora, encompassed a wide range of target topics, question types, question formulations, audience types, and knowledge organization methods. It offered a fair evaluation of retrieving question-relevant supporting documents from a 15M documents subset of the FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline achieved first place in the competition.",
        "translated": "本文介绍了Marikarp团队针对SIGIR 2025 LiveRAG竞赛提出的解决方案。该竞赛的评估数据集由DataMorgana从互联网语料库自动生成，涵盖了广泛的目标主题、问题类型、问题表述方式、受众类型和知识组织方法。它旨在公平地评估从FineWeb语料库的1500万文档子集中检索与问题相关的支持文档的能力。我们提出的知识感知多样化重排RAG管线在该竞赛中获得了第一名。"
    },
    {
        "title": "Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce\n  Search",
        "url": "http://arxiv.org/abs/2506.20330v1",
        "pub_date": "2025-06-25",
        "summary": "Semantic retrieval, which retrieves semantically matched items given a textual query, has been an essential component to enhance system effectiveness in e-commerce search. In this paper, we study the multimodal retrieval problem, where the visual information (e.g, image) of item is leveraged as supplementary of textual information to enrich item representation and further improve retrieval performance. Though learning from cross-modality data has been studied extensively in tasks such as visual question answering or media summarization, multimodal retrieval remains a non-trivial and unsolved problem especially in the asymmetric scenario where the query is unimodal while the item is multimodal. In this paper, we propose a novel model named SMAR, which stands for Semantic-enhanced Modality-Asymmetric Retrieval, to tackle the problem of modality fusion and alignment in this kind of asymmetric scenario. Extensive experimental results on an industrial dataset show that the proposed model outperforms baseline models significantly in retrieval accuracy. We have open sourced our industrial dataset for the sake of reproducibility and future research works.",
        "translated": "语义检索，即基于文本查询来检索语义匹配的物品，一直是提升电商搜索系统有效性的关键组件。本文研究了多模态检索问题，其中物品的视觉信息（例如图像）被用作文本信息的补充，以丰富物品表示并进一步提升检索性能。尽管跨模态数据学习已在视觉问答或媒体摘要等任务中得到了广泛研究，但多模态检索仍然是一个非平凡且尚未解决的问题，尤其是在查询是单模态而物品是多模态的非对称场景中。本文提出了一种名为 SMAR（意为“语义增强型模态非对称检索”）的新模型，旨在解决此类非对称场景中的模态融合与对齐问题。在一个工业数据集上进行的大量实验结果表明，所提出的模型在检索精度方面显著优于基线模型。为了可复现性和未来的研究工作，我们已开源了我们的工业数据集。"
    },
    {
        "title": "A Literature Review on Simulation in Conversational Recommender Systems",
        "url": "http://arxiv.org/abs/2506.20291v1",
        "pub_date": "2025-06-25",
        "summary": "Conversational Recommender Systems (CRSs) have garnered attention as a novel approach to delivering personalized recommendations through multi-turn dialogues. This review developed a taxonomy framework to systematically categorize relevant publications into four groups: dataset construction, algorithm design, system evaluation, and empirical studies, providing a comprehensive analysis of simulation methods in CRSs research. Our analysis reveals that simulation methods play a key role in tackling CRSs' main challenges. For example, LLM-based simulation methods have been used to create conversational recommendation data, enhance CRSs algorithms, and evaluate CRSs. Despite several challenges, such as dataset bias, the limited output flexibility of LLM-based simulations, and the gap between text semantic space and behavioral semantics, persist due to the complexity in Human-Computer Interaction (HCI) of CRSs, simulation methods hold significant potential for advancing CRS research. This review offers a thorough summary of the current research landscape in this domain and identifies promising directions for future inquiry.",
        "translated": "对话推荐系统（CRS）作为一种通过多轮对话提供个性化推荐的新颖方法，受到了广泛关注。本综述构建了一个分类框架，将相关文献系统地分为数据集构建、算法设计、系统评估和实证研究四个类别，并对CRS研究中的模拟方法进行了全面分析。我们的分析表明，模拟方法在解决CRS的主要挑战中发挥着关键作用。例如，基于大型语言模型（LLM）的模拟方法已被用于创建对话推荐数据、增强CRS算法以及评估CRS。尽管存在数据集偏差、基于LLM的模拟输出灵活性不足以及文本语义空间与行为语义之间的鸿沟等挑战（这些挑战因CRS中人机交互（HCI）的复杂性而持续存在），但模拟方法在推进CRS研究方面仍具有巨大潜力。本综述对该领域当前的研究现状进行了全面总结，并指明了未来研究的几个有前景方向。"
    },
    {
        "title": "Irec: A Metacognitive Scaffolding for Self-Regulated Learning through\n  Just-in-Time Insight Recall: A Conceptual Framework and System Prototype",
        "url": "http://arxiv.org/abs/2506.20156v1",
        "pub_date": "2025-06-25",
        "summary": "The core challenge in learning has shifted from knowledge acquisition to effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting on one's learning. Existing digital tools, however, inadequately support metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized review, overlooking the role of context, while Personal Knowledge Management (PKM) tools require high manual maintenance.   To address these challenges, this paper introduces \"Insight Recall,\" a novel paradigm that conceptualizes the context-triggered retrieval of personal past insights as a metacognitive scaffold to promote SRL. We formalize this paradigm using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses a dynamic knowledge graph of the user's learning history. When a user faces a new problem, a hybrid retrieval engine recalls relevant personal \"insights.\" Subsequently, a large language model (LLM) performs a deep similarity assessment to filter and present the most relevant scaffold in a just-in-time manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline for LLM-based knowledge graph construction. We also propose an optional \"Guided Inquiry\" module, where users can engage in a Socratic dialogue with an expert LLM, using the current problem and recalled insights as context. The contribution of this paper is a solid theoretical framework and a usable system platform for designing next-generation intelligent learning systems that enhance metacognition and self-regulation.",
        "translated": "学习的核心挑战已从知识获取转向有效的自我调节学习（SRL），即对自身学习进行规划、监控和反思。然而，现有的数字工具未能充分支持元认知反思。间隔重复系统（SRS）采用脱离语境的复习方式，忽略了语境的作用，而个人知识管理（PKM）工具则需要大量手动维护。\n\n为应对这些挑战，本文引入了“洞察召回”（Insight Recall）这一新颖范式，该范式将语境触发的个人过往洞察检索概念化为促进SRL的元认知支架。我们使用即时自适应干预（JITAI）框架对该范式进行了形式化，并实现了一个名为Irec的原型系统以证明其可行性。Irec的核心是利用用户学习历史的动态知识图谱。当用户面临新问题时，一个混合检索引擎会召回相关的个人“洞察”。随后，一个大型语言模型（LLM）会进行深度相似度评估，以即时地筛选并呈现最相关的支架。为减少认知负荷，Irec采用了基于LLM的知识图谱构建人机协作（human-in-the-loop）流程。我们还提出了一个可选的“引导式探究”（Guided Inquiry）模块，用户可以在其中以当前问题和召回的洞察作为语境，与一个专家LLM进行苏格拉底式对话。本文的贡献在于提供了一个扎实的理论框架和一个可用的系统平台，用于设计旨在增强元认知和自我调节能力的下一代智能学习系统。"
    },
    {
        "title": "Multimodal Information Retrieval for Open World with Edit Distance Weak\n  Supervision",
        "url": "http://arxiv.org/abs/2506.20070v1",
        "pub_date": "2025-06-25",
        "summary": "Existing multi-media retrieval models either rely on creating a common subspace with modality-specific representation models or require schema mapping among modalities to measure similarities among multi-media data. Our goal is to avoid the annotation overhead incurred from considering retrieval as a supervised classification task and re-use the pretrained encoders in large language models and vision tasks. We propose \"FemmIR\", a framework to retrieve multimodal results relevant to information needs expressed with multimodal queries by example without any similarity label. Such identification is necessary for real-world applications where data annotations are scarce and satisfactory performance is required without fine-tuning with a common framework across applications. We curate a new dataset called MuQNOL for benchmarking progress on this task. Our technique is based on weak supervision introduced through edit distance between samples: graph edit distance can be modified to consider the cost of replacing a data sample in terms of its properties, and relevance can be measured through the implicit signal from the amount of edit cost among the objects. Unlike metric learning or encoding networks, FemmIR re-uses the high-level properties and maintains the property value and relationship constraints with a multi-level interaction score between data samples and the query example provided by the user. We empirically evaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs comparably to similar retrieval systems in delivering on-demand retrieval results with exact and approximate similarities while using the existing property identifiers in the system.",
        "translated": "现有多媒体检索模型要么依赖于通过模态特定表示模型创建一个公共子空间，要么需要进行模态间模式映射来衡量多媒体数据间的相似性。我们的目标是避免将检索视为监督分类任务所产生的标注开销，并重用大型语言模型和视觉任务中预训练好的编码器。我们提出了“FemmIR”，这是一个无需任何相似性标签，即可通过示例（by example）检索与多模态查询表达的信息需求相关的多模态结果的框架。这种识别能力对于数据标注稀缺且需要在不跨应用通用框架下进行微调的情况下获得满意性能的实际应用至关重要。我们整理了一个名为MuQNOL的新数据集，用于衡量该任务的进展。\n\n我们的技术基于通过样本间编辑距离引入的弱监督：图编辑距离可以被修改，以考虑根据数据样本属性替换其所产生的成本，并且相关性可以通过对象间编辑成本量化的隐式信号来衡量。不同于度量学习或编码网络，FemmIR重用高层属性，并通过数据样本与用户提供的查询示例之间的多级交互分数，来维持属性值和关系约束。我们使用MuQNOL数据集，在一个走失人员用例上对FemmIR进行了实证评估。FemmIR在利用系统中现有属性标识符的情况下，在提供具备精确和近似相似性的按需检索结果方面，表现出与类似检索系统相当的性能。"
    },
    {
        "title": "Controlled Retrieval-augmented Context Evaluation for Long-form RAG",
        "url": "http://arxiv.org/abs/2506.20051v1",
        "pub_date": "2025-06-24",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models by incorporating context retrieved from external knowledge sources. While the effectiveness of the retrieval module is typically evaluated with relevance-based ranking metrics, such metrics may be insufficient to reflect the retrieval's impact on the final RAG result, especially in long-form generation scenarios. We argue that providing a comprehensive retrieval-augmented context is important for long-form RAG tasks like report generation and propose metrics for assessing the context independent of generation. We introduce CRUX, a \\textbf{C}ontrolled \\textbf{R}etrieval-a\\textbf{U}gmented conte\\textbf{X}t evaluation framework designed to directly assess retrieval-augmented contexts. This framework uses human-written summaries to control the information scope of knowledge, enabling us to measure how well the context covers information essential for long-form generation. CRUX uses question-based evaluation to assess RAG's retrieval in a fine-grained manner. Empirical results show that CRUX offers more reflective and diagnostic evaluation. Our findings also reveal substantial room for improvement in current retrieval methods, pointing to promising directions for advancing RAG's retrieval. Our data and code are publicly available to support and advance future research on retrieval.",
        "translated": "检索增强生成（RAG）通过整合来自外部知识源的上下文来增强大型语言模型。尽管检索模块的有效性通常使用基于相关性的排序指标进行评估，但此类指标可能不足以反映检索对最终RAG结果的影响，尤其是在长篇生成场景中。我们认为，为报告生成等长篇RAG任务提供全面的检索增强上下文至关重要，并提出了独立于生成评估上下文的指标。我们介绍了CRUX，一个**受控检索增强上下文评估框架**（**C**ontrolled **R**etrieval-a**U**gmented conte**X**t evaluation framework），旨在直接评估检索增强上下文。该框架利用人工编写的摘要来控制知识的信息范围，使我们能够衡量上下文对长篇生成所需信息的覆盖程度。CRUX采用基于问题的评估来以细粒度方式评估RAG的检索。实验结果表明，CRUX提供了更具反映性和诊断性的评估。我们的发现还揭示了当前检索方法存在巨大的改进空间，为推进RAG的检索指明了有前景的方向。我们的数据和代码已公开可用，以支持和推动未来关于检索的研究。"
    },
    {
        "title": "CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2506.19993v1",
        "pub_date": "2025-06-24",
        "summary": "Recommender systems play a pivotal role in providing relevant content to users. With the rapid development of large language models (LLMs), researchers have begun utilizing LLMs to build more powerful recommender systems. However, existing approaches that focus on aligning LLMs with recommendation tasks do not fully leverage their sequential information processing capabilities, leading to suboptimal performance.   In this paper, we propose a novel system called compressed vocabulary expansion (CoVE). In CoVE, each item is assigned a unique ID within the expanded vocabulary. Our framework effectively capitalizes on sequence understanding abilities of LLMs, significantly enhancing their performance on recommendation tasks. Additionally, we compress the embedding layer, making CoVE practical for large-scale industrial applications. The effectiveness and performance of CoVE are demonstrated through comprehensive experiments on multiple recommendation datasets and comparisons with prior works. Our code can be found at https://github.com/HaochenZhang717/CoVE-official-Repo.",
        "translated": "推荐系统在为用户提供相关内容方面发挥着关键作用。随着大语言模型（LLMs）的快速发展，研究人员已开始利用LLMs来构建更强大的推荐系统。然而，现有侧重于将LLMs与推荐任务对齐的方法未能充分利用它们的序列信息处理能力，导致次优性能。\n\n在本文中，我们提出了一种名为压缩词汇扩展（CoVE）的新颖系统。在CoVE中，每个物品在扩展词汇内被赋予一个唯一的ID。我们的框架有效利用了LLMs的序列理解能力，显著增强了它们在推荐任务上的性能。此外，我们压缩了嵌入层，使CoVE在大规模工业应用中具有实用性。CoVE的有效性和性能通过在多个推荐数据集上的全面实验以及与先前工作的比较得到了验证。我们的代码已发布于 https://github.com/HaochenZhang717/CoVE-official-Repo。"
    },
    {
        "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base",
        "url": "http://arxiv.org/abs/2506.20608v1",
        "pub_date": "2025-06-25",
        "summary": "Generative AI, especially through large language models (LLMs), is transforming how technical knowledge can be accessed, reused, and extended. PETSc, a widely used numerical library for high-performance scientific computing, has accumulated a rich but fragmented knowledge base over its three decades of development, spanning source code, documentation, mailing lists, GitLab issues, Discord conversations, technical papers, and more. Much of this knowledge remains informal and inaccessible to users and new developers. To activate and utilize this knowledge base more effectively, the PETSc team has begun building an LLM-powered system that combines PETSc content with custom LLM tools -- including retrieval-augmented generation (RAG), reranking algorithms, and chatbots -- to assist users, support developers, and propose updates to formal documentation. This paper presents initial experiences designing and evaluating these tools, focusing on system architecture, using RAG and reranking for PETSc-specific information, evaluation methodologies for various LLMs and embedding models, and user interface design. Leveraging the Argonne Leadership Computing Facility resources, we analyze how LLM responses can enhance the development and use of numerical software, with an initial focus on scalable Krylov solvers. Our goal is to establish an extensible framework for knowledge-centered AI in scientific software, enabling scalable support, enriched documentation, and enhanced workflows for research and development. We conclude by outlining directions for expanding this system into a robust, evolving platform that advances software ecosystems to accelerate scientific discovery.",
        "translated": "生成式AI，特别是通过大语言模型（LLM），正在改变技术知识的获取、重用和扩展方式。PETSc是一个广泛用于高性能科学计算的数值计算库，在其三十年的发展过程中积累了丰富但碎片化的知识库，这些知识散布于源代码、文档、邮件列表、GitLab问题、Discord对话、技术论文等多个来源。其中大部分知识是非正式的，且用户和新开发者难以获取。\n\n为了更有效地激活和利用这些知识，PETSc团队已着手构建一个基于LLM的系统。该系统将PETSc内容与包括检索增强生成（RAG）、重排序算法和聊天机器人在内的定制LLM工具相结合，旨在协助用户、支持开发人员，并为正式文档的更新提供建议。本文介绍了设计和评估这些工具的初步经验，重点关注系统架构、如何将RAG和重排序应用于PETSc特定信息、针对各种LLM和嵌入模型的评估方法，以及用户界面设计。利用阿贡领导计算设施的资源，我们分析了LLM响应如何增强数值软件的开发和使用，初步侧重于可扩展的Krylov求解器。\n\n我们的目标是为科学软件中的知识中心化AI建立一个可扩展的框架，从而实现可扩展的支持、充实的文档和增强的研发工作流程。最后，我们概述了将该系统扩展为一个强大、不断演进的平台的方向，以期推动软件生态系统发展，加速科学发现。"
    },
    {
        "title": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of\n  Multi-Agent AI for Addressing Sustainable Protein Production Challenges",
        "url": "http://arxiv.org/abs/2506.20598v1",
        "pub_date": "2025-06-25",
        "summary": "The global demand for sustainable protein sources has accelerated the need for intelligent tools that can rapidly process and synthesise domain-specific scientific knowledge. In this study, we present a proof-of-concept multi-agent Artificial Intelligence (AI) framework designed to support sustainable protein production research, with an initial focus on microbial protein sources. Our Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based LLM agents: (1) a literature search agent that retrieves relevant scientific literature on microbial protein production for a specified microbial strain, and (2) an information extraction agent that processes the retrieved content to extract relevant biological and chemical information. Two parallel methodologies, fine-tuning and prompt engineering, were explored for agent optimisation. Both methods demonstrated effectiveness at improving the performance of the information extraction agent in terms of transformer-based cosine similarity scores between obtained and ideal outputs. Mean cosine similarity scores were increased by up to 25%, while universally reaching mean scores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved the mean scores to a greater extent (consistently of $\\geq 0.94$) compared to prompt engineering, although lower statistical uncertainties were observed with the latter approach. A user interface was developed and published for enabling the use of the multi-agent AI system, alongside preliminary exploration of additional chemical safety-based search capabilities",
        "translated": "全球对可持续蛋白质来源的需求加速了对智能工具的需求，这些工具能够快速处理和整合领域特定科学知识。本研究中，我们提出了一个概念验证性的多智能体人工智能（AI）框架，旨在支持可持续蛋白质生产研究，初期重点关注微生物蛋白质来源。我们面向检索增强生成（RAG）的系统由两个基于GPT的大型语言模型（LLM）智能体组成：(1) 一个文献检索智能体，负责为特定微生物菌株检索微生物蛋白质生产相关的科学文献；(2) 一个信息提取智能体，负责处理检索到的内容以提取相关的生物和化学信息。我们探索了两种并行方法——微调和提示工程——用于智能体优化。两种方法都证明能有效提升信息提取智能体的性能，衡量标准是获取输出与理想输出之间基于Transformer的余弦相似度分数。平均余弦相似度分数提升高达25%，并且普遍达到针对理想输出文本的平均分数$\\geq 0.89$。总体而言，微调比提示工程在更大程度上（始终达到$\\geq 0.94$）提升了平均分数，尽管后者方法观察到更低的统计不确定性。我们开发并发布了一个用户界面，以支持该多智能体AI系统的使用，并初步探索了额外的基于化学安全的搜索功能。"
    },
    {
        "title": "MMSearch-R1: Incentivizing LMMs to Search",
        "url": "http://arxiv.org/abs/2506.20670v1",
        "pub_date": "2025-06-25",
        "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search.",
        "translated": "大型多模态模型（LMMs）在现实世界场景中的鲁棒部署，鉴于现实世界信息的复杂性和动态性，需要访问外部知识源。现有方法，如检索增强生成（RAG）和基于提示工程的搜索智能体，依赖于固定的流程，往往导致低效或过度的搜索行为。\n\n我们提出了 MMSearch-R1，这是首个端到端强化学习框架，它使大型多模态模型（LMMs）能够在现实世界的互联网环境中执行按需、多轮搜索。我们的框架整合了图像和文本搜索工具，允许模型在基于结果的奖励和搜索惩罚的指导下，推断何时以及如何调用这些工具。为支持训练，我们通过半自动化流程收集了一个多模态搜索VQA数据集，该数据集涵盖了多样化的视觉和文本知识需求，并精心策划了一个搜索平衡子集，其中包含既需要搜索又无需搜索的样本，这对于塑造高效和按需的搜索行为至关重要。\n\n在知识密集型和信息搜索型VQA任务上进行的大量实验表明，我们的模型不仅优于相同模型规模的基于RAG的基线模型，而且与更大规模的基于RAG的模型性能相当，同时将搜索调用次数减少了30%以上。我们进一步分析了关键的实证发现，以期为促进多模态搜索领域的研究提供可行的见解。"
    },
    {
        "title": "Memento: Note-Taking for Your Future Self",
        "url": "http://arxiv.org/abs/2506.20642v1",
        "pub_date": "2025-06-25",
        "summary": "Large language models (LLMs) excel at reasoning-only tasks, but struggle when reasoning must be tightly coupled with retrieval, as in multi-hop question answering. To overcome these limitations, we introduce a prompting strategy that first decomposes a complex question into smaller steps, then dynamically constructs a database of facts using LLMs, and finally pieces these facts together to solve the question. We show how this three-stage strategy, which we call Memento, can boost the performance of existing prompting strategies across diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the performance of chain-of-thought (CoT) when all information is provided in context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1 percentage points, demonstrating its utility in agentic settings.",
        "translated": "大型语言模型 (LLMs) 擅长纯推理任务，但在推理必须与检索紧密耦合时（例如在多跳问答中）表现不佳。为了克服这些局限性，我们引入了一种提示策略，该策略首先将复杂问题分解为更小的步骤，然后利用LLM动态构建一个事实数据库，最后将这些事实整合起来以解决问题。我们表明，这种我们称之为 Memento 的三阶段策略，能够在各种不同设置下提升现有提示策略的性能。在9步 PhantomWiki 基准测试中，当所有信息都在上下文中提供时，Memento 使思维链 (CoT) 的性能翻倍。在 2WikiMultiHopQA 的开放域版本上，结合 Memento 的 CoT-RAG 比基础版 CoT-RAG 的 F1 分数提高了 20 多个百分点，并且比多跳 RAG 基线 IRCoT 的 F1 分数提高了 13 多个百分点。在具有挑战性的 MuSiQue 数据集上，Memento 使 ReAct 的 F1 分数提高了 3 多个百分点，这表明了其在代理设置中的实用性。"
    },
    {
        "title": "Maximal Matching Matters: Preventing Representation Collapse for Robust\n  Cross-Modal Retrieval",
        "url": "http://arxiv.org/abs/2506.21538v1",
        "pub_date": "2025-06-26",
        "summary": "Cross-modal image-text retrieval is challenging because of the diverse possible associations between content from different modalities. Traditional methods learn a single-vector embedding to represent semantics of each sample, but struggle to capture nuanced and diverse relationships that can exist across modalities. Set-based approaches, which represent each sample with multiple embeddings, offer a promising alternative, as they can capture richer and more diverse relationships. In this paper, we show that, despite their promise, these set-based representations continue to face issues including sparse supervision and set collapse, which limits their effectiveness. To address these challenges, we propose Maximal Pair Assignment Similarity to optimize one-to-one matching between embedding sets which preserve semantic diversity within the set. We also introduce two loss functions to further enhance the representations: Global Discriminative Loss to enhance distinction among embeddings, and Intra-Set Divergence Loss to prevent collapse within each set. Our method achieves state-of-the-art performance on MS-COCO and Flickr30k without relying on external data.",
        "translated": "跨模态图文检索具有挑战性，原因在于不同模态内容之间可能存在的多样化关联。传统方法学习一个单一向量嵌入来表示每个样本的语义，但难以捕捉跨模态之间可能存在的细致入微且多样化的关系。集合式方法通过使用多个嵌入来表示每个样本，提供了一种有前景的替代方案，因为它们能够捕获更丰富、更多样化的关系。在本文中，我们指出，尽管集合式表示具有潜力，但它们仍面临稀疏监督和集合坍缩等问题，从而限制了其有效性。为了解决这些挑战，我们提出了“最大对分配相似度 (Maximal Pair Assignment Similarity)”，旨在优化嵌入集合之间的一对一匹配，以保持集合内部的语义多样性。我们还引入了两个损失函数以进一步增强这些表示：全局判别损失 (Global Discriminative Loss, GDL) 用于增强嵌入之间的区分度，以及集合内散度损失 (Intra-Set Divergence Loss, ISDL) 用于防止每个集合内部发生坍缩。我们的方法在 MS-COCO 和 Flickr30k 数据集上实现了最先进的性能，且无需依赖外部数据。"
    },
    {
        "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond\n  English",
        "url": "http://arxiv.org/abs/2506.21445v1",
        "pub_date": "2025-06-26",
        "summary": "Recent advances in large language models have enabled natural language interfaces that translate user questions into database queries, such as Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database accessibility, most research today focuses solely on English, with limited evaluation in other languages. This paper investigates the performance of foundational LLMs on the Text2Cypher task across multiple languages. We create and release a multilingual test set by translating English questions into Spanish and Turkish while preserving the original Cypher queries, enabling fair cross-lingual comparison. We evaluate multiple foundational models using standardized prompts and metrics. Our results show a consistent performance pattern: highest on English, then Spanish, and lowest on Turkish. We attribute this to differences in training data availability and linguistic characteristics. Additionally, we explore the impact of translating task prompts into Spanish and Turkish. Results show little to no change in evaluation metrics, suggesting prompt translation has minor impact. Our findings highlight the need for more inclusive evaluation and development in multilingual query generation. Future work includes schema localization and fine-tuning across diverse languages.",
        "translated": "大语言模型的最新进展催生了将用户问题转化为数据库查询的自然语言接口，例如 Text2SQL、Text2SPARQL 和 Text2Cypher。尽管这些接口提升了数据库的可访问性，但当前大多数研究仍局限于英语，对其他语言的评估非常有限。本文探讨了基础大语言模型在多语言环境下执行 Text2Cypher 任务的性能。我们构建并发布了一个多语言测试集，其方法是将英语问题翻译成西班牙语和土耳其语，同时保留原始的 Cypher 查询，这使得公平的跨语言比较成为可能。我们使用标准化的提示和指标评估了多个基础模型。我们的结果显示出一致的性能模式：英语表现最佳，其次是西班牙语，土耳其语表现最差。我们将其归因于训练数据可用性和语言特征的差异。此外，我们还探究了将任务提示词翻译成西班牙语和土耳其语的影响。结果显示评估指标几乎没有变化，这表明提示词翻译的影响甚微。我们的研究结果凸显了在多语言查询生成方面，进行更具包容性的评估和开发的必要性。未来工作包括模式本地化和针对不同语言的微调。"
    },
    {
        "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2506.21384v1",
        "pub_date": "2025-06-26",
        "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant challenges when processing user queries that are often noisy, ambiguous, and contain multiple intents. While RAG enhances large language models (LLMs) with external knowledge, current systems typically struggle with such complex inputs, as they are often trained or evaluated on cleaner data. This paper introduces Omni-RAG, a novel framework designed to improve the robustness and effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs LLM-assisted query understanding to preprocess user inputs through three key modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs with tailored prompts to denoise queries (e.g., correcting spelling errors) and decompose multi-intent queries into structured sub-queries; (2) Intent-Aware Knowledge Retrieval, which performs retrieval for each sub-query from a corpus (i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking and Generation, where a reranker (i.e., BGE) refines document selection before a final response is generated by an LLM (i.e., Falcon-10B) using a chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG capabilities and the demands of real-world applications, such as those highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex and noisy queries.",
        "translated": "实时检索增强生成（RAG）系统在处理用户查询时面临重大挑战，这些查询通常嘈杂、模糊且包含多重意图。尽管RAG通过外部知识增强了大型语言模型（LLM），但当前系统通常难以处理此类复杂输入，因为它们通常是在更干净的数据上进行训练或评估的。本文介绍了Omni-RAG，这是一个新颖的框架，旨在提高RAG系统在实时、开放域环境中的鲁棒性和有效性。Omni-RAG采用LLM辅助的查询理解，通过三个关键模块对用户输入进行预处理：(1) **深度查询理解与分解**：该模块利用LLM结合定制化提示（prompts）对查询进行去噪（例如，纠正拼写错误），并将多意图查询分解为结构化的子查询；(2) **意图感知知识检索**：该模块从语料库（即使用OpenSearch的FineWeb）中对每个子查询执行检索，并聚合结果；(3) **重排序与生成**：该模块使用重排序器（即BGE）精炼文档选择，然后由LLM（即Falcon-10B）利用思维链提示（prompt）生成最终响应。Omni-RAG旨在通过鲁棒地处理复杂和嘈杂的查询，弥合当前RAG能力与真实世界应用需求之间的差距，例如SIGIR 2025 LiveRAG挑战赛所强调的需求。"
    },
    {
        "title": "Real-time and personalized product recommendations for large e-commerce\n  platforms",
        "url": "http://arxiv.org/abs/2506.21368v1",
        "pub_date": "2025-06-26",
        "summary": "We present a methodology to provide real-time and personalized product recommendations for large e-commerce platforms, specifically focusing on fashion retail. Our approach aims to achieve accurate and scalable recommendations with minimal response times, ensuring user satisfaction, leveraging Graph Neural Networks and parsimonious learning methodologies. Extensive experimentation with datasets from one of the largest e-commerce platforms demonstrates the effectiveness of our approach in forecasting purchase sequences and handling multi-interaction scenarios, achieving efficient personalized recommendations under real-world constraints.",
        "translated": "我们提出了一种方法，旨在为大型电商平台，尤其是时尚零售领域，提供实时且个性化的商品推荐。我们的方法利用图神经网络（GNNs）和节俭学习方法，旨在实现准确、可扩展且响应时间极短的推荐，从而确保用户满意度。我们利用来自一家最大的电商平台之一的数据集进行了广泛实验，结果表明，我们的方法在预测购买序列和处理多交互场景方面表现出有效性，并在真实世界约束下实现了高效的个性化推荐。"
    },
    {
        "title": "Enhancing Automatic Term Extraction with Large Language Models via\n  Syntactic Retrieval",
        "url": "http://arxiv.org/abs/2506.21222v1",
        "pub_date": "2025-06-26",
        "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that are crucial for downstream tasks such as machine translation and information retrieval. Although large language models (LLMs) have significantly advanced various NLP tasks, their potential for ATE has scarcely been examined. We propose a retrieval-based prompting strategy that, in the few-shot setting, selects demonstrations according to \\emph{syntactic} rather than semantic similarity. This syntactic retrieval method is domain-agnostic and provides more reliable guidance for capturing term boundaries. We evaluate the approach in both in-domain and cross-domain settings, analyzing how lexical overlap between the query sentence and its retrieved examples affects performance. Experiments on three specialized ATE benchmarks show that syntactic retrieval improves F1-score. These findings highlight the importance of syntactic cues when adapting LLMs to terminology-extraction tasks.",
        "translated": "自动术语提取（ATE）旨在识别领域特定表达，这些表达对于机器翻译和信息检索等下游任务至关重要。尽管大型语言模型（LLM）已显著推动了各种自然语言处理（NLP）任务的发展，但其在自动术语提取（ATE）方面的潜力却鲜有探究。\n\n本文提出了一种基于检索的提示策略，该策略在少样本设置下，根据*句法*相似性而非语义相似性来选择示例。这种句法检索方法是领域无关的，并能为捕获术语边界提供更可靠的指导。我们在域内和跨域设置下对该方法进行了评估，并分析了查询语句与所检索示例之间的词汇重叠度如何影响性能。在三个专门的自动术语提取基准数据集上的实验表明，句法检索能够提升F1分数。这些发现凸显了在将大型语言模型应用于术语提取任务时，句法线索的重要性。"
    },
    {
        "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query\n  Classification",
        "url": "http://arxiv.org/abs/2506.21049v1",
        "pub_date": "2025-06-26",
        "summary": "Query classification, including multiple subtasks such as intent and category prediction, is vital to e-commerce applications. E-commerce queries are usually short and lack context, and the information between labels cannot be used, resulting in insufficient prior information for modeling. Most existing industrial query classification methods rely on users' posterior click behavior to construct training samples, resulting in a Matthew vicious cycle. Furthermore, the subtasks of query classification lack a unified framework, leading to low efficiency for algorithm optimization.   In this paper, we propose a novel Semi-supervised Scalable Unified Framework (SSUF), containing multiple enhanced modules to unify the query classification tasks. The knowledge-enhanced module uses world knowledge to enhance query representations and solve the problem of insufficient query information. The label-enhanced module uses label semantics and semi-supervised signals to reduce the dependence on posterior labels. The structure-enhanced module enhances the label representation based on the complex label relations. Each module is highly pluggable, and input features can be added or removed as needed according to each subtask. We conduct extensive offline and online A/B experiments, and the results show that SSUF significantly outperforms the state-of-the-art models.",
        "translated": "查询分类，包括意图预测和类别预测等多个子任务，对电商应用至关重要。电商查询通常较短且缺乏上下文，标签间信息无法利用，导致建模的先验信息不足。大多数现有工业界查询分类方法依赖用户的后验点击行为来构建训练样本，导致马太效应式的恶性循环。此外，查询分类的子任务缺乏统一框架，导致算法优化效率低下。\n\n在本文中，我们提出了一种新颖的半监督可扩展统一框架（SSUF），其中包含多个增强模块，以统一查询分类任务。知识增强模块利用世界知识增强查询表示，解决查询信息不足的问题。标签增强模块利用标签语义和半监督信号，减少对后验标签的依赖。结构增强模块基于复杂的标签关系增强标签表示。每个模块都具有高度可插拔性，可以根据每个子任务的需要增删输入特征。我们进行了大量的离线和在线A/B实验，结果表明SSUF显著优于现有最先进的模型。"
    },
    {
        "title": "RecCoT: Enhancing Recommendation via Chain-of-Thought",
        "url": "http://arxiv.org/abs/2506.21032v1",
        "pub_date": "2025-06-26",
        "summary": "In real-world applications, users always interact with items in multiple aspects, such as through implicit binary feedback (e.g., clicks, dislikes, long views) and explicit feedback (e.g., comments, reviews). Modern recommendation systems (RecSys) learn user-item collaborative signals from these implicit feedback signals as a large-scale binary data-streaming, subsequently recommending other highly similar items based on users' personalized historical interactions. However, from this collaborative-connection perspective, the RecSys does not focus on the actual content of the items themselves but instead prioritizes higher-probability signals of behavioral co-occurrence among items. Consequently, under this binary learning paradigm, the RecSys struggles to understand why a user likes or dislikes certain items. To alleviate it, some works attempt to utilize the content-based reviews to capture the semantic knowledge to enhance recommender models. However, most of these methods focus on predicting the ratings of reviews, but do not provide a human-understandable explanation.",
        "translated": "在实际应用中，用户总是通过多个方面与物品进行交互，例如隐式二元反馈（如点击、不喜欢、长时间观看）以及显式反馈（如评论、评价）。现代推荐系统（RecSys）从这些隐式反馈信号中以大规模二元数据流的形式学习用户-物品协同信号，进而基于用户个性化的历史交互推荐其他高度相似的物品。然而，从这种协同关联的角度来看，推荐系统并不侧重于物品本身的实际内容，而是优先考虑物品间行为共现的高概率信号。因此，在这种二元学习范式下，推荐系统难以理解用户为什么喜欢或不喜欢某些物品。为了缓解这个问题，一些工作尝试利用基于内容的评论来捕获语义知识，以增强推荐模型。然而，这些方法大多侧重于预测评论的评分，但未能提供人类可理解的解释。"
    },
    {
        "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge",
        "url": "http://arxiv.org/abs/2506.21506v1",
        "pub_date": "2025-06-26",
        "summary": "Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems.",
        "translated": "智能体搜索，例如深度研究系统（Deep Research systems），其中大语言模型能够自主浏览网页、综合信息并返回全面且有引用支持的答案，代表了用户与网络规模信息交互方式的重大转变。尽管智能体搜索有望带来更高的效率和认知卸载，但其日益增长的复杂性和开放性已超越了现有的评估基准和方法。这些现有方法主要假设搜索范围较短且答案是静态的。\n\n在本文中，我们引入了Mind2Web 2，这是一个包含130个真实、高质量、长周期任务的基准，这些任务需要实时网页浏览和广泛的信息综合，其构建耗费了超过1,000小时的人工投入。为了解决评估时变且复杂答案的挑战，我们提出了一种新颖的“智能体即评判者”（Agent-as-a-Judge）框架。我们的方法基于树状结构评分标准设计构建任务特定的评判智能体，以自动评估答案的正确性和来源归因。\n\n我们对九个前沿智能体搜索系统和人类表现进行了全面评估，并进行了详细的错误分析，旨在为未来的发展提供见解。表现最佳的系统——OpenAI深度研究系统（OpenAI Deep Research）——已能达到人类表现的50-70%，同时只花费一半时间，展现出巨大的潜力。总而言之，Mind2Web 2为开发和基准测试下一代智能体搜索系统提供了严谨的基础。"
    },
    {
        "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness",
        "url": "http://arxiv.org/abs/2506.21288v1",
        "pub_date": "2025-06-26",
        "summary": "Augmenting large language models (LLMs) with external context significantly improves their performance in natural language processing (NLP) tasks. However, LLMs struggle to answer queries reliably when the provided context lacks information, often resorting to ungrounded speculation or internal knowledge. Groundedness - generating responses strictly supported by the context - is essential for ensuring factual consistency and trustworthiness. This study focuses on detecting whether a given query is grounded in a document provided in context before the costly answer generation by LLMs. Such a detection mechanism can significantly reduce both inference time and resource consumption. We show that lightweight, task specific encoder models such as RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in groundedness detection while reducing inference latency by orders of magnitude. The code is available at : https://github.com/chandarlab/Hallucinate-less",
        "translated": "通过外部上下文增强大型语言模型（LLMs）可以显著提升其在自然语言处理（NLP）任务中的性能。然而，当提供的上下文信息不足时，LLMs 难以可靠地回答查询，常常诉诸于无根据的推测或其内部知识。忠实性——即生成严格由上下文支持的响应——对于确保事实一致性和可信赖性至关重要。本研究着重于在 LLMs 进行耗时的答案生成之前，检测给定查询是否能在所提供的文档上下文中得到支持（即是否具备忠实性）。这种检测机制可以显著减少推理时间和资源消耗。我们研究显示，轻量级、任务专用的编码器模型（例如 RoBERTa 和 NomicBERT），在精心策划的数据集上进行微调后，在忠实性检测方面可以达到与 Llama3 8B 和 GPT4o 等最先进的 LLMs 相媲美的准确性，同时将推理延迟降低数个数量级。代码已开源于：https://github.com/chandarlab/Hallucinate-less"
    },
    {
        "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and\n  Measurement",
        "url": "http://arxiv.org/abs/2506.22372v1",
        "pub_date": "2025-06-27",
        "summary": "The presence of social biases in Natural Language Processing (NLP) and Information Retrieval (IR) systems is an ongoing challenge, which underlines the importance of developing robust approaches to identifying and evaluating such biases. In this paper, we aim to address this issue by leveraging Large Language Models (LLMs) to detect and measure gender bias in passage ranking. Existing gender fairness metrics rely on lexical- and frequency-based measures, leading to various limitations, e.g., missing subtle gender disparities. Building on our LLM-based gender bias detection method, we introduce a novel gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to address existing limitations. To measure the effectiveness of our proposed metric and study LLMs' effectiveness in detecting gender bias, we annotate a subset of the MS MARCO Passage Ranking collection and release our new gender bias collection, called MSMGenderBias, to foster future research in this area. Our extensive experimental results on various ranking models show that our proposed metric offers a more detailed evaluation of fairness compared to previous metrics, with improved alignment to human labels (58.77% for Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa agreement), effectively distinguishing gender bias in ranking. By integrating LLM-driven bias detection, an improved fairness metric, and gender bias annotations for an established dataset, this work provides a more robust framework for analyzing and mitigating bias in IR systems.",
        "translated": "自然语言处理（NLP）和信息检索（IR）系统中社会偏见的存在是一个持续存在的挑战，这凸显了开发鲁棒方法来识别和评估此类偏见的重要性。本文旨在通过利用大语言模型（LLM）来检测和测量段落排序中的性别偏见，从而解决这一问题。现有的性别公平性度量依赖于基于词汇和频率的度量，导致诸多局限性，例如无法捕捉到细微的性别差异。在我们基于LLM的性别偏见检测方法的基础上，我们引入了一种名为“类别加权曝光”（Class-wise Weighted Exposure, CWEx）的新型性别公平性度量，旨在解决现有局限性。为了衡量我们提出的度量的有效性并研究LLM在检测性别偏见方面的有效性，我们对MS MARCO 段落排序数据集的一个子集进行了标注，并发布了我们新的性别偏见数据集MSMGenderBias，以促进该领域的未来研究。我们在各种排序模型上的广泛实验结果表明，与现有度量相比，我们提出的度量提供了更细致的公平性评估，并且与人工标注的对齐度得到提升（使用Cohen's Kappa一致性系数衡量，Grep-BiasIR数据集为58.77%，MSMGenderBias数据集为18.51%），从而有效地区分了排序中的性别偏见。通过整合LLM驱动的偏见检测、改进的公平性度量以及对成熟数据集的性别偏见标注，这项工作为分析和缓解信息检索系统中的偏见提供了一个更鲁棒的框架。"
    },
    {
        "title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval",
        "url": "http://arxiv.org/abs/2506.22356v1",
        "pub_date": "2025-06-27",
        "summary": "The HLTCOE LiveRAG submission utilized the GPT-researcher framework for researching the context of the question, filtering the returned results, and generating the final answer. The retrieval system was a ColBERT bi-encoder architecture, which represents a passage with many dense tokens. Retrieval used a local, compressed index of the FineWeb10-BT collection created with PLAID-X, using a model fine-tuned for multilingual retrieval. Query generation from context was done with Qwen2.5-7B-Instruct, while filtering was accomplished with m2-bert-80M-8k-retrieval. Up to nine passages were used as context to generate an answer using Falcon3-10B. This system placed 5th in the LiveRAG automatic evaluation for correctness with a score of 1.07.",
        "translated": "HLTCOE LiveRAG 参赛系统采用了 GPT-researcher 框架，用于研究问题的上下文、筛选返回结果以及生成最终答案。检索系统采用了 ColBERT 双编码器架构，该架构使用多个稠密标记表示文本段。检索利用了使用 PLAID-X 创建的 FineWeb10-BT 数据集的本地压缩索引，并采用了针对多语言检索微调的模型。从上下文生成查询通过 Qwen2.5-7B-Instruct 完成，而筛选则通过 m2-bert-80M-8k-retrieval 实现。生成答案时，最多使用了九个文本段作为上下文，并通过 Falcon3-10B 模型完成。该系统在 LiveRAG 自动评估的正确性方面位列第五，取得了 1.07 的分数。"
    },
    {
        "title": "Education-Oriented Graph Retrieval-Augmented Generation for Learning\n  Path Recommendation",
        "url": "http://arxiv.org/abs/2506.22303v1",
        "pub_date": "2025-06-27",
        "summary": "Learning path recommendation seeks to provide learners with a structured sequence of learning items (e.g., knowledge concepts or exercises) to optimize their learning efficiency. Despite significant efforts in this area, most existing methods primarily rely on prerequisite relationships, which present two major limitations: 1) Many educational datasets do not explicitly provide prerequisite relationships between knowledge concepts, hindering the application of current learning path recommendation methods. 2) Relying solely on prerequisite relationships as the sole knowledge structure can impede learning progress and negatively impact student outcomes. To address these challenges, we propose a novel approach, Discrimination Learning Enhances Learning Path Recommendation (DLELP), which enhances learning path recommendations by incorporating both prerequisite and similarity relationships between knowledge concepts. Specifically, we introduce a knowledge concept structure graph generation module that adaptively constructs knowledge concept structure graphs for different educational datasets, significantly improving the generalizability of learning path recommendation methods. We then propose a Discrimination Learning-driven Reinforcement Learning (DLRL) framework, which mitigates the issue of blocked learning paths, further enhancing the efficacy of learning path recommendations. Finally, we conduct extensive experiments on three benchmark datasets, demonstrating that our method not only achieves state-of-the-art performance but also provides interpretable reasoning for the recommended learning paths.",
        "translated": "学习路径推荐旨在为学习者提供结构化的学习项目（例如，知识概念或练习）序列，以优化其学习效率。尽管在该领域已进行了大量研究，但大多数现有方法主要依赖于先决条件关系，这存在两大主要局限性：1）许多教育数据集并未明确提供知识概念之间的先决条件关系，这阻碍了当前学习路径推荐方法的应用。2）仅依赖先决条件关系作为唯一的知识结构会阻碍学习进度，并对学生学习成果产生负面影响。\n\n为解决这些挑战，我们提出了一种新颖的方法——判别学习增强学习路径推荐（Discrimination Learning Enhances Learning Path Recommendation, DLELP），该方法通过整合知识概念之间的先决条件和相似性关系来增强学习路径推荐。具体而言，我们引入了一个知识概念结构图生成模块，该模块能够自适应地针对不同的教育数据集构建知识概念结构图，显著提高了学习路径推荐方法的泛化能力。接着，我们提出了一个判别学习驱动的强化学习（Discrimination Learning-driven Reinforcement Learning, DLRL）框架，该框架缓解了学习路径受阻的问题，进一步提升了学习路径推荐的有效性。最后，我们在三个基准数据集上进行了广泛的实验，结果表明我们的方法不仅实现了最先进的性能，而且为推荐的学习路径提供了可解释的推理。"
    },
    {
        "title": "JointRank: Rank Large Set with Single Pass",
        "url": "http://arxiv.org/abs/2506.22262v1",
        "pub_date": "2025-06-27",
        "summary": "Efficiently ranking relevant items from large candidate pools is a cornerstone of modern information retrieval systems -- such as web search, recommendation, and retrieval-augmented generation. Listwise rerankers, which improve relevance by jointly considering multiple candidates, are often limited in practice: either by model input size constraints, or by degraded quality when processing large sets. We propose a model-agnostic method for fast reranking large sets that exceed a model input limits. The method first partitions candidate items into overlapping blocks, each of which is ranked independently in parallel. Implicit pairwise comparisons are then derived from these local rankings. Finally, these comparisons are aggregated to construct a global ranking using algorithms such as Winrate or PageRank. Experiments on TREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the 57.68 for full-context listwise approach using gpt-4.1-mini as long-context model, while reducing latency from 21 to 8 seconds.   The implementation of the algorithm and the experiments is available in the repository: https://github.com/V3RGANz/jointrank",
        "translated": "高效地从大规模候选集中对相关条目进行排序，是现代信息检索系统（如网络搜索、推荐系统和检索增强生成）的基石。列表式重排序器通过联合考虑多个候选来提高相关性，但在实践中常受到限制：要么受限于模型输入尺寸，要么在处理大型数据集时导致质量下降。我们提出了一种模型无关的方法，用于快速重排序超出模型输入限制的大型数据集。该方法首先将候选条目划分为重叠的块，每个块独立并行地进行排序。随后，从这些局部排序中导出隐式成对比较。最后，聚合这些比较以使用诸如Winrate或PageRank等算法构建一个全局排序。在TREC DL-2019上的实验表明，我们的方法实现了70.88的nDCG@10，而使用gpt-4.1-mini作为长上下文模型的全上下文列表式方法的nDCG@10为57.68，同时将延迟从21秒减少到8秒。该算法的实现和实验代码已在以下仓库中开源：https://github.com/V3RGANz/jointrank"
    },
    {
        "title": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation\n  of Responses",
        "url": "http://arxiv.org/abs/2506.22210v1",
        "pub_date": "2025-06-27",
        "summary": "Retrieval-augmented generation (RAG) faces challenges related to factual correctness, source attribution, and response completeness. The LiveRAG Challenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus and a shared, open-source LLM. We propose a modular pipeline that operates on information nuggets-minimal, atomic units of relevant information extracted from retrieved documents. This multistage pipeline encompasses query rewriting, passage retrieval and reranking, nugget detection and clustering, cluster ranking and summarization, and response fluency enhancement. This design inherently promotes grounding in specific facts, facilitates source attribution, and ensures maximum information inclusion within length constraints. In this challenge, we extend our focus to also address the retrieval component of RAG, building upon our prior work on multi-faceted query rewriting. Furthermore, for augmented generation, we concentrate on improving context curation capabilities, maximizing the breadth of information covered in the response while ensuring pipeline efficiency. Our results show that combining original queries with a few sub-query rewrites boosts recall, while increasing the number of documents used for reranking and generation beyond a certain point reduces effectiveness, without improving response quality.",
        "translated": "检索增强生成（RAG）面临着事实准确性、来源归因和回复完整性等方面的挑战。SIGIR'25 举办的 LiveRAG 挑战赛旨在利用固定语料库和共享的开源大型语言模型，推动 RAG 研究的进展。我们提出了一种模块化流水线，该流水线基于从检索到的文档中提取的“信息片段”（即最小的、原子化的相关信息单元）进行操作。这一多阶段流水线涵盖了查询重写、片段检索和重排、信息片段检测和聚类、聚类排序和摘要生成，以及回复流畅性增强。这种设计本质上促进了对特定事实的支撑，便于来源归因，并确保在长度限制内最大程度地涵盖信息。在本次挑战赛中，我们扩大了关注范围，旨在解决 RAG 的检索部分，并借鉴了我们先前在多方面查询重写方面的工作。此外，对于增强生成，我们专注于提升上下文编排能力，在确保流水线效率的同时，最大化回复中信息覆盖的广度。我们的结果表明，将原始查询与少量子查询重写相结合可以提升召回率，而用于重排和生成的文档数量超过某个点后，反而会降低有效性，且不提高回复质量。"
    },
    {
        "title": "The Missing Link: Joint Legal Citation Prediction using Heterogeneous\n  Graph Enrichment",
        "url": "http://arxiv.org/abs/2506.22165v1",
        "pub_date": "2025-06-27",
        "summary": "Legal systems heavily rely on cross-citations of legal norms as well as previous court decisions. Practitioners, novices and legal AI systems need access to these relevant data to inform appraisals and judgments. We propose a Graph-Neural-Network (GNN) link prediction model that can identify Case-Law and Case-Case citations with high proficiency through fusion of semantic and topological information. We introduce adapted relational graph convolutions operating on an extended and enriched version of the original citation graph that allow the topological integration of semantic meta-information. This further improves prediction by 3.1 points of average precision and by 8.5 points in data sparsity as well as showing robust performance over time and in challenging fully inductive prediction. Jointly learning and predicting case and norm citations achieves a large synergistic effect that improves case citation prediction by up to 4.7 points, at almost doubled efficiency.",
        "translated": "法律系统高度依赖法律规范以及过往判例的相互引用。从业者、新手以及法律AI系统都需要获取这些相关数据，以指导其进行评估和判断。为此，我们提出了一种图神经网络（GNN）链接预测模型，该模型通过融合语义和拓扑信息，能够高效准确地识别判例法引用（Case-Law citations）和判例间引用（Case-Case citations）。我们引入了改进的关系图卷积，其作用于原始引用图的扩展和丰富版本，从而实现了语义元信息的拓扑整合。这进一步将预测的平均精度（average precision）提高了3.1个百分点，并将数据稀疏性方面的性能提升了8.5个百分点。同时，该模型在时间维度上以及在具有挑战性的完全归纳预测（fully inductive prediction）任务中均表现出稳健的性能。联合学习并预测判例和规范引用产生了显著的协同效应，使判例引用预测的性能提高了多达4.7个百分点，同时效率几乎翻倍。"
    },
    {
        "title": "DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family\n  Level",
        "url": "http://arxiv.org/abs/2506.22141v1",
        "pub_date": "2025-06-27",
        "summary": "In the landscape of publicly available patent retrieval datasets, the need for explicit indomain and out-of-domain labeling, multi-jurisdiction coverage, balanced query domain representation and manageable sizes that support sub document level experiments on moderate computational resources is often overlooked. To address these gaps, we propose DAPFAM, a new open access domain-aware patent retrieval dataset constructed at the simple-family level. The dataset contains 1,247 domain balanced full text query families and 45,336 full text target families. The dataset is enriched by clear relevance judgments (forward/backward citations as positive links, random negatives), as well as explicit in-domain or out-of-domain relationships via a novel proposed labelling scheme based on via International Patent Classification (IPC) codes, resulting in 49,869 evaluation pairs. The dataset is multi jurisdictional, requires little to no preprocessing for retrieval evaluation, and remains of a size manageable for entities with limited ressources allowing for sub document level retrieval experiments without excessive computational costs. We describe our three-step data-curation pipeline, present comprehensive dataset statistics, and provide baseline experiments using lexical and neural retrieval methods. Our baseline experiments highlight significant challenges in crossdomain patent retrieval. The dataset will be publicly available (for now the access link is this repository: https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).",
        "translated": "在现有公开专利检索数据集领域中，对于明确的域内和域外标注、多司法管辖区覆盖、平衡的查询领域表示以及支持在适中计算资源上进行子文档级实验的可管理规模的需求常常被忽视。为解决这些空白，我们提出了DAPFAM，这是一个构建在专利简单同族级别上的新型开放获取领域感知专利检索数据集。\n\n该数据集包含1,247个领域平衡的全文查询同族和45,336个全文目标同族。该数据集通过明确的相关性判断（将正向和反向引用作为正向链接，随机负样本作为负向链接），以及通过一种基于国际专利分类（IPC）代码的新颖标注方案，明确了域内或域外关系，从而生成了49,869个评估对。该数据集是多司法管辖区的，进行检索评估时几乎无需预处理，并且其规模对于资源有限的实体来说仍是可管理的，使得在不产生过高计算成本的情况下进行子文档级检索实验成为可能。\n\n我们描述了我们的三步数据整理流程，提供了全面的数据集统计信息，并使用词法和神经检索方法进行了基线实验。我们的基线实验突显了跨领域专利检索中的重大挑战。该数据集将公开发布（目前可通过此仓库链接访问：https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b）。"
    },
    {
        "title": "Reward Balancing Revisited: Enhancing Offline Reinforcement Learning for\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2506.22112v1",
        "pub_date": "2025-06-27",
        "summary": "Offline reinforcement learning (RL) has emerged as a prevalent and effective methodology for real-world recommender systems, enabling learning policies from historical data and capturing user preferences. In offline RL, reward shaping encounters significant challenges, with past efforts to incorporate prior strategies for uncertainty to improve world models or penalize underexplored state-action pairs. Despite these efforts, a critical gap remains: the simultaneous balancing of intrinsic biases in world models and the diversity of policy recommendations. To address this limitation, we present an innovative offline RL framework termed Reallocated Reward for Recommender Systems (R3S). By integrating inherent model uncertainty to tackle the intrinsic fluctuations in reward predictions, we boost diversity for decision-making to align with a more interactive paradigm, incorporating extra penalizers with decay that deter actions leading to diminished state variety at both local and global scales. The experimental results demonstrate that R3S improves the accuracy of world models and efficiently harmonizes the heterogeneous preferences of the users.",
        "translated": "离线强化学习（RL）已成为真实世界推荐系统中的一种普遍且有效的方法论，能够从历史数据中学习策略并捕捉用户偏好。在离线强化学习中，奖励塑形面临显著挑战，过去的工作致力于结合处理不确定性的先验策略以改进世界模型，或惩罚探索不足的状态-动作对。尽管付出了这些努力，但仍存在一个关键空白：如何同时平衡世界模型中固有的偏差和策略推荐的多样性。\n\n为解决此局限，我们提出了一个创新的离线强化学习框架，命名为推荐系统重分配奖励（R3S）。通过整合固有的模型不确定性以应对奖励预测中的内在波动，我们提升了决策的多样性，使其契合更具交互性的范式。同时，我们引入了带有衰减的额外惩罚项，以阻止在局部和全局尺度上导致状态多样性降低的动作。实验结果表明，R3S 提高了世界模型的准确性，并有效地协调了用户的异构偏好。"
    },
    {
        "title": "Literature-Grounded Novelty Assessment of Scientific Ideas",
        "url": "http://arxiv.org/abs/2506.22026v1",
        "pub_date": "2025-06-27",
        "summary": "Automated scientific idea generation systems have made remarkable progress, yet the automatic evaluation of idea novelty remains a critical and underexplored challenge. Manual evaluation of novelty through literature review is labor-intensive, prone to error due to subjectivity, and impractical at scale. To address these issues, we propose the Idea Novelty Checker, an LLM-based retrieval-augmented generation (RAG) framework that leverages a two-stage retrieve-then-rerank approach. The Idea Novelty Checker first collects a broad set of relevant papers using keyword and snippet-based retrieval, then refines this collection through embedding-based filtering followed by facet-based LLM re-ranking. It incorporates expert-labeled examples to guide the system in comparing papers for novelty evaluation and in generating literature-grounded reasoning. Our extensive experiments demonstrate that our novelty checker achieves approximately 13% higher agreement than existing approaches. Ablation studies further showcases the importance of the facet-based re-ranker in identifying the most relevant literature for novelty evaluation.",
        "translated": "自动化科学思想生成系统已取得显著进展，然而，思想新颖性的自动评估仍是一个关键且未充分探索的挑战。通过文献综述进行人工新颖性评估是劳动密集型的，易因主观性而产生误差，且难以大规模实施。为解决这些问题，我们提出了一种名为“思想新颖性检查器”（Idea Novelty Checker）的框架，它是一个基于大型语言模型（LLM）的检索增强生成（RAG）框架，采用两阶段的“检索-重排序”方法。思想新颖性检查器首先利用关键词和片段检索收集广泛的相关论文，然后通过基于嵌入的过滤以及基于方面的LLM重排序来精炼此集合。该系统整合了专家标注的示例，以指导其在进行新颖性评估时比较论文，并生成有文献依据的推理。我们广泛的实验表明，我们的新颖性检查器比现有方法实现了约13%的更高一致性。消融研究进一步突显了基于方面的重排序器在识别新颖性评估最相关文献方面的重要性。"
    },
    {
        "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware\n  Layout Design",
        "url": "http://arxiv.org/abs/2506.21934v1",
        "pub_date": "2025-06-27",
        "summary": "Automated content-aware layout generation -- the task of arranging visual elements such as text, logos, and underlays on a background canvas -- remains a fundamental yet under-explored problem in intelligent design systems. While recent advances in deep generative models and large language models (LLMs) have shown promise in structured content generation, most existing approaches lack grounding in contextual design exemplars and fall short in handling semantic alignment and visual coherence. In this work we introduce CAL-RAG, a retrieval-augmented, agentic framework for content-aware layout generation that integrates multimodal retrieval, large language models, and collaborative agentic reasoning. Our system retrieves relevant layout examples from a structured knowledge base and invokes an LLM-based layout recommender to propose structured element placements. A vision-language grader agent evaluates the layout with visual metrics, and a feedback agent provides targeted refinements, enabling iterative improvement. We implement our framework using LangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in semantic and structural variability. CAL-RAG achieves state-of-the-art performance across multiple layout metrics -- including underlay effectiveness, element alignment, and overlap -- substantially outperforming strong baselines such as LayoutPrompter. These results demonstrate that combining retrieval augmentation with agentic multi-step reasoning yields a scalable, interpretable, and high-fidelity solution for automated layout generation.",
        "translated": "自动化内容感知布局生成——即在背景画布上排列文本、标志和底层元素等视觉元素的任务——在智能设计系统中仍是一个基础但有待深入探索的问题。尽管深度生成模型和大型语言模型（LLMs）的最新进展在结构化内容生成方面展现出潜力，但大多数现有方法缺乏基于上下文设计范例的支撑，并且在处理语义对齐和视觉一致性方面表现不足。\n\n在这项工作中，我们引入了CAL-RAG，这是一个用于内容感知布局生成的检索增强型智能体框架，它整合了多模态检索、大型语言模型以及协作式智能体推理。我们的系统从结构化知识库中检索相关的布局示例，并调用一个基于LLM的布局推荐器来提出结构化的元素放置方案。一个视觉-语言评估智能体利用视觉指标评估布局，而一个反馈智能体则提供有针对性的改进建议，从而实现迭代优化。\n\n我们使用LangGraph实现了我们的框架，并在PKU PosterLayout数据集上对其进行评估，该数据集是一个富含语义和结构多样性的基准。CAL-RAG在多项布局指标上达到了最先进的性能——包括底层元素有效性、元素对齐和重叠——显著优于LayoutPrompter等强劲基线。这些结果表明，将检索增强与智能体多步推理相结合，能够为自动化布局生成带来一个可扩展、可解释且高保真度的解决方案。"
    },
    {
        "title": "CLoVE: Personalized Federated Learning through Clustering of Loss Vector\n  Embeddings",
        "url": "http://arxiv.org/abs/2506.22427v1",
        "pub_date": "2025-06-27",
        "summary": "We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped into clusters based on their data distribution. However, identifying these clusters is challenging, as client assignments are unknown. CLoVE utilizes client embeddings derived from model losses on client data, and leverages the insight that clients in the same cluster share similar loss values, while those in different clusters exhibit distinct loss patterns. Based on these embeddings, CLoVE is able to iteratively identify and separate clients from different clusters and optimize cluster-specific models through federated aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its simplicity, (2) its applicability to both supervised and unsupervised settings, and (3) the fact that it eliminates the need for near-optimal model initialization, which makes it more robust and better suited for real-world applications. We establish theoretical convergence bounds, showing that CLoVE can recover clusters accurately with high probability in a single round and converges exponentially fast to optimal models in a linear setting. Our comprehensive experiments comparing with a variety of both CFL and generic Personalized Federated Learning (PFL) algorithms on different types of datasets and an extensive array of non-IID settings demonstrate that CLoVE achieves highly accurate cluster recovery in just a few rounds of training, along with state-of-the-art model accuracy, across a variety of both supervised and unsupervised PFL tasks.",
        "translated": "我们提出 CLoVE（损失向量嵌入聚类），这是一种用于联邦聚类学习（CFL）的新颖算法。在联邦聚类学习中，客户端根据其数据分布自然地分组为不同的簇。然而，识别这些簇极具挑战性，因为客户端的归属是未知的。CLoVE 利用从模型在客户端数据上的损失中提取的客户端嵌入，并借助于以下洞察：同一簇中的客户端共享相似的损失值，而不同簇中的客户端则表现出不同的损失模式。基于这些嵌入，CLoVE 能够迭代地识别并分离来自不同簇的客户端，并通过联邦聚合优化特定于簇的模型。相较于现有联邦聚类学习算法，CLoVE 的主要优势在于：(1) 其简单性；(2) 其适用于监督和无监督两种设置；(3) 它消除了对近似最优模型初始化的需求，这使其更具鲁棒性，更适合实际应用。我们建立了理论收敛边界，表明 CLoVE 可以在单轮中以高概率准确地恢复簇，并且在线性设置下以指数级速度收敛到最优模型。我们进行了全面的实验，将 CLoVE 与多种联邦聚类学习（CFL）和通用个性化联邦学习（PFL）算法在不同类型的数据集和广泛的非独立同分布（non-IID）设置下进行了比较。实验结果表明，CLoVE 仅在少数几轮训练中就能实现高度准确的簇恢复，并在各种监督和无监督的个性化联邦学习任务中同时达到最先进的模型准确性。"
    },
    {
        "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
        "url": "http://arxiv.org/abs/2506.22396v1",
        "pub_date": "2025-06-27",
        "summary": "Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms:   (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length.   Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (&lt;=0.2).",
        "translated": "大语言模型（LLM）部署中，推理过程占据了大部分延迟和能耗，通常超过总成本的90%。尽管训练阶段的效率已取得了长足进展，但运行时优化仍是关键瓶颈，尤其是在自回归解码过程中。现有方法——如剪枝、量化、提前退出和推测解码——通常需要重新训练、进行架构更改，或会破坏解码兼容性。\n\n我们引入了QuickSilver，这是一个模块化的令牌级框架，它能够在不改变模型权重或结构的情况下，在推理时实现语义自适应性。QuickSilver集成了四种协同机制：(i) **动态令牌停止**：对表示已收敛的令牌停止计算；(ii) **KV缓存跳过**：选择性地抑制内存写入以减少注意力开销；(iii) **上下文令牌融合**：将冗余令牌合并到共享路径中，以缩短序列长度。与推测解码或MoE路由不同，QuickSilver完全在冻结的密集模型上运行，并且不需要辅助网络。将QuickSilver应用于GPT-2和Llama-2模型，并在WikiText-103和C4数据集上进行测试，QuickSilver实现了高达39.6%的FLOPs（浮点运算）减少，同时困惑度下降可忽略不计（≤0.2）。"
    },
    {
        "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and\n  Measurement",
        "url": "http://arxiv.org/abs/2506.22372v1",
        "pub_date": "2025-06-27",
        "summary": "The presence of social biases in Natural Language Processing (NLP) and Information Retrieval (IR) systems is an ongoing challenge, which underlines the importance of developing robust approaches to identifying and evaluating such biases. In this paper, we aim to address this issue by leveraging Large Language Models (LLMs) to detect and measure gender bias in passage ranking. Existing gender fairness metrics rely on lexical- and frequency-based measures, leading to various limitations, e.g., missing subtle gender disparities. Building on our LLM-based gender bias detection method, we introduce a novel gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to address existing limitations. To measure the effectiveness of our proposed metric and study LLMs' effectiveness in detecting gender bias, we annotate a subset of the MS MARCO Passage Ranking collection and release our new gender bias collection, called MSMGenderBias, to foster future research in this area. Our extensive experimental results on various ranking models show that our proposed metric offers a more detailed evaluation of fairness compared to previous metrics, with improved alignment to human labels (58.77% for Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa agreement), effectively distinguishing gender bias in ranking. By integrating LLM-driven bias detection, an improved fairness metric, and gender bias annotations for an established dataset, this work provides a more robust framework for analyzing and mitigating bias in IR systems.",
        "translated": "自然语言处理（NLP）和信息检索（IR）系统中社会偏见的存在是一个持续存在的难题，这凸显了开发鲁棒方法来识别和评估此类偏见的重要性。在本文中，我们旨在通过利用大型语言模型（LLM）来检测和衡量段落排序中的性别偏见，从而解决这一问题。现有的性别公平性指标依赖于基于词汇和频率的度量方法，导致了各种局限性，例如遗漏了细微的性别差异。基于我们提出的LLM性别偏见检测方法，我们引入了一种新颖的性别公平性指标，名为“类别加权曝光度”（CWEx），旨在解决现有局限性。为了衡量我们提出的指标的有效性，并研究LLM在检测性别偏见方面的有效性，我们标注了MS MARCO段落排序数据集的一个子集，并发布了我们新的性别偏见数据集，命名为MSMGenderBias，以促进该领域的未来研究。我们在各种排序模型上进行的广泛实验结果表明，与现有指标相比，我们提出的指标提供了更详细的公平性评估，并且与人工标注的一致性更高（Grep-BiasIR数据集上为58.77%，MSMGenderBias数据集上为18.51%，使用Cohen's Kappa一致性系数衡量），从而有效地区分了排序中的性别偏见。通过整合LLM驱动的偏见检测、改进的公平性指标以及对既有数据集的性别偏见标注，这项工作为分析和缓解信息检索系统中的偏见提供了一个更鲁棒的框架。"
    },
    {
        "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation",
        "url": "http://arxiv.org/abs/2506.23662v1",
        "pub_date": "2025-06-30",
        "summary": "Context-aware embedding methods boost retrieval accuracy by conditioning on corpus statistics (e.g., term co-occurrence and topical patterns) extracted from neighboring documents. However, this context-aware approach requires access to the target corpus or requires domain-specific finetuning, posing practical barriers in privacy-sensitive or resource-constrained settings. We present ZEST, a zero-shot contextual adaptation framework that replaces real corpus access with a one-time offline synthesis of a compact proxy. Given only a handful exemplar documents representative of the general target domain, we use a multi-step hierarchical procedure to generate a synthetic context corpus of several hundred documents that aims to emulate key domain-specific distributions. At inference, the frozen context-aware encoder uses this proxy corpus -- without any finetuning or target corpus access -- to produce domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot synthetic context adaptation using only five example documents performs within 0.5% of models leveraging full target corpus access -- demonstrating remarkable efficacy without any retraining. ZEST thus provides a practical method for deploying high-performance, adaptable embeddings in constrained environments.",
        "translated": "上下文感知嵌入方法通过利用从相邻文档中提取的语料库统计信息（例如词项共现和主题模式）来提升检索精度。然而，这种上下文感知方法需要访问目标语料库或进行领域特定的微调，这在隐私敏感或资源受限的环境中构成了实际障碍。\n\n我们提出了 ZEST，一个零样本上下文适应框架，它通过一次性离线合成一个紧凑的代理来取代对真实语料库的访问。只需提供少量代表通用目标领域的示例文档，我们便能使用多步分层程序生成一个包含数百个文档的合成上下文语料库，旨在模拟关键的领域特定分布。在推理时，冻结的上下文感知编码器无需任何微调或目标语料库访问，即可使用这个代理语料库来生成领域适应性嵌入。\n\n在 MTEB 基准测试中，ZEST 仅使用五个示例文档进行零样本合成上下文适应，其表现与利用完整目标语料库访问的模型相差不到 0.5%，这表明其在无需任何再训练的情况下具有显著的效能。因此，ZEST 为在受限环境中部署高性能、适应性强的嵌入提供了一种实用方法。"
    },
    {
        "title": "Act-With-Think: Chunk Auto-Regressive Modeling for Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2506.23643v1",
        "pub_date": "2025-06-30",
        "summary": "Generative recommendation (GR) typically encodes behavioral or semantic aspects of item information into discrete tokens, leveraging the standard autoregressive (AR) generation paradigm to make predictions. However, existing methods tend to overlook their intrinsic relationship, that is, the semantic usually provides some reasonable explainability \"$\\textbf{why}$\" for the behavior \"$\\textbf{what}$\", which may constrain the full potential of GR. To this end, we present Chunk AutoRegressive Modeling (CAR), a new generation paradigm following the decision pattern that users usually think semantic aspects of items (e.g. brand) and then take actions on target items (e.g. purchase). Our CAR, for the $\\textit{first time}$, incorporates semantics (SIDs) and behavior (UID) into a single autoregressive transformer from an ``act-with-think'' dual perspective via chunk-level autoregression. Specifically, CAR packs SIDs and UID into a conceptual chunk for item unified representation, allowing each decoding step to make a holistic prediction. Experiments show that our CAR significantly outperforms existing methods based on traditional AR, improving Recall@5 by 7.93% to 22.30%. Furthermore, we verify the scaling effect between model performance and SIDs bit number, demonstrating that CAR preliminary emulates a kind of slow-thinking style mechanism akin to the reasoning processes observed in large language models (LLMs).",
        "translated": "生成式推荐（GR）通常将物品信息的行为或语义方面编码成离散标记，并利用标准的自回归（AR）生成范式进行预测。然而，现有方法往往忽视了它们内在的关系，即语义通常为行为“是什么”（$\\textbf{what}$）提供合理的解释“为什么”（$\\textbf{why}$），这可能限制了GR的全部潜力。为此，我们提出了块自回归建模（CAR），这是一种新的生成范式，遵循用户通常先思考物品的语义方面（例如品牌），然后对目标物品采取行动（例如购买）的决策模式。我们的CAR首次将语义信息（SIDs）和行为信息（UID）从“行动与思考”的双重视角，通过块级自回归整合到一个单一的自回归Transformer中。具体而言，CAR将SIDs和UID打包成一个概念块，用于物品的统一表示，使得每个解码步骤都能进行整体预测。实验表明，我们的CAR显著优于基于传统AR的现有方法，将Recall@5提升了7.93%至22.30%。此外，我们验证了模型性能与SIDs比特数之间的规模效应，证明CAR初步模拟了一种类似于大型语言模型（LLMs）中观察到的推理过程的慢思考（slow-thinking）式机制。"
    },
    {
        "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent",
        "url": "http://arxiv.org/abs/2506.23485v1",
        "pub_date": "2025-06-30",
        "summary": "Interactive recommendation is a typical information-seeking task that allows users to interactively express their needs through natural language and obtain personalized recommendations. Large language model-powered (LLM-powered) agents have become a new paradigm in interactive recommendations, effectively capturing users' real-time needs and enhancing personalized experiences. However, due to limited planning and generalization capabilities, existing formulations of LLM-powered interactive recommender agents struggle to effectively address diverse and complex user intents, such as intuitive, unrefined, or occasionally ambiguous requests. To tackle this challenge, we propose a novel thought-augmented interactive recommender agent system (TAIRA) that addresses complex user intents through distilled thought patterns. Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring a manager agent that orchestrates recommendation tasks by decomposing user needs and planning subtasks, with its planning capacity strengthened through Thought Pattern Distillation (TPD), a thought-augmentation method that extracts high-level thoughts from the agent's and human experts' experiences. Moreover, we designed a set of user simulation schemes to generate personalized queries of different difficulties and evaluate the recommendations based on specific datasets. Through comprehensive experiments conducted across multiple datasets, TAIRA exhibits significantly enhanced performance compared to existing methods. Notably, TAIRA shows a greater advantage on more challenging tasks while generalizing effectively on novel tasks, further validating its superiority in managing complex user intents within interactive recommendation systems. The code is publicly available at:https://github.com/Alcein/TAIRA.",
        "translated": "交互式推荐是一种典型的**信息检索任务**，它允许用户通过自然语言交互式地表达其需求，并获得个性化推荐。大语言模型（LLM）驱动的智能体已成为交互式推荐领域的新范式，能够有效捕获用户的实时需求并增强个性化体验。然而，由于规划和泛化能力的局限性，现有的大语言模型驱动的交互式推荐智能体设计难以有效应对多样化和复杂的用户意图，例如直观的、未经精炼的或偶尔模糊的请求。为解决这一挑战，我们提出了一种新颖的**思想增强型交互式推荐智能体系统（TAIRA）**，该系统通过蒸馏的思维模式来解决复杂的用户意图。具体来说，TAIRA 被设计为一个大语言模型驱动的多智能体系统，其核心是一个管理智能体，负责通过分解用户需求和规划子任务来协调推荐任务。其规划能力通过**思维模式蒸馏（TPD）**得到强化，这是一种从智能体自身和人类专家的经验中提取高层次思维的思想增强方法。此外，我们设计了一套用户模拟方案，以生成不同难度的个性化查询，并基于特定数据集评估推荐效果。通过在多个数据集上进行的综合实验，TAIRA 表现出与现有方法相比显著增强的性能。值得注意的是，TAIRA 在更具挑战性的任务上显示出更大的优势，同时在全新任务上也能有效泛化，这进一步验证了其在交互式推荐系统中管理复杂用户意图方面的优越性。代码已公开，地址为：https://github.com/Alcein/TAIRA。"
    },
    {
        "title": "KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation,\n  And Try-On",
        "url": "http://arxiv.org/abs/2506.23471v1",
        "pub_date": "2025-06-30",
        "summary": "The global fashion e-commerce industry has become integral to people's daily lives, leveraging technological advancements to offer personalized shopping experiences, primarily through recommendation systems that enhance customer engagement through personalized suggestions. To improve customers' experience in online shopping, we propose a novel comprehensive KiseKloset system for outfit retrieval, recommendation, and try-on. We explore two approaches for outfit retrieval: similar item retrieval and text feedback-guided item retrieval. Notably, we introduce a novel transformer architecture designed to recommend complementary items from diverse categories. Furthermore, we enhance the overall performance of the search pipeline by integrating approximate algorithms to optimize the search process. Additionally, addressing the crucial needs of online shoppers, we employ a lightweight yet efficient virtual try-on framework capable of real-time operation, memory efficiency, and maintaining realistic outputs compared to its predecessors. This virtual try-on module empowers users to visualize specific garments on themselves, enhancing the customers' experience and reducing costs associated with damaged items for retailers. We deployed our end-to-end system for online users to test and provide feedback, enabling us to measure their satisfaction levels. The results of our user study revealed that 84% of participants found our comprehensive system highly useful, significantly improving their online shopping experience.",
        "translated": "全球时尚电商行业已深度融入人们的日常生活，它借助技术进步提供个性化购物体验，主要通过推荐系统提供个性化建议以增强客户参与度。为了提升用户在线购物体验，我们提出了一种新颖的综合性KiseKloset系统，用于实现服饰搭配检索、推荐和试穿功能。在服饰搭配检索方面，我们探索了两种方法：相似商品检索和文本反馈引导的商品检索。值得注意的是，我们引入了一种新颖的Transformer架构，旨在推荐来自不同品类的互补商品。此外，我们通过整合近似算法来优化搜索过程，从而提升了整个搜索流程的整体性能。此外，为了满足在线购物者的关键需求，我们采用了一种轻量级但高效的虚拟试穿框架，该框架能够实现实时运行、高效利用内存，并与现有技术相比保持逼真的输出效果。这一虚拟试穿模块赋能用户直观地看到特定服装穿在自己身上的效果，从而提升了客户体验，并为零售商降低了因商品损坏产生的成本。我们将端到端系统部署供在线用户测试并提供反馈，以便衡量他们的满意度。我们的用户研究结果显示，84%的参与者认为我们这套综合系统非常实用，显著改善了他们的在线购物体验。"
    },
    {
        "title": "NaviX: A Native Vector Index Design for Graph DBMSs With Robust\n  Predicate-Agnostic Search Performance",
        "url": "http://arxiv.org/abs/2506.23397v1",
        "pub_date": "2025-06-29",
        "summary": "There is an increasing demand for extending existing DBMSs with vector indices so that they become unified systems capable of supporting modern predictive applications, which require joint querying of vector embeddings together with the structured properties and connections of objects. We present NaviX, a native vector index for graph DBMSs (GDBMSs) that has two main design goals. First, we aim to implement a disk-based vector index that leverages the core storage and query-processing capabilities of the underlying GDBMS. To this end, NaviX is built on the Hierarchical Navigable Small-World (HNSW) graph, which itself is a graph-based structure. Second, we aim to support predicate-agnostic filtered vector search queries, in which the k nearest neighbors (kNNs) of a query vector vQ are searched only within an arbitrary subset S of vectors defined by an ad-hoc selection sub-query QS. We adopt a prefiltering approach that evaluates QS first and passes the full description of subset S to the kNN search operator. We study how to design a prefiltering search algorithm that remains robust under varying selectivities and under different correlations between subset S and query vector vQ. We propose an adaptive algorithm that uses the local selectivity of each vector in the HNSW graph to choose an appropriate heuristic at every iteration of the kNN search. Finally, We demonstrate NaviX's robustness and efficiency through extensive experiments against both existing prefiltering- and postfiltering-based baselines.",
        "translated": "当前，业界对扩展现有数据库管理系统（DBMS）以支持向量索引的需求日益增长，旨在使其成为能够支持现代预测应用的统一系统。这些应用需要联合查询向量嵌入以及对象的结构化属性和连接。我们提出了NaviX，一个用于图数据库管理系统（GDBMS）的原生向量索引，它有两个主要设计目标。\n\n首先，我们旨在实现一个基于磁盘的向量索引，该索引能够利用底层GDBMS的核心存储和查询处理能力。为此，NaviX构建在分层可导航小世界（HNSW）图上，HNSW本身也是一种基于图的结构。其次，我们旨在支持谓词无关的过滤向量搜索查询，即查询向量vQ的k近邻（kNNs）仅在由临时选择子查询QS定义的任意向量子集S中进行搜索。我们采用了一种预过滤方法，该方法首先评估QS，并将子集S的完整描述传递给kNN搜索操作符。\n\n我们研究了如何设计一种预过滤搜索算法，使其在不同选择度下以及子集S与查询向量vQ之间存在不同相关性时，仍能保持鲁棒性。我们提出了一种自适应算法，该算法利用HNSW图中每个向量的局部选择度，在kNN搜索的每次迭代中选择合适的启发式方法。最后，通过与现有基于预过滤和后过滤的基线进行大量实验，我们证明了NaviX的鲁棒性和效率。"
    },
    {
        "title": "Learning to Rank with Variable Result Presentation Lengths",
        "url": "http://arxiv.org/abs/2506.23319v1",
        "pub_date": "2025-06-29",
        "summary": "Learning to Rank (LTR) methods generally assume that each document in a top-K ranking is presented in an equal format. However, previous work has shown that users' perceptions of relevance can be changed by varying presentations, i.e., allocating more vertical space to some documents to provide additional textual or image information. Furthermore, presentation length can also redirect attention, as users are more likely to notice longer presentations when scrolling through results. Deciding on the document presentation lengths in a fixed vertical space ranking is an important problem that has not been addressed by existing LTR methods.   We address this gap by introducing the variable presentation length ranking task, where simultaneously the ordering of documents and their presentation length is decided. Despite being a generalization of standard ranking, we show that this setting brings significant new challenges: Firstly, the probability ranking principle no longer applies to this setting, and secondly, the problem cannot be divided into separate ordering and length selection tasks.   We therefore propose VLPL - a new family of Plackett-Luce list-wise gradient estimation methods for the joint optimization of document ordering and lengths. Our semi-synthetic experiments show that VLPL can effectively balance the expected exposure and attractiveness of all documents, achieving the best performance across different ranking settings. Furthermore, we observe that even simple length-aware methods can achieve significant performance improvements over fixed-length models. Altogether, our theoretical and empirical results highlight the importance and difficulties of combining document presentation with LTR.",
        "translated": "学习排序（LTR）方法通常假设顶K排序中的每个文档都以相同的格式呈现。然而，先前的工作表明，通过改变呈现方式（即为某些文档分配更多垂直空间以提供额外的文本或图像信息），用户对相关性的感知会发生变化。此外，呈现长度也能重定向用户的注意力，因为用户在滚动浏览结果时更可能注意到较长的呈现内容。在固定垂直空间的排序中决定文档的呈现长度是一个重要问题，而现有LTR方法尚未解决。\n\n我们通过引入可变呈现长度排序任务来弥补这一空白，该任务同时决定文档的排序及其呈现长度。尽管这是标准排序的一种泛化，但我们表明此设置带来了重大的新挑战：首先，概率排序原则（PRP）不再适用于此设置；其次，该问题不能分解为独立的排序和长度选择任务。\n\n因此，我们提出了VLPL——一种新的Plackett-Luce列表式梯度估计方法家族，用于文档排序和长度的联合优化。我们的半合成实验表明，VLPL能够有效平衡所有文档的预期曝光度和吸引力，在不同排序设置下均实现了最佳性能。此外，我们观察到，即使是简单的长度感知方法，相较于固定长度模型也能实现显著的性能提升。总而言之，我们的理论和经验结果凸显了将文档呈现与LTR相结合的重要性和困难性。"
    },
    {
        "title": "On the Predictive Power of Representation Dispersion in Language Models",
        "url": "http://arxiv.org/abs/2506.24106v1",
        "pub_date": "2025-06-30",
        "summary": "We show that a language model's ability to predict text is tightly linked to the breadth of its embedding space: models that spread their contextual representations more widely tend to achieve lower perplexity. Concretely, we find that representation dispersion - the average pairwise cosine distance among hidden vectors - strongly and negatively correlates with perplexity across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia, news, scientific abstracts). Beyond illustrating this link, we show how dispersion can be leveraged for a range of practical tasks without requiring labeled data. First, measuring dispersion on unlabeled text allows us to predict downstream accuracy in new domains, offering a data-efficient tool for model selection. Next, we find that identifying layers with higher dispersion pinpoints the best representations for retrieval-based methods such as kNN-LM, bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple push-away objective into training, which increases dispersion in both single-domain and cross-domain scenarios and directly improves perplexity in each.",
        "translated": "我们发现，语言模型的文本预测能力与其嵌入空间的广度紧密相关：那些能够将上下文表示更广泛地分散的模型，往往能实现更低的困惑度。具体而言，我们发现表示分散度——即隐藏向量之间的平均成对余弦距离——在多种模型家族（LLaMA、Qwen等）和不同领域（维基百科、新闻、科学摘要）中，与困惑度呈强烈负相关。\n\n除了阐明这一关联之外，我们还展示了如何在无需标注数据的情况下，利用表示分散度来执行一系列实际任务。首先，在未标注文本上测量分散度，使我们能够预测模型在新领域中的下游任务准确性，从而提供了一种数据高效的模型选择工具。其次，我们发现识别出具有更高分散度的层，能够精确定位用于基于检索的方法（如kNN-LM）的最佳表示，从而避免了详尽的逐层搜索。最后，我们还在训练中整合了一个简单的“推开”（push-away）目标，它能增加单领域和跨领域场景中的表示分散度，并直接改善了这两种情况下的困惑度。"
    },
    {
        "title": "Computational Detection of Intertextual Parallels in Biblical Hebrew: A\n  Benchmark Study Using Transformer-Based Language Models",
        "url": "http://arxiv.org/abs/2506.24117v1",
        "pub_date": "2025-06-30",
        "summary": "Identifying parallel passages in biblical Hebrew is foundational in biblical scholarship for uncovering intertextual relationships. Traditional methods rely on manual comparison, which is labor-intensive and prone to human error. This study evaluates the potential of pre-trained transformer-based language models, including E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in the Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings and Chronicles, I assessed each model's capability to generate word embeddings that delineate parallel from non-parallel passages. Utilizing cosine similarity and Wasserstein Distance measures, I found that E5 and AlephBERT show significant promise, with E5 excelling in parallel detection and AlephBERT demonstrating stronger non-parallel differentiation. These findings indicate that pre-trained models can enhance the efficiency and accuracy of detecting intertextual parallels in ancient texts, suggesting broader applications for ancient language studies.",
        "translated": "识别希伯来圣经中的平行篇章在圣经研究中具有基础性意义，有助于揭示互文关系。传统方法依赖人工比对，耗时耗力且容易出现人为错误。本研究评估了预训练的基于Transformer的语言模型（包括E5、AlephBERT、MPNet和LaBSE）在检测希伯来圣经文本平行篇章方面的潜力。我侧重于撒母耳记/列王纪与历代志之间的已知平行篇章，评估了每个模型生成词嵌入以区分平行篇章与非平行篇章的能力。利用余弦相似度和Wasserstein距离度量，我发现E5和AlephBERT展现出显著潜力，其中E5在平行检测方面表现出色，而AlephBERT则在非平行区分方面表现出更强的能力。这些发现表明，预训练模型可以提高古代文本中互文平行检测的效率和准确性，预示着其在古代语言研究中更广泛的应用前景。"
    },
    {
        "title": "On the Predictive Power of Representation Dispersion in Language Models",
        "url": "http://arxiv.org/abs/2506.24106v1",
        "pub_date": "2025-06-30",
        "summary": "We show that a language model's ability to predict text is tightly linked to the breadth of its embedding space: models that spread their contextual representations more widely tend to achieve lower perplexity. Concretely, we find that representation dispersion - the average pairwise cosine distance among hidden vectors - strongly and negatively correlates with perplexity across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia, news, scientific abstracts). Beyond illustrating this link, we show how dispersion can be leveraged for a range of practical tasks without requiring labeled data. First, measuring dispersion on unlabeled text allows us to predict downstream accuracy in new domains, offering a data-efficient tool for model selection. Next, we find that identifying layers with higher dispersion pinpoints the best representations for retrieval-based methods such as kNN-LM, bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple push-away objective into training, which increases dispersion in both single-domain and cross-domain scenarios and directly improves perplexity in each.",
        "translated": "我们发现，语言模型预测文本的能力与其嵌入空间的广度紧密相关：那些能更广泛地分散其上下文表示的模型，往往能实现更低的困惑度。具体而言，我们发现表示分散度——即隐藏向量之间的平均成对余弦距离——在不同的模型家族（LLaMA、Qwen等）和领域（维基百科、新闻、科学摘要）中，与困惑度呈现强烈负相关。\n\n除了阐明这一联系，我们还展示了如何在无需标注数据的情况下，将分散度应用于一系列实际任务。首先，在未标注文本上测量分散度使我们能够预测在新领域中的下游准确性，为模型选择提供了一种数据高效的工具。其次，我们发现识别具有更高分散度的层可以精确定位基于检索的方法（如kNN-LM）的最佳表示，从而绕过详尽的逐层搜索。最后，我们将一个简单的“推开”（push-away）目标集成到训练中，这在单领域和跨领域场景中都增加了分散度，并直接改善了各自的困惑度。"
    },
    {
        "title": "Deep Recommender Models Inference: Automatic Asymmetric Data Flow\n  Optimization",
        "url": "http://arxiv.org/abs/2507.01676v1",
        "pub_date": "2025-07-02",
        "summary": "Deep Recommender Models (DLRMs) inference is a fundamental AI workload accounting for more than 79% of the total AI workload in Meta's data centers. DLRMs' performance bottleneck is found in the embedding layers, which perform many random memory accesses to retrieve small embedding vectors from tables of various sizes. We propose the design of tailored data flows to speedup embedding look-ups. Namely, we propose four strategies to look up an embedding table effectively on one core, and a framework to automatically map the tables asymmetrically to the multiple cores of a SoC. We assess the effectiveness of our method using the Huawei Ascend AI accelerators, comparing it with the default Ascend compiler, and we perform high-level comparisons with Nvidia A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload distributions, and more than 20x for extremely unbalanced distributions. Furthermore, the method proves to be much more independent of the query distribution than the baseline.",
        "translated": "深度推荐模型（DLRMs）的推理是Meta数据中心中一项核心的AI工作负载，其占总AI工作负载的比例超过79%。DLRMs的性能瓶颈在于其嵌入层，该层需要执行大量的随机内存访问，以从各种大小的表中检索小型嵌入向量。为此，我们提出设计定制的数据流，以加速嵌入查找。具体而言，我们提出了四种在一核上高效查找嵌入表的策略，以及一个将这些表非对称地自动映射到片上系统（SoC）多核心的框架。我们使用华为昇腾AI加速器评估了我们方法的有效性，将其与默认的昇腾编译器进行了比较，并与英伟达A100进行了高层级比较。结果显示，对于实际工作负载分布，加速比达到1.5倍至6.5倍；对于极度不平衡的分布，加速则超过20倍。此外，结果表明该方法相比基线而言，对查询分布的依赖性要小得多。"
    },
    {
        "title": "Enhanced Influence-aware Group Recommendation for Online Media\n  Propagation",
        "url": "http://arxiv.org/abs/2507.01616v1",
        "pub_date": "2025-07-02",
        "summary": "Group recommendation over social media streams has attracted significant attention due to its wide applications in domains such as e-commerce, entertainment, and online news broadcasting. By leveraging social connections and group behaviours, group recommendation (GR) aims to provide more accurate and engaging content to a set of users rather than individuals. Recently, influence-aware GR has emerged as a promising direction, as it considers the impact of social influence on group decision-making. In earlier work, we proposed Influence-aware Group Recommendation (IGR) to solve this task. However, this task remains challenging due to three key factors: the large and ever-growing scale of social graphs, the inherently dynamic nature of influence propagation within user groups, and the high computational overhead of real-time group-item matching.   To tackle these issues, we propose an Enhanced Influence-aware Group Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based Sampling (GES) strategy to minimise redundancy across multiple temporal social graphs and effectively capture the evolving dynamics of both groups and items. Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict how influence propagates over time across social items and user groups. Finally, we develop a two-level hash-based User Group Index (UG-Index) to efficiently organise user groups and enable real-time recommendation generation. Extensive experiments on real-world datasets demonstrate that our proposed framework, EIGR, consistently outperforms state-of-the-art baselines in both effectiveness and efficiency.",
        "translated": "社交媒体流上的群组推荐因其在电子商务、娱乐、在线新闻广播等领域的广泛应用而引起了广泛关注。通过利用社交关系和群组行为，群组推荐（GR）旨在为一组用户而非个体提供更准确、更具吸引力的内容。近年来，影响力感知群组推荐（influence-aware GR）作为一个有前景的方向而兴起，因为它考虑了社交影响力对群组决策的影响。在我们之前的工作中，我们提出了影响力感知群组推荐（IGR）来解决这项任务。然而，这项任务仍然具有挑战性，原因在于三个关键因素：社交图谱规模庞大且不断增长、用户群组内影响力传播固有的动态性，以及实时群组-项目匹配的高计算开销。\n\n为了解决这些问题，我们提出了一个增强型影响力感知群组推荐（EIGR）框架。首先，我们引入了一种基于图提取的采样（GES）策略，以最小化跨多个时间社交图谱的冗余，并有效捕捉群组和项目的演变动态。其次，我们设计了一个新颖的动态独立级联（DYIC）模型，用于预测影响力如何随时间跨社交项目和用户群组传播。最后，我们开发了一个两级哈希用户群组索引（UG-Index），以有效组织用户群组并实现实时推荐生成。在真实世界数据集上进行的大量实验表明，我们提出的框架 EIGR 在有效性和效率两方面都始终优于最先进的基线。"
    },
    {
        "title": "DARTS: A Dual-View Attack Framework for Targeted Manipulation in\n  Federated Sequential Recommendation",
        "url": "http://arxiv.org/abs/2507.01383v1",
        "pub_date": "2025-07-02",
        "summary": "Federated recommendation (FedRec) preserves user privacy by enabling decentralized training of personalized models, but this architecture is inherently vulnerable to adversarial attacks. Significant research has been conducted on targeted attacks in FedRec systems, motivated by commercial and social influence considerations. However, much of this work has largely overlooked the differential robustness of recommendation models. Moreover, our empirical findings indicate that existing targeted attack methods achieve only limited effectiveness in Federated Sequential Recommendation(FSR) tasks. Driven by these observations, we focus on investigating targeted attacks in FSR and propose a novel dualview attack framework, named DV-FSR. This attack method uniquely combines a sampling-based explicit strategy with a contrastive learning-based implicit gradient strategy to orchestrate a coordinated attack. Additionally, we introduce a specific defense mechanism tailored for targeted attacks in FSR, aiming to evaluate the mitigation effects of the attack method we proposed. Extensive experiments validate the effectiveness of our proposed approach on representative sequential models. Our codes are publicly available.",
        "translated": "联邦推荐（FedRec）通过去中心化训练个性化模型来保护用户隐私，但其架构本质上容易受到对抗性攻击。针对FedRec系统中的定向攻击已进行了大量研究，其动机源于商业和社交流量考量。然而，其中大部分工作在很大程度上忽略了推荐模型的差异鲁棒性。此外，我们的实证研究发现，现有定向攻击方法在联邦序列推荐（FSR）任务中效果有限。\n\n受这些观察启发，我们着重研究FSR中的定向攻击，并提出了一个名为DV-FSR的新型双视角攻击框架。该攻击方法独特地结合了基于采样的显式策略和基于对比学习的隐式梯度策略，以协同发动攻击。此外，我们引入了一种针对FSR中定向攻击量身定制的防御机制，旨在评估其对我们所提攻击方法的缓解效果。广泛的实验验证了我们所提方法在代表性序列模型上的有效性。我们的代码已公开发布。"
    },
    {
        "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive\n  Benchmarks",
        "url": "http://arxiv.org/abs/2507.01297v1",
        "pub_date": "2025-07-02",
        "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited settings, such as factoid question answering; more challenging, reasoning-intensive benchmarks have seen limited success from minimal RAG. In this work, we challenge this prevailing view on established, reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We identify a key missing component in prior work: a usable, web-scale datastore aligned with the breadth of pretraining data. To this end, we introduce CompactDS: a diverse, high-quality, web-scale datastore that achieves high retrieval accuracy and subsecond latency on a single-node. The key insights are (1) most web content can be filtered out without sacrificing coverage, and a compact, high-quality subset is sufficient; and (2) combining in-memory approximate nearest neighbor (ANN) retrieval and on-disk exact search balances speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves consistent accuracy improvements across all benchmarks and model sizes (8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA, and 19% on MATH. No single data source suffices alone, highlighting the importance of diversity of sources (web crawls, curated math, academic papers, textbooks). Finally, we show that our carefully designed in-house datastore matches or outperforms web search engines such as Google Search, as well as recently proposed, complex agent-based RAG systems--all while maintaining simplicity, reproducibility, and self-containment. We release CompactDS and our retrieval pipeline, supporting future research exploring retrieval-based AI systems.",
        "translated": "检索增强生成（RAG）主要在有限场景下进行了研究，例如事实型问答；而更具挑战性、推理密集型基准上，极简RAG（minimal RAG）的表现却不尽如人意。在这项工作中，我们挑战了关于极简RAG在现有推理密集型基准（MMLU、MMLU Pro、AGI Eval、GPQA和MATH）上表现不佳的普遍看法。我们发现先前工作中缺少一个关键组件：一个可用、网络规模且与预训练数据内容范围相匹配的数据存储。\n\n为此，我们引入了CompactDS：一个多样化、高质量、网络规模的数据存储，它能在单节点上实现高检索精度和亚秒级延迟。其核心见解在于：(1) 大部分网络内容可以在不牺牲覆盖范围的情况下被过滤掉，一个紧凑、高质量的子集就已足够；(2) 结合内存近似最近邻（ANN）检索和磁盘精确搜索可以平衡速度与召回率。\n\n借助CompactDS，我们展示了极简RAG流水线在所有基准和不同模型规模（80亿至700亿参数）上均实现了持续的精度提升，在MMLU上取得了10%的相对增益，MMLU Pro上为33%，GPQA上为14%，MATH上为19%。没有单一数据源能独自满足需求，这凸显了数据源多样性（网络爬取数据、精选数学资料、学术论文、教科书）的重要性。最后，我们展示了我们精心设计的内部数据存储能媲美或超越谷歌搜索等网络搜索引擎，以及最近提出的复杂基于代理的RAG系统——同时保持了简单性、可复现性和自包含性。我们发布了CompactDS及其检索流水线，以支持未来对基于检索的AI系统的研究。"
    },
    {
        "title": "Far From Sight, Far From Mind: Inverse Distance Weighting for Graph\n  Federated Recommendation",
        "url": "http://arxiv.org/abs/2507.01285v1",
        "pub_date": "2025-07-02",
        "summary": "Graph federated recommendation systems offer a privacy-preserving alternative to traditional centralized recommendation architectures, which often raise concerns about data security. While federated learning enables personalized recommendations without exposing raw user data, existing aggregation methods overlook the unique properties of user embeddings in this setting. Indeed, traditional aggregation methods fail to account for their complexity and the critical role of user similarity in recommendation effectiveness. Moreover, evolving user interactions require adaptive aggregation while preserving the influence of high-relevance anchor users (the primary users before expansion in graph-based frameworks). To address these limitations, we introduce Dist-FedAvg, a novel distance-based aggregation method designed to enhance personalization and aggregation efficiency in graph federated learning. Our method assigns higher aggregation weights to users with similar embeddings, while ensuring that anchor users retain significant influence in local updates. Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg consistently outperforms baseline aggregation techniques, improving recommendation accuracy while maintaining seamless integration into existing federated learning frameworks.",
        "translated": "图联邦推荐系统为传统的中心化推荐架构提供了一种保护隐私的替代方案，后者常常引发数据安全担忧。尽管联邦学习能够在不暴露原始用户数据的情况下实现个性化推荐，但现有的聚合方法忽略了在此背景下用户嵌入的独特性质。事实上，传统聚合方法未能充分考虑其复杂性以及用户相似性在推荐效果中的关键作用。此外，不断演变的用户交互需要自适应聚合，同时还要保留高相关性锚点用户（图基框架中扩展前的原始用户）的影响力。为了解决这些局限性，我们引入了 Dist-FedAvg，这是一种新颖的基于距离的聚合方法，旨在增强图联邦学习中的个性化和聚合效率。我们的方法为具有相似嵌入的用户分配更高的聚合权重，同时确保锚点用户在本地更新中保持显著影响力。在多个数据集上的实证评估表明，Dist-FedAvg 始终优于基线聚合技术，在提高推荐准确性的同时，还能保持与现有联邦学习框架的无缝集成。"
    },
    {
        "title": "Towards a Signal Detection Based Measure for Assessing Information\n  Quality of Explainable Recommender Systems",
        "url": "http://arxiv.org/abs/2507.01168v1",
        "pub_date": "2025-07-01",
        "summary": "There is growing interest in explainable recommender systems that provide recommendations along with explanations for the reasoning behind them. When evaluating recommender systems, most studies focus on overall recommendation performance. Only a few assess the quality of the explanations. Explanation quality is often evaluated through user studies that subjectively gather users' opinions on representative explanatory factors that shape end-users' perspective towards the results, not about the explanation contents itself. We aim to fill this gap by developing an objective metric to evaluate Veracity: the information quality of explanations. Specifically, we decompose Veracity into two dimensions: Fidelity and Attunement. Fidelity refers to whether the explanation includes accurate information about the recommended item. Attunement evaluates whether the explanation reflects the target user's preferences. By applying signal detection theory, we first determine decision outcomes for each dimension and then combine them to calculate a sensitivity, which serves as the final Veracity value. To assess the effectiveness of the proposed metric, we set up four cases with varying levels of information quality to validate whether our metric can accurately capture differences in quality. The results provided meaningful insights into the effectiveness of our proposed metric.",
        "translated": "对提供推荐结果并解释其背后推理过程的**可解释推荐系统**（explainable recommender systems）的兴趣日益增长。在评估推荐系统时，大多数研究侧重于**整体推荐性能**，而只有少数研究会**评估解释的质量**。解释质量通常通过**用户研究**进行评估，这些研究主观地收集用户对影响最终用户看待推荐结果的**代表性解释因素**的看法，而非针对**解释内容本身**。\n\n为填补这一空白，我们旨在开发一个**客观指标**来评估**真实性（Veracity）**，即解释的**信息质量**。具体而言，我们将**真实性**分解为两个维度：**忠实度（Fidelity）**和**契合度（Attunement）**。**忠实度**指解释是否包含关于所推荐物品的**准确信息**。**契合度**评估解释是否**反映目标用户的偏好**。\n\n通过应用**信号检测理论（Signal Detection Theory, SDT）**，我们首先确定每个维度的**决策结果**，然后将它们组合起来计算一个**敏感度（sensitivity）**，该敏感度即作为最终的**真实性值**。为了评估所提出指标的有效性，我们设置了**四种不同信息质量水平的案例**，以验证我们的指标是否能**准确捕捉质量差异**。结果为我们所提出的指标的有效性提供了**有意义的见解**。"
    },
    {
        "title": "Digital Collections Explorer: An Open-Source, Multimodal Viewer for\n  Searching Digital Collections",
        "url": "http://arxiv.org/abs/2507.00961v1",
        "pub_date": "2025-07-01",
        "summary": "We present Digital Collections Explorer, a web-based, open-source exploratory search platform that leverages CLIP (Contrastive Language-Image Pre-training) for enhanced visual discovery of digital collections. Our Digital Collections Explorer can be installed locally and configured to run on a visual collection of interest on disk in just a few steps. Building upon recent advances in multimodal search techniques, our interface enables natural language queries and reverse image searches over digital collections with visual features. This paper describes the system's architecture, implementation, and application to various cultural heritage collections, demonstrating its potential for democratizing access to digital archives, especially those with impoverished metadata. We present case studies with maps, photographs, and PDFs extracted from web archives in order to demonstrate the flexibility of the Digital Collections Explorer, as well as its ease of use. We demonstrate that the Digital Collections Explorer scales to hundreds of thousands of images on a MacBook Pro with an M4 chip. Lastly, we host a public demo of Digital Collections Explorer.",
        "translated": "我们推出数字馆藏探索器（Digital Collections Explorer），这是一个基于网络的开源探索性搜索平台，它利用 CLIP（对比语言-图像预训练模型）来增强数字馆藏的视觉发现能力。该数字馆藏探索器可以本地安装，并仅需简单几步即可配置运行于磁盘上任意感兴趣的视觉藏品集。基于多模态搜索技术的最新进展，该界面支持对具有视觉特征的数字馆藏进行自然语言查询和反向图像搜索。本文描述了该系统的架构、实现及其在各种文化遗产馆藏中的应用，展示了其在促进数字档案访问民主化方面的潜力，特别是对于那些元数据匮乏的档案。我们展示了利用从网络档案中提取的地图、照片和PDF文件进行的案例研究，以证明数字馆藏探索器的灵活性和易用性。我们证明，数字馆藏探索器在搭载M4芯片的MacBook Pro上可扩展处理数十万张图像。最后，我们提供了数字馆藏探索器的公开演示。"
    },
    {
        "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2507.01915v1",
        "pub_date": "2025-07-02",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user's specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness.",
        "translated": "强化学习自人类反馈（RLHF）已成为使大型语言模型（LLMs）与人类偏好对齐的强大技术。然而，使LLMs与多样化的人类偏好有效对齐仍然是一个重大挑战，尤其当这些偏好相互冲突时。为解决此问题，我们将人类价值对齐建模为一个多目标优化问题，旨在最大化一组可能相互冲突的目标。\n\n我们引入了梯度自适应策略优化（Gradient-Adaptive Policy Optimization, GAPO），这是一种新颖的微调范式，它采用多梯度下降来使LLMs与多样化的偏好分布对齐。GAPO自适应地重新缩放每个目标的梯度，以确定一个能够最佳地平衡各目标间权衡的更新方向。此外，我们还提出了P-GAPO，它纳入了用户在不同目标上的偏好，并实现了能够更好地满足用户特定需求的帕累托解。我们的理论分析表明，GAPO收敛于多目标的帕累托最优解。在Mistral-7B上的实证结果显示，GAPO优于现有最先进（SOTA）方法，在有用性（helpfulness）和无害性（harmlessness）方面均取得了卓越性能。"
    },
    {
        "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2507.01915v1",
        "pub_date": "2025-07-02",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user's specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness.",
        "translated": "强化学习与人类反馈（RLHF）已发展成为一种强大的技术，用于使大语言模型（LLMs）与人类偏好对齐。然而，有效使大语言模型与多样化的人类偏好对齐仍是一个重大挑战，尤其当这些偏好相互冲突时。为解决此问题，我们将人类价值观对齐建模为一个多目标优化问题，旨在最大化一组可能相互冲突的目标。\n\n我们引入了梯度自适应策略优化（Gradient-Adaptive Policy Optimization, GAPO），这是一种新颖的微调范式，它采用多梯度下降来使大语言模型与多样化的偏好分布对齐。GAPO 自适应地重缩放每个目标的梯度，以确定一个能够最佳平衡各目标之间权衡的更新方向。此外，我们还引入了 P-GAPO，它融入了用户在不同目标上的偏好，从而实现了更符合用户特定需求的帕累托解。我们的理论分析表明，GAPO 能够收敛到针对多目标的帕累托最优解。在 Mistral-7B 上的实证结果表明，GAPO 优于当前最先进的方法，在有益性和无害性两方面均取得了卓越的性能。"
    },
    {
        "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System",
        "url": "http://arxiv.org/abs/2507.01872v1",
        "pub_date": "2025-07-02",
        "summary": "Existing language learning tools, even those powered by Large Language Models (LLMs), often lack support for polyglot learners to build linguistic connections across vocabularies in multiple languages, provide limited customization for individual learning paces or needs, and suffer from detrimental cognitive offloading. To address these limitations, we design Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system that supports polyglot language learning. DIY-MKG allows the user to build personalized vocabulary knowledge graphs, which are constructed by selective expansion with related words suggested by an LLM. The system further enhances learning through rich annotation capabilities and an adaptive review module that leverages LLMs for dynamic, personalized quiz generation. In addition, DIY-MKG allows users to flag incorrect quiz questions, simultaneously increasing user engagement and providing a feedback loop for prompt refinement. Our evaluation of LLM-based components in DIY-MKG shows that vocabulary expansion is reliable and fair across multiple languages, and that the generated quizzes are highly accurate, validating the robustness of DIY-MKG.",
        "translated": "现有的语言学习工具，即使是那些由大型语言模型（LLMs）驱动的工具，通常缺乏对多语种学习者的支持，无法帮助他们在多种语言的词汇之间建立语言联系；它们为个人学习进度或需求提供的个性化定制也十分有限；并且，它们还存在有害的认知卸载问题。为了解决这些局限性，我们设计了“自制多语种知识图谱”（DIY-MKG），这是一个支持多语种语言学习的开源系统。DIY-MKG允许用户构建个性化的词汇知识图谱，这些图谱通过LLM建议的相关词汇进行选择性扩展来构建。该系统通过丰富的注释功能和利用LLM进行动态、个性化测验生成的自适应复习模块，进一步增强了学习效果。此外，DIY-MKG允许用户标记错误的测验问题，这不仅提高了用户参与度，还为提示词的优化提供了反馈循环。我们对DIY-MKG中基于LLM的组件进行的评估表明，词汇扩展在多种语言中是可靠且公平的，并且生成的测验高度准确，验证了DIY-MKG的鲁棒性。"
    },
    {
        "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for\n  Deep Search",
        "url": "http://arxiv.org/abs/2507.02652v1",
        "pub_date": "2025-07-03",
        "summary": "Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.",
        "translated": "实际搜索场景中的复杂信息需求需要跨多种来源进行深度推理和知识综合，而传统检索增强生成（RAG）流水线难以有效应对。当前基于推理的方法存在一个根本局限：它们使用单一模型来同时处理高层规划和详细执行，这导致了推理效率低下和可扩展性有限。在本文中，我们引入了 HiRA，一个将战略规划与专业化执行分离的分层框架。我们的方法将复杂搜索任务分解为聚焦的子任务，将每个子任务分配给配备外部工具和推理能力的领域特定代理，并通过结构化集成机制协调结果。这种分离防止了执行细节干扰高层推理，同时使系统能够利用专业知识处理不同类型的信息。在四个复杂的、跨模态的深度搜索基准上进行的实验表明，HiRA 显著优于最先进的 RAG 和基于代理的系统。我们的结果表明在回答质量和系统效率方面都有所改进，突出了解耦规划和执行对于多步信息查询任务的有效性。我们的代码可在 https://github.com/ignorejjj/HiRA 获取。"
    },
    {
        "title": "Calibrated Recommendations: Survey and Future Directions",
        "url": "http://arxiv.org/abs/2507.02643v1",
        "pub_date": "2025-07-03",
        "summary": "The idea of calibrated recommendations is that the properties of the items that are suggested to users should match the distribution of their individual past preferences. Calibration techniques are therefore helpful to ensure that the recommendations provided to a user are not limited to a certain subset of the user's interests. Over the past few years, we have observed an increasing number of research works that use calibration for different purposes, including questions of diversity, biases, and fairness. In this work, we provide a survey on the recent developments in the area of calibrated recommendations. We both review existing technical approaches for calibration and provide an overview on empirical and analytical studies on the effectiveness of calibration for different use cases. Furthermore, we discuss limitations and common challenges when implementing calibration in practice.",
        "translated": "校准推荐的核心思想在于，推荐给用户的物品属性应与其个体过往偏好的分布相匹配。因此，校准技术有助于确保为用户提供的推荐不会局限于其兴趣的某一特定子集。在过去几年中，我们注意到越来越多的研究工作将校准应用于不同目的，如多样性、偏置和公平性等问题。\n\n本文对校准推荐领域的最新进展进行了综述。我们不仅回顾了现有的校准技术方法，还概述了关于校准在不同应用场景中有效性的实证和分析研究。此外，我们讨论了在校准的实际应用中存在的局限性与常见挑战。"
    },
    {
        "title": "Resolving CAP Through Automata-Theoretic Economic Design: A Unified\n  Mathematical Framework for Real-Time Partition-Tolerant Systems",
        "url": "http://arxiv.org/abs/2507.02464v1",
        "pub_date": "2025-07-03",
        "summary": "The CAP theorem asserts a trilemma between consistency, availability, and partition tolerance. This paper introduces a rigorous automata-theoretic and economically grounded framework that reframes the CAP trade-off as a constraint optimization problem. We model distributed systems as partition-aware state machines and embed economic incentive layers to stabilize consensus behavior across adversarially partitioned networks. By incorporating game-theoretic mechanisms into the global transition semantics, we define provable bounds on convergence, liveness, and correctness. Our results demonstrate that availability and consistency can be simultaneously preserved within bounded epsilon margins, effectively extending the classical CAP limits through formal economic control.",
        "translated": "CAP 定理断言一致性（Consistency）、可用性（Availability）和分区容错性（Partition Tolerance）之间存在三难困境。本文引入了一个严谨的、基于自动机理论和经济学基础的框架，将 CAP 权衡重新定义为一个约束优化问题。我们将分布式系统建模为分区感知状态机，并嵌入经济激励层，以稳定对抗性分区网络中的共识行为。通过将博弈论机制整合到全局转换语义中，我们定义了关于收敛性、活性和正确性的可证明界限。我们的结果表明，可用性和一致性可以在有界epsilon误差范围内同时保持，通过形式化经济控制有效拓展了经典的 CAP 限制。"
    },
    {
        "title": "Content filtering methods for music recommendation: A review",
        "url": "http://arxiv.org/abs/2507.02282v1",
        "pub_date": "2025-07-03",
        "summary": "Recommendation systems have become essential in modern music streaming platforms, shaping how users discover and engage with songs. One common approach in recommendation systems is collaborative filtering, which suggests content based on the preferences of users with similar listening patterns to the target user. However, this method is less effective on media where interactions are sparse. Music is one such medium, since the average user of a music streaming service will never listen to the vast majority of tracks. Due to this sparsity, there are several challenges that have to be addressed with other methods. This review examines the current state of research in addressing these challenges, with an emphasis on the role of content filtering in mitigating biases inherent in collaborative filtering approaches. We explore various methods of song classification for content filtering, including lyrical analysis using Large Language Models (LLMs) and audio signal processing techniques. Additionally, we discuss the potential conflicts between these different analysis methods and propose avenues for resolving such discrepancies.",
        "translated": "推荐系统在现代音乐流媒体平台中变得必不可少，它塑造了用户发现和互动歌曲的方式。推荐系统的一种常见方法是协同过滤，它根据与目标用户听歌模式相似的用户的偏好来推荐内容。然而，这种方法在交互稀疏的媒体上效果不佳。音乐就是这样一种媒体，因为音乐流媒体服务的普通用户绝大部分曲目都不会收听。由于这种稀疏性，存在一些必须通过其他方法解决的挑战。本综述探讨了解决这些挑战的当前研究现状，重点关注内容过滤在减轻协同过滤方法固有偏差方面的作用。我们探讨了用于内容过滤的各种歌曲分类方法，包括使用大型语言模型（LLMs）进行歌词分析和音频信号处理技术。此外，我们讨论了这些不同分析方法之间潜在的冲突，并提出了解决此类差异的途径。"
    },
    {
        "title": "Listwise Preference Alignment Optimization for Tail Item Recommendation",
        "url": "http://arxiv.org/abs/2507.02255v1",
        "pub_date": "2025-07-03",
        "summary": "Preference alignment has achieved greater success on Large Language Models (LLMs) and drawn broad interest in recommendation research. Existing preference alignment methods for recommendation either require explicit reward modeling or only support pairwise preference comparison. The former directly increases substantial computational costs, while the latter hinders training efficiency on negative samples. Moreover, no existing effort has explored preference alignment solutions for tail-item recommendation. To bridge the above gaps, we propose LPO4Rec, which extends the Bradley-Terry model from pairwise comparison to listwise comparison, to improve the efficiency of model training. Specifically, we derive a closed form optimal policy to enable more efficient and effective training without explicit reward modeling. We also present an adaptive negative sampling and reweighting strategy to prioritize tail items during optimization and enhance performance in tail-item recommendations. Besides, we theoretically prove that optimizing the listwise preference optimization (LPO) loss is equivalent to maximizing the upper bound of the optimal reward. Our experiments on three public datasets show that our method outperforms 10 baselines by a large margin, achieving up to 50% performance improvement while reducing 17.9% GPU memory usage when compared with direct preference optimization (DPO) in tail-item recommendation. Our code is available at https://github.com/Yuhanleeee/LPO4Rec.",
        "translated": "偏好对齐在大型语言模型（LLM）上取得了显著成功，并在推荐系统研究中引起了广泛兴趣。现有推荐系统中的偏好对齐方法要么需要显式奖励建模，要么只支持成对偏好比较。前者直接增加了巨大的计算成本，而后者则阻碍了在负样本上的训练效率。此外，现有工作尚未探索针对长尾商品推荐的偏好对齐解决方案。\n\n为了弥补上述空白，我们提出了LPO4Rec，该方法将Bradley-Terry模型从成对比较扩展到列表式比较，以提高模型训练效率。具体来说，我们推导了一个闭式最优策略，以实现在无需显式奖励建模的情况下进行更高效且有效的训练。我们还提出了一种自适应负采样和重加权策略，以在优化过程中优先考虑长尾商品，并提升长尾商品推荐的性能。此外，我们理论证明，优化列表式偏好优化（LPO）损失等价于最大化最优奖励的上限。\n\n我们在三个公开数据集上的实验表明，我们的方法大幅超越了10个基线模型，性能提升高达50%，同时在长尾商品推荐方面与直接偏好优化（DPO）相比，降低了17.9%的GPU内存使用。我们的代码已在 https://github.com/Yuhanleeee/LPO4Rec 公开。"
    },
    {
        "title": "When LLMs Disagree: Diagnosing Relevance Filtering Bias and Retrieval\n  Divergence in SDG Search",
        "url": "http://arxiv.org/abs/2507.02139v1",
        "pub_date": "2025-07-02",
        "summary": "Large language models (LLMs) are increasingly used to assign document relevance labels in information retrieval pipelines, especially in domains lacking human-labeled data. However, different models often disagree on borderline cases, raising concerns about how such disagreement affects downstream retrieval. This study examines labeling disagreement between two open-weight LLMs, LLaMA and Qwen, on a corpus of scholarly abstracts related to Sustainable Development Goals (SDGs) 1, 3, and 7. We isolate disagreement subsets and examine their lexical properties, rank-order behavior, and classification predictability. Our results show that model disagreement is systematic, not random: disagreement cases exhibit consistent lexical patterns, produce divergent top-ranked outputs under shared scoring functions, and are distinguishable with AUCs above 0.74 using simple classifiers. These findings suggest that LLM-based filtering introduces structured variability in document retrieval, even under controlled prompting and shared ranking logic. We propose using classification disagreement as an object of analysis in retrieval evaluation, particularly in policy-relevant or thematic search tasks.",
        "translated": "大型语言模型（LLM）正越来越多地被用于在信息检索流程中分配文档相关性标签，尤其是在缺乏人工标注数据的领域。然而，不同模型在边缘案例上经常存在分歧，这引发了人们对这种分歧如何影响下游检索的担忧。本研究考察了两个开放权重LLM（LLaMA和Qwen）在与可持续发展目标（SDG）1、3和7相关的学术摘要语料库上的标签分歧。我们分离出分歧子集，并考察它们的词汇特性、排名行为和分类可预测性。我们的结果表明，模型分歧是系统性的，而非随机的：分歧案例表现出一致的词汇模式，在共同的评分函数下产生分歧的顶部排名输出，并且可以使用简单分类器以高于0.74的AUC值进行区分。这些发现表明，即使在受控提示和共享排名逻辑下，基于LLM的过滤也会在文档检索中引入结构化变异性。我们建议将分类分歧作为检索评估中的一个分析对象，尤其是在政策相关或主题搜索任务中。"
    },
    {
        "title": "The Future is Agentic: Definitions, Perspectives, and Open Challenges of\n  Multi-Agent Recommender Systems",
        "url": "http://arxiv.org/abs/2507.02097v1",
        "pub_date": "2025-07-02",
        "summary": "Large language models (LLMs) are rapidly evolving from passive engines of text generation into agentic entities that can plan, remember, invoke external tools, and co-operate with one another. This perspective paper investigates how such LLM agents (and societies thereof) can transform the design space of recommender systems.   We introduce a unified formalism that (i) models an individual agent as a tuple comprising its language core, tool set, and hierarchical memory, and (ii) captures a multi-agent recommender as a triple of agents, shared environment, and communication protocol. Within this framework, we present four end-to-end use cases-interactive party planning, synthetic user-simulation for offline evaluation, multi-modal furniture recommendation, and brand-aligned explanation generation-each illustrating a distinct capability unlocked by agentic orchestration.   We then surface five cross-cutting challenge families: protocol complexity, scalability, hallucination and error propagation, emergent misalignment (including covert collusion), and brand compliance.   For each, we formalize the problem, review nascent mitigation strategies, and outline open research questions. The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools that keep pace with this new degree of autonomy. By unifying agentic abstractions with recommender objectives, the paper lays the groundwork for the next generation of personalized, trustworthy, and context-rich recommendation services.",
        "translated": "大型语言模型（LLM）正从被动的文本生成引擎迅速发展为具备规划、记忆、调用外部工具及相互协作能力的智能体实体。本视角论文探讨了这类LLM智能体（及其群落）如何重塑推荐系统的设计空间。\n\n我们引入了一个统一的形式化框架，该框架（i）将单个智能体建模为由其语言核心、工具集和分层记忆组成的元组，以及（ii）将多智能体推荐系统建模为由智能体、共享环境和通信协议构成的三元组。在此框架内，我们提出了四个端到端用例——交互式派对规划、用于离线评估的合成用户模拟、多模态家具推荐和品牌对齐的解释生成——每个用例都展示了智能体编排所解锁的独特能力。\n\n随后，我们提出了五大跨领域挑战：协议复杂性、可伸缩性、幻觉与错误传播、涌现的错位（包括隐蔽串通）以及品牌合规性。针对每个挑战，我们都对其问题进行了形式化描述，回顾了新兴的缓解策略，并概述了开放的研究问题。本文成果既是一份蓝图，也是一份议程：一份展示如何将记忆增强、工具使用的LLM智能体组合成稳健推荐管道的蓝图；一份邀请推荐系统社区开发基准、理论保证和治理工具以与这种新程度的自主性发展同步的议程。通过将智能体抽象与推荐目标相结合，本文为下一代个性化、可信赖且上下文丰富的推荐服务奠定了基础。"
    },
    {
        "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large\n  Language Model",
        "url": "http://arxiv.org/abs/2507.02822v1",
        "pub_date": "2025-07-03",
        "summary": "With the widespread adoption of large language models (LLMs) in practical applications, selecting an appropriate model requires balancing not only performance but also operational cost. The emergence of reasoning-capable models has further widened the cost gap between \"thinking\" (high reasoning) and \"non-thinking\" (fast, low-cost) modes. In this work, we reveal that approximately 58% of medical questions can be accurately answered by the non-thinking mode alone, without requiring the high-cost reasoning process. This highlights a clear dichotomy in problem complexity and suggests that dynamically routing queries to the appropriate mode based on complexity could optimize accuracy, cost-efficiency, and overall user experience. Based on this, we further propose SynapseRoute, a machine learning-based dynamic routing framework that intelligently assigns input queries to either thinking or non-thinking modes. Experimental results on several medical datasets demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs. 0.8272) compared to the thinking mode alone but also reduces inference time by 36.8% and token consumption by 39.66%. Importantly, qualitative analysis indicates that over-reasoning on simpler queries can lead to unnecessary delays and even decreased accuracy, a pitfall avoided by our adaptive routing. Finally, this work further introduces the Accuracy-Inference-Token (AIT) index to comprehensively evaluate the trade-offs among accuracy, latency, and token cost.",
        "translated": "随着大型语言模型（LLMs）在实际应用中的广泛普及，选择合适的模型不仅需要平衡性能，还需要考虑运营成本。具备推理能力的模型出现后，进一步拉大了“思考型”（高推理）和“非思考型”（快速、低成本）模式之间的成本差距。在这项工作中，我们发现大约58%的医学问题仅凭非思考型模式即可准确回答，无需启动高成本的推理过程。这突出表明了问题复杂性存在清晰的二元性，并暗示基于复杂性将查询动态路由到适当模式，能够优化准确性、成本效益和整体用户体验。\n\n基于此，我们进一步提出了SynapseRoute，这是一个基于机器学习的动态路由框架，能够智能地将输入查询分配给思考型或非思考型模式。在多个医学数据集上的实验结果表明，与仅使用思考型模式相比，SynapseRoute不仅提高了整体准确性（0.8390 对 0.8272），而且将推理时间缩短了36.8%，并将token消耗降低了39.66%。重要的是，定性分析表明，对简单查询进行过度推理可能导致不必要的延迟甚至降低准确性，而我们的自适应路由则避免了这一缺陷。最后，这项工作还引入了准确性-推理-Token（AIT）指标，以全面评估准确性、延迟和Token成本之间的权衡。"
    },
    {
        "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge\n  Injection to All Users",
        "url": "http://arxiv.org/abs/2507.02850v1",
        "pub_date": "2025-07-03",
        "summary": "We describe a vulnerability in language models (LMs) trained with user feedback, whereby a single user can persistently alter LM knowledge and behavior given only the ability to provide prompts and upvote / downvote feedback on LM outputs. To implement the attack, the attacker prompts the LM to stochastically output either a \"poisoned\" or benign response, then upvotes the poisoned response or downvotes the benign one. When feedback signals are used in a subsequent preference tuning behavior, LMs exhibit increased probability of producing poisoned responses even in contexts without malicious prompts. We show that this attack can be used to (1) insert factual knowledge the model did not previously possess, (2) modify code generation patterns in ways that introduce exploitable security flaws, and (3) inject fake financial news. Our finding both identifies a new qualitative feature of language model preference tuning (showing that it even highly restricted forms of preference data can be used to exert fine-grained control over behavior), and a new attack mechanism for LMs trained with user feedback (extending work on pretraining-time data poisoning and deployment-time prompt injection).",
        "translated": "我们描述了一种在通过用户反馈训练的语言模型（LM）中存在的漏洞，使得单个用户仅通过提供提示以及对LM输出进行赞成/反对反馈的能力，就能持续改变LM的知识和行为。为实现该攻击，攻击者提示LM随机生成“投毒”或良性响应，然后赞成投毒响应，或反对良性响应。当反馈信号被用于后续的偏好微调时，LM即使在没有恶意提示的上下文中，也会表现出生成投毒响应的更高概率。我们表明这种攻击可用于：(1) 插入模型之前不具备的事实知识；(2) 修改代码生成模式，以引入可利用的安全漏洞；以及 (3) 注入虚假金融新闻。我们的发现既揭示了语言模型偏好微调的一个新的定性特征（表明即使是高度受限的偏好数据形式也可用于对行为施加细粒度控制），也提出了一种针对通过用户反馈训练的LM的新型攻击机制（扩展了预训练时数据投毒和部署时提示注入的工作）。"
    },
    {
        "title": "In-Context Learning as an Effective Estimator of Functional Correctness\n  of LLM-Generated Code",
        "url": "http://arxiv.org/abs/2507.05200v1",
        "pub_date": "2025-07-07",
        "summary": "When applying LLM-based code generation to software development projects that follow a feature-driven or rapid application development approach, it becomes necessary to estimate the functional correctness of the generated code in the absence of test cases. Just as a user selects a relevant document from a ranked list of retrieved ones, a software generation workflow requires a developer to choose (and potentially refine) a generated solution from a ranked list of alternative solutions, ordered by their posterior likelihoods. This implies that estimating the quality of a ranked list -- akin to estimating \"relevance\" for query performance prediction (QPP) in IR -- is also crucial for generative software development, where quality is defined in terms of \"functional correctness\". In this paper, we propose an in-context learning (ICL) based approach for code quality estimation. Our findings demonstrate that providing few-shot examples of functionally correct code from a training set enhances the performance of existing QPP approaches as well as a zero-shot-based approach for code quality estimation.",
        "translated": "在将基于大模型的代码生成应用于遵循功能驱动或快速应用开发方法的软件开发项目时，在缺少测试用例的情况下，估算生成代码的功能正确性变得至关重要。正如用户从检索到的排序结果列表中选择相关文档一样，软件生成工作流也要求开发者从一个依照后验概率排序的备选解决方案列表中，选择（并可能细化）一个生成的解决方案。这表明，估算排序列表的质量——类似于信息检索 (IR) 中为查询性能预测 (QPP) 估算“相关性”——对于生成式软件开发也至关重要，其中质量被定义为“功能正确性”。在本文中，我们提出了一种基于上下文学习 (ICL) 的代码质量估算方法。我们的研究结果表明，从训练集中提供功能正确的少样本代码示例，可以提升现有 QPP 方法以及基于零样本的代码质量估算方法的性能。"
    },
    {
        "title": "Do We Really Need Specialization? Evaluating Generalist Text Embeddings\n  for Zero-Shot Recommendation and Search",
        "url": "http://arxiv.org/abs/2507.05006v2",
        "pub_date": "2025-07-07",
        "summary": "Pre-trained language models (PLMs) are widely used to derive semantic representations from item metadata in recommendation and search. In sequential recommendation, PLMs enhance ID-based embeddings through textual metadata, while in product search, they align item characteristics with user intent. Recent studies suggest task and domain-specific fine-tuning are needed to improve representational power. This paper challenges this assumption, showing that Generalist Text Embedding Models (GTEs), pre-trained on large-scale corpora, can guarantee strong zero-shot performance without specialized adaptation. Our experiments demonstrate that GTEs outperform traditional and fine-tuned models in both sequential recommendation and product search. We attribute this to a superior representational power, as they distribute features more evenly across the embedding space. Finally, we show that compressing embedding dimensions by focusing on the most informative directions (e.g., via PCA) effectively reduces noise and improves the performance of specialized models. To ensure reproducibility, we provide our repository at https://split.to/gte4ps.",
        "translated": "预训练语言模型（PLMs）被广泛应用于从推荐和搜索中的物品元数据中提取语义表示。在序列推荐中，PLMs通过文本元数据增强了基于ID的嵌入；而在商品搜索中，它们将物品特征与用户意图对齐。近期研究表明，需要进行任务和领域特定的微调以提高表示能力。本文挑战了这一假设，表明在大型语料库上预训练的通用文本嵌入模型（GTEs）无需专门适应即可保证强大的零样本性能。我们的实验表明，GTEs在序列推荐和商品搜索中均优于传统模型和微调模型。我们将此归因于它们更优越的表示能力，因为它们在嵌入空间中更均匀地分布特征。最后，我们表明，通过关注信息量最大的方向（例如，通过PCA）来压缩嵌入维度，可以有效减少噪声并提高专门模型的性能。为确保可复现性，我们提供了我们的代码库：https://split.to/gte4ps。"
    },
    {
        "title": "Interest Networks (iNETs) for Cities: Cross-Platform Insights and Urban\n  Behavior Explanations",
        "url": "http://arxiv.org/abs/2507.04995v1",
        "pub_date": "2025-07-07",
        "summary": "Location-Based Social Networks (LBSNs) provide a rich foundation for modeling urban behavior through iNETs (Interest Networks), which capture how user interests are distributed throughout urban spaces. This study compares iNETs across platforms (Google Places and Foursquare) and spatial granularities, showing that coarser levels reveal more consistent cross-platform patterns, while finer granularities expose subtle, platform-specific behaviors. Our analysis finds that, in general, user interest is primarily shaped by geographic proximity and venue similarity, while socioeconomic and political contexts play a lesser role. Building on these insights, we develop a multi-level, explainable recommendation system that predicts high-interest urban regions for different user types. The model adapts to behavior profiles -- such as explorers, who are driven by proximity, and returners, who prefer familiar venues -- and provides natural-language explanations using explainable AI (XAI) techniques. To support our approach, we introduce h3-cities, a tool for multi-scale spatial analysis, and release a public demo for interactively exploring personalized urban recommendations. Our findings contribute to urban mobility research by providing scalable, context-aware, and interpretable recommendation systems.",
        "translated": "定位社交网络（LBSNs）为通过兴趣网络（iNETs）建模城市行为提供了丰富的土壤，iNETs能够捕捉用户兴趣在城市空间中的分布方式。本研究比较了不同平台（如Google Places和Foursquare）及不同空间粒度下的iNETs，结果表明，较粗的粒度揭示了更一致的跨平台模式，而较细的粒度则暴露了微妙的、特定于平台的行为。我们的分析发现，总体而言，用户兴趣主要受地理邻近性和场所相似性的影响，而社会经济和政治背景则作用较小。\n\n基于这些洞察，我们开发了一个多层次、可解释的推荐系统，能够预测不同用户类型的高兴趣城市区域。该模型能够适应不同的行为画像——例如受邻近性驱动的“探索者”和偏好熟悉场所的“回头客”——并利用可解释人工智能（XAI）技术提供自然语言解释。为支持我们的方法，我们引入了h3-cities——一个用于多尺度空间分析的工具，并发布了一个公开演示，供用户交互式探索个性化城市推荐。我们的研究结果通过提供可伸缩、上下文感知且可解释的推荐系统，为城市流动性研究做出了贡献。"
    },
    {
        "title": "SIGIR 2025 -- LiveRAG Challenge Report",
        "url": "http://arxiv.org/abs/2507.04942v2",
        "pub_date": "2025-07-07",
        "summary": "The LiveRAG Challenge at SIGIR 2025, held between March and May 2025, provided a competitive platform for advancing Retrieval-Augmented Generation (RAG) technologies. Participants from academia and industry were invited to develop a RAG-based question-answering system using a fixed corpus (Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal was to facilitate challenging comparisons of retrieval and prompting strategies. During the Live Challenge Day, 70 teams from 27 different countries provided answers and supportive information to 500 unseen questions within a strict two-hour time window. Evaluation was conducted in two stages: first an automated LLM-as-a-judge approach was used to compute correctness and faithfulness score, then a manual review of top ranked submissions was conducted. The finalists were announced on June 12, 2025, with prizes awarded during the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.",
        "translated": "SIGIR 2025 LiveRAG 挑战赛于2025年3月至5月期间举行，为推动检索增强生成（RAG）技术的发展提供了一个竞争平台。挑战赛邀请了来自学术界和工业界的参与者，利用固定的语料库（Fineweb-10BT）和一个通用的开源大型语言模型（Falcon3-10B-Instruct）来开发基于RAG的问答系统。其目标是促进对检索和提示策略的深入比较。在现场挑战日，来自27个不同国家的70支团队在严格的两小时内，为500个未曾见过的问题提供了答案和支持信息。评估分两个阶段进行：首先采用自动化的大型语言模型作为评委（LLM-as-a-judge）的方法计算答案的正确性和忠实度分数，随后对排名靠前的提交进行了人工审核。决赛入围者于2025年6月12日公布，奖项在意大利帕多瓦举行的SIGIR 2025 LiveRAG 研讨会期间颁发。"
    },
    {
        "title": "SimLab: A Platform for Simulation-based Evaluation of Conversational\n  Information Access Systems",
        "url": "http://arxiv.org/abs/2507.04888v1",
        "pub_date": "2025-07-07",
        "summary": "Research on interactive and conversational information access systems, including search engines, recommender systems, and conversational assistants, has been hindered by the difficulty in evaluating such systems with reproducible experiments. User simulation provides a promising solution, but there is a lack of infrastructure and tooling to support this kind of evaluation. To facilitate simulation-based evaluation of conversational information access systems, we introduce SimLab, the first cloud-based platform to provide a centralized general solution for the community to benchmark both conversational systems and user simulators in a controlled and reproducible environment. We articulate requirements for such a platform and propose a general infrastructure to address these requirements. We then present the design and implementation of an initial version of SimLab and showcase its features with an initial evaluation task of conversational movie recommendation, which is made publicly available. Furthermore, we discuss the sustainability of the platform and its future opportunities. This paper is a call for the community to contribute to the platform to drive progress in the field of conversational information access and user simulation.",
        "translated": "鉴于在可复现实验中评估交互式会话信息访问系统（包括搜索引擎、推荐系统和会话助手）存在困难，相关研究进展一直受到阻碍。用户模拟提供了一个有前景的解决方案，但目前缺乏支持此类评估的基础设施和工具。为了促进基于模拟的会话信息访问系统评估，我们推出了SimLab，这是首个基于云的平台，旨在为社区提供一个集中式通用解决方案，以便在受控且可复现的环境中对会话系统和用户模拟器进行基准测试。本文阐明了对此类平台的需求，并提出了一个通用基础设施来满足这些需求。接着，我们介绍了SimLab初始版本的设计与实现，并通过一项已公开的会话式电影推荐初步评估任务展示了其功能。此外，我们还讨论了平台的可持续性及其未来发展机遇。本文呼吁社区为该平台贡献力量，以共同推动会话信息访问和用户模拟领域的进步。"
    },
    {
        "title": "Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking\n  Distillation",
        "url": "http://arxiv.org/abs/2507.04820v1",
        "pub_date": "2025-07-07",
        "summary": "While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is one of the most effective zero-shot document ranking methods, it has a quadratic computational complexity with respect to the number of documents to be ranked, as it requires an enumeration over all possible document pairs. Consequently, the outstanding ranking performance of PRP has remained unreachable for most real-world ranking applications.   In this work, we propose to harness the effectiveness of PRP through pairwise distillation. Specifically, we distill a pointwise student ranker from pairwise teacher labels generated by PRP, resulting in an efficient student model that retains the performance of PRP with substantially lower computational costs. Furthermore, we find that the distillation process can be made sample-efficient: with only 2% of pairs, we are able to obtain the same performance as using all pairs for teacher labels. Thus, our novel approach provides a solution to harness the ranking performance of PRP without incurring high computational costs during both distillation and serving.",
        "translated": "尽管大语言模型（LLMs）的对偶排序提示（Pairwise Ranking Prompting, PRP）是目前最有效的零样本文档排序方法之一，但它对需要排序的文档数量具有二次计算复杂度，因为它需要枚举所有可能的文档对。因此，PRP 出色的排序性能在大多数实际排序应用中仍难以企及。\n\n在这项工作中，我们提出通过对偶蒸馏来利用PRP的有效性。具体而言，我们从PRP生成的对偶教师标签中蒸馏出一个逐点学生排序器，从而得到一个高效的学生模型，该模型保留了PRP的性能，同时计算成本大幅降低。此外，我们发现蒸馏过程可以做到样本高效：仅使用2%的对，我们就能获得与使用所有对作为教师标签相同的性能。因此，我们的新颖方法提供了一种解决方案，可以在蒸馏和部署（serving）阶段，在不产生高昂计算成本的情况下，利用PRP的排序性能。"
    },
    {
        "title": "\"This Suits You the Best\": Query Focused Comparative Explainable\n  Summarization",
        "url": "http://arxiv.org/abs/2507.04733v1",
        "pub_date": "2025-07-07",
        "summary": "Product recommendations inherently involve comparisons, yet traditional opinion summarization often fails to provide holistic comparative insights. We propose the novel task of generating Query-Focused Comparative Explainable Summaries (QF-CES) using Multi-Source Opinion Summarization (M-OS). To address the lack of query-focused recommendation datasets, we introduce MS-Q2P, comprising 7,500 queries mapped to 22,500 recommended products with metadata. We leverage Large Language Models (LLMs) to generate tabular comparative summaries with query-specific explanations. Our approach is personalized, privacy-preserving, recommendation engine-agnostic, and category-agnostic. M-OS as an intermediate step reduces inference latency approximately by 40% compared to the direct input approach (DIA), which processes raw data directly. We evaluate open-source and proprietary LLMs for generating and assessing QF-CES. Extensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity, faithfulness, informativeness, format adherence, and query relevance) showed an average Spearman correlation of 0.74 with human judgments, indicating its potential for QF-CES evaluation.",
        "translated": "产品推荐天然地包含比较，然而传统的意见摘要往往无法提供全面的比较性洞察。我们提出了一项新颖的任务：利用多源意见摘要 (M-OS) 生成查询导向的比较性可解释摘要 (QF-CES)。为解决查询导向推荐数据集的缺乏，我们引入了 MS-Q2P，该数据集包含 7,500 个查询，这些查询映射到 22,500 个带有元数据的推荐产品。我们利用大语言模型 (LLMs) 生成带有查询相关解释的表格形式比较摘要。我们的方法具有个性化、隐私保护、推荐引擎无关和类别无关的特点。M-OS 作为中间步骤，与直接处理原始数据的直接输入方法 (DIA) 相比，将推理延迟大约减少了 40%。我们评估了开源和专有 LLMs 在生成和评估 QF-CES 方面的表现。使用 QF-CES-PROMPT 在 5 个维度（清晰度、忠实性、信息量、格式依从性和查询相关性）上进行的广泛评估显示，其与人工判断的平均 Spearman 相关性达到 0.74，表明其在 QF-CES 评估中具有巨大潜力。"
    },
    {
        "title": "FindRec: Stein-Guided Entropic Flow for Multi-Modal Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2507.04651v1",
        "pub_date": "2025-07-07",
        "summary": "Modern recommendation systems face significant challenges in processing multimodal sequential data, particularly in temporal dynamics modeling and information flow coordination. Traditional approaches struggle with distribution discrepancies between heterogeneous features and noise interference in multimodal signals. We propose \\textbf{FindRec}~ (\\textbf{F}lexible unified \\textbf{in}formation \\textbf{d}isentanglement for multi-modal sequential \\textbf{Rec}ommendation), introducing a novel \"information flow-control-output\" paradigm. The framework features two key innovations: (1) A Stein kernel-based Integrated Information Coordination Module (IICM) that theoretically guarantees distribution consistency between multimodal features and ID streams, and (2) A cross-modal expert routing mechanism that adaptively filters and combines multimodal features based on their contextual relevance. Our approach leverages multi-head subspace decomposition for routing stability and RBF-Stein gradient for unbiased distribution alignment, enhanced by linear-complexity Mamba layers for efficient temporal modeling. Extensive experiments on three real-world datasets demonstrate FindRec's superior performance over state-of-the-art baselines, particularly in handling long sequences and noisy multimodal inputs. Our framework achieves both improved recommendation accuracy and enhanced model interpretability through its modular design. The implementation code is available anonymously online for easy reproducibility~\\footnote{https://github.com/Applied-Machine-Learning-Lab/FindRec}.",
        "translated": "现代推荐系统在处理多模态序列数据方面面临严峻挑战，尤其是在时序动态建模和信息流协调方面。传统方法难以处理异构特征之间的分布差异以及多模态信号中的噪声干扰。我们提出了**FindRec**（**F**lexible unified **in**formation **d**isentanglement for multi-modal sequential **Rec**ommendation，即“多模态序列推荐中的灵活统一信息解耦”），引入了一种新颖的“信息流-控制-输出”范式。\n\n该框架包含两项关键创新：(1) 一个基于Stein核的集成信息协调模块（IICM），在理论上保证了多模态特征与ID流之间的分布一致性；(2) 一个跨模态专家路由机制，能够基于多模态特征的上下文相关性自适应地过滤和组合它们。我们的方法利用多头子空间分解以确保路由的稳定性，并利用RBF-Stein梯度实现无偏分布对齐，同时通过线性复杂度的Mamba层增强了高效的时序建模能力。\n\n在三个真实世界数据集上进行的大量实验表明，FindRec在处理长序列和噪声多模态输入方面表现尤为突出，性能超越了最先进的基线模型。我们的框架通过其模块化设计，同时实现了推荐准确性的提升和模型可解释性的增强。实施代码已在线匿名提供，方便复现。"
    },
    {
        "title": "Heterogeneous User Modeling for LLM-based Recommendation",
        "url": "http://arxiv.org/abs/2507.04626v1",
        "pub_date": "2025-07-07",
        "summary": "Leveraging Large Language Models (LLMs) for recommendation has demonstrated notable success in various domains, showcasing their potential for open-domain recommendation. A key challenge to advancing open-domain recommendation lies in effectively modeling user preferences from users' heterogeneous behaviors across multiple domains. Existing approaches, including ID-based and semantic-based modeling, struggle with poor generalization, an inability to compress noisy interactions effectively, and the domain seesaw phenomenon. To address these challenges, we propose a Heterogeneous User Modeling (HUM) method, which incorporates a compression enhancer and a robustness enhancer for LLM-based recommendation. The compression enhancer uses a customized prompt to compress heterogeneous behaviors into a tailored token, while a masking mechanism enhances cross-domain knowledge extraction and understanding. The robustness enhancer introduces a domain importance score to mitigate the domain seesaw phenomenon by guiding domain optimization. Extensive experiments on heterogeneous datasets validate that HUM effectively models user heterogeneity by achieving both high efficacy and robustness, leading to superior performance in open-domain recommendation.",
        "translated": "大语言模型（LLM）在推荐领域的应用已在多个领域取得了显著成功，展现了它们在开放域推荐方面的巨大潜力。推动开放域推荐发展的关键挑战之一在于如何有效地从用户在多个领域中的异构行为中建模用户偏好。现有方法，包括基于ID和基于语义的建模，普遍存在泛化能力差、难以有效压缩噪声交互以及领域跷跷板现象等问题。为解决这些挑战，我们提出了一种异构用户建模（HUM）方法，该方法为基于LLM的推荐引入了压缩增强器和鲁棒性增强器。压缩增强器通过定制化提示词将异构行为压缩成一个定制化token，同时引入掩码机制以增强跨领域知识的提取与理解。鲁棒性增强器则引入了领域重要性分数，通过引导领域优化来缓解领域跷跷板现象。在异构数据集上进行的大量实验验证了HUM通过实现高有效性（efficacy）和鲁棒性，能够有效地建模用户异构性，从而在开放域推荐中取得了卓越的性能。"
    },
    {
        "title": "Hierarchical Intent-guided Optimization with Pluggable LLM-Driven\n  Semantics for Session-based Recommendation",
        "url": "http://arxiv.org/abs/2507.04623v1",
        "pub_date": "2025-07-07",
        "summary": "Session-based Recommendation (SBR) aims to predict the next item a user will likely engage with, using their interaction sequence within an anonymous session. Existing SBR models often focus only on single-session information, ignoring inter-session relationships and valuable cross-session insights. Some methods try to include inter-session data but struggle with noise and irrelevant information, reducing performance. Additionally, most models rely on item ID co-occurrence and overlook rich semantic details, limiting their ability to capture fine-grained item features. To address these challenges, we propose a novel hierarchical intent-guided optimization approach with pluggable LLM-driven semantic learning for session-based recommendations, called HIPHOP. First, we introduce a pluggable embedding module based on large language models (LLMs) to generate high-quality semantic representations, enhancing item embeddings. Second, HIPHOP utilizes graph neural networks (GNNs) to model item transition relationships and incorporates a dynamic multi-intent capturing module to address users' diverse interests within a session. Additionally, we design a hierarchical inter-session similarity learning module, guided by user intent, to capture global and local session relationships, effectively exploring users' long-term and short-term interests. To mitigate noise, an intent-guided denoising strategy is applied during inter-session learning. Finally, we enhance the model's discriminative capability by using contrastive learning to optimize session representations. Experiments on multiple datasets show that HIPHOP significantly outperforms existing methods, demonstrating its effectiveness in improving recommendation quality. Our code is available: https://github.com/hjx159/HIPHOP.",
        "translated": "会话推荐（Session-based Recommendation, SBR）旨在利用用户在匿名会话中的交互序列，预测用户下一个可能感兴趣的物品。现有SBR模型通常只关注单会话信息，忽略了会话间关系和有价值的跨会话洞察。尽管一些方法尝试纳入会话间数据，但它们往往受噪声和无关信息困扰，从而降低了性能。此外，大多数模型依赖物品ID共现，忽略了丰富的语义细节，限制了其捕获细粒度物品特征的能力。\n\n为解决这些挑战，我们提出了一种新颖的、面向会话推荐的分层意图引导优化方法，并引入了可插拔的LLM驱动语义学习，命名为HIPHOP。首先，我们引入了一个基于大型语言模型（LLM）的可插拔嵌入模块，用于生成高质量的语义表示，从而增强物品嵌入。其次，HIPHOP利用图神经网络（GNN）建模物品转换关系，并结合一个动态多意图捕获模块，以处理用户在会话内的多样化兴趣。此外，我们设计了一个由用户意图引导的分层会话间相似性学习模块，以捕获全局和局部会话关系，有效探索用户的长期和短期兴趣。为缓解噪声，在会话间学习过程中应用了意图引导去噪策略。最后，我们通过使用对比学习优化会话表示，增强了模型的判别能力。在多个数据集上的实验表明，HIPHOP显著优于现有方法，证明了其在提升推荐质量方面的有效性。我们的代码已开源：https://github.com/hjx159/HIPHOP。"
    },
    {
        "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions",
        "url": "http://arxiv.org/abs/2507.05257v1",
        "pub_date": "2025-07-07",
        "summary": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and conflict resolution. Existing datasets either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Furthermore, no existing benchmarks cover all four competencies. Therefore, we introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark combines reformulated existing datasets with newly constructed ones, covering the above four memory competencies, providing a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.",
        "translated": "近期针对大语言模型（LLM）智能体的基准测试主要侧重于评估其推理、规划和执行能力，然而，另一个关键组成部分——记忆（涵盖智能体如何记忆、更新和检索长期信息）——由于缺乏基准而未得到充分评估。我们将具备记忆机制的智能体称为记忆智能体。\n\n在本文中，我们识别了对记忆智能体至关重要的四项核心能力：准确检索、测试时学习、长期理解和冲突解决。现有数据集要么依赖于有限的上下文长度，要么是为静态、长上下文设置（如基于书籍的问答）量身定制的，这些都无法反映记忆智能体逐步积累信息的交互式、多轮性质。此外，没有任何现有基准能涵盖所有这四项能力。\n\n因此，我们引入了MemoryAgentBench，这是一个专门为记忆智能体设计的新基准。我们的基准结合了重新制定的现有数据集和新构建的数据集，涵盖了上述四项记忆能力，为评估记忆质量提供了一个系统且具有挑战性的测试平台。我们评估了多种记忆智能体，从简单的基于上下文和检索增强生成（RAG）系统，到具备外部记忆模块和工具集成的高级智能体。经验结果表明，当前方法未能完全掌握所有这四项能力，这强调了需要进一步研究针对LLM智能体的全面记忆机制。"
    },
    {
        "title": "Pre-Trained Policy Discriminators are General Reward Models",
        "url": "http://arxiv.org/abs/2507.05197v1",
        "pub_date": "2025-07-07",
        "summary": "We offer a novel perspective on reward modeling by formulating it as a policy discriminator, which quantifies the difference between two policies to generate a reward signal, guiding the training policy towards a target policy with desired behaviors. Based on this conceptual insight, we propose a scalable pre-training method named Policy Discriminative Learning (POLAR), which trains a reward model (RM) to discern identical policies and discriminate different ones. Unlike traditional reward modeling methods relying on absolute preferences, POLAR captures the relative difference between one policy and an arbitrary target policy, which is a scalable, high-level optimization objective suitable for modeling generic ranking relationships. Leveraging the POLAR pre-training paradigm, we present a series of RMs with parameter scales from 1.8B to 7B. Empirical results show that POLAR substantially outperforms traditional non-pre-trained methods, significantly enhancing RM performance. For instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on STEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA baselines. POLAR also shows robust generalization capabilities in RLHF using Reinforcement Fine-tuning (RFT), providing reliable reward signals and markedly enhancing policy performance--improving LLaMa3.1-8B from an average of 47.36% to 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover, scaling experiments reveal a clear power-law relationship between computation and performance, supported by linear correlation coefficients approaching 0.99. The impressive performance, strong generalization, and scaling properties suggest that POLAR is a promising direction for developing general and strong reward models.",
        "translated": "我们提出了一种新颖的奖励建模视角，将其表述为策略判别器。该判别器量化了两种策略之间的差异，以生成奖励信号，引导训练策略趋向于具有期望行为的目标策略。基于这一概念性洞察，我们提出了一种可扩展的预训练方法，名为策略判别学习（Policy Discriminative Learning, POLAR）。POLAR通过训练奖励模型（RM）来识别相同策略并区分不同策略。与传统依赖绝对偏好的奖励建模方法不同，POLAR捕捉的是一个策略与任意目标策略之间的相对差异。这是一种可扩展、高层次的优化目标，适用于建模通用的排序关系。\n\n利用POLAR预训练范式，我们提出了一系列参数规模从18亿到70亿的奖励模型。实证结果表明，POLAR显著优于传统的非预训练方法，极大地提升了奖励模型性能。例如，与现有最佳（SOTA）基线相比，POLAR-7B在STEM任务上能将偏好准确率从54.8%提高到81.0%，在创意写作任务上从57.9%提高到85.5%。POLAR在使用强化微调（Reinforcement Fine-tuning, RFT）的RLHF（人类反馈强化学习）中也展现出强大的泛化能力，提供了可靠的奖励信号，并显著增强了策略性能——在20个基准测试中，将LLaMa3.1-8B的平均性能从47.36%提升到56.33%，将Qwen2.5-32B的平均性能从64.49%提升到70.47%。此外，扩展性实验揭示了计算与性能之间存在清晰的幂律关系，且其线性相关系数接近0.99。其卓越的性能、强大的泛化能力和良好的扩展特性表明，POLAR是开发通用且强大的奖励模型的一个有前途的方向。"
    },
    {
        "title": "Boosting Parameter Efficiency in LLM-Based Recommendation through\n  Sophisticated Pruning",
        "url": "http://arxiv.org/abs/2507.07064v1",
        "pub_date": "2025-07-09",
        "summary": "LLM-based recommender systems have made significant progress; however, the deployment cost associated with the large parameter volume of LLMs still hinders their real-world applications. This work explores parameter pruning to improve parameter efficiency while maintaining recommendation quality, thereby enabling easier deployment. Unlike existing approaches that focus primarily on inter-layer redundancy, we uncover intra-layer redundancy within components such as self-attention and MLP modules. Building on this analysis, we propose a more fine-grained pruning approach that integrates both intra-layer and layer-wise pruning. Specifically, we introduce a three-stage pruning strategy that progressively prunes parameters at different levels and parts of the model, moving from intra-layer to layer-wise pruning, or from width to depth. Each stage also includes a performance restoration step using distillation techniques, helping to strike a balance between performance and parameter efficiency. Empirical results demonstrate the effectiveness of our approach: across three datasets, our models achieve an average of 88% of the original model's performance while pruning more than 95% of the non-embedding parameters. This underscores the potential of our method to significantly reduce resource requirements without greatly compromising recommendation quality. Our code will be available at: https://github.com/zheng-sl/PruneRec",
        "translated": "基于LLM（大型语言模型）的推荐系统已取得显著进展；然而，LLM庞大的参数量所带来的部署成本仍然阻碍了它们在实际场景中的应用。本工作旨在探索参数剪枝（parameter pruning），以提高参数效率并保持推荐质量，从而实现更便捷的部署。与现有主要关注层间冗余（inter-layer redundancy）的方法不同，我们发现了自注意力（self-attention）和多层感知机（MLP）模块等组件内部的层内冗余（intra-layer redundancy）。\n\n基于这一分析，我们提出了一种更细粒度的剪枝方法，该方法整合了层内剪枝和层级剪枝（layer-wise pruning）。具体来说，我们引入了一种三阶段剪枝策略，该策略逐步剪枝模型不同级别和部分的参数，从层内剪枝到层级剪枝，或从宽度到深度。每个阶段还包含一个使用蒸馏技术（distillation techniques）进行性能恢复的步骤，有助于在性能和参数效率之间取得平衡。\n\n实验结果证明了我们方法的有效性：在三个数据集上，我们的模型在剪枝超过95%的非嵌入参数（non-embedding parameters）的同时，平均达到了原始模型88%的性能。这凸显了我们的方法在不大幅损害推荐质量的情况下显著降低资源需求的潜力。我们的代码将发布于：https://github.com/zheng-sl/PruneRec"
    },
    {
        "title": "SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label\n  Contrastive Learning and Bayesian kNN",
        "url": "http://arxiv.org/abs/2507.06895v1",
        "pub_date": "2025-07-09",
        "summary": "The growing demand for efficient knowledge graph (KG) enrichment leveraging external corpora has intensified interest in relation extraction (RE), particularly under low-supervision settings. To address the need for adaptable and noise-resilient RE solutions that integrate seamlessly with pre-trained large language models (PLMs), we introduce SCoRE, a modular and cost-effective sentence-level RE system. SCoRE enables easy PLM switching, requires no finetuning, and adapts smoothly to diverse corpora and KGs. By combining supervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN) classifier for multi-label classification, it delivers robust performance despite the noisy annotations of distantly supervised corpora. To improve RE evaluation, we propose two novel metrics: Correlation Structure Distance (CSD), measuring the alignment between learned relational patterns and KG structures, and Precision at R (P@R), assessing utility as a recommender system. We also release Wiki20d, a benchmark dataset replicating real-world RE conditions where only KG-derived annotations are available. Experiments on five benchmarks show that SCoRE matches or surpasses state-of-the-art methods while significantly reducing energy consumption. Further analyses reveal that increasing model complexity, as seen in prior work, degrades performance, highlighting the advantages of SCoRE's minimal design. Combining efficiency, modularity, and scalability, SCoRE stands as an optimal choice for real-world RE applications.",
        "translated": "对利用外部语料库进行高效知识图谱（KG）丰富/扩充的需求日益增长，这使得关系抽取（RE），尤其是在低监督设置下的关系抽取，引起了更广泛的关注。为了满足对适应性强、抗噪声、并能与预训练大型语言模型（PLMs）无缝集成的关系抽取解决方案的需求，我们引入了 SCoRE，这是一个模块化、经济高效的句子级关系抽取系统。SCoRE 支持轻松切换 PLM，无需微调，并且能平滑适应多样化的语料库和知识图谱。通过将有监督对比学习与用于多标签分类的贝叶斯 k-近邻（kNN）分类器相结合，SCoRE 即使在远程监督语料库的噪声标注下也能提供鲁棒的性能。为了改进关系抽取评估，我们提出了两个新颖的指标：关联结构距离（CSD），用于衡量学习到的关系模式与知识图谱结构之间的一致性；以及 R 值精确率（P@R），用于评估其作为推荐系统的效用。我们还发布了 Wiki20d，这是一个基准数据集，它复制了仅提供知识图谱派生标注的真实世界关系抽取条件。在五个基准数据集上的实验表明，SCoRE 的性能与最先进的方法相当或超越，同时显著降低了能耗。进一步的分析揭示，如先前工作所示，增加模型复杂度反而会降低性能，这凸显了 SCoRE 精简设计的优势。SCoRE 集效率、模块化和可伸缩性于一身，是真实世界关系抽取应用的理想选择。"
    },
    {
        "title": "CDC: Causal Domain Clustering for Multi-Domain Recommendation",
        "url": "http://arxiv.org/abs/2507.06877v1",
        "pub_date": "2025-07-09",
        "summary": "Multi-domain recommendation leverages domain-general knowledge to improve recommendations across several domains. However, as platforms expand to dozens or hundreds of scenarios, training all domains in a unified model leads to performance degradation due to significant inter-domain differences. Existing domain grouping methods, based on business logic or data similarities, often fail to capture the true transfer relationships required for optimal grouping. To effectively cluster domains, we propose Causal Domain Clustering (CDC). CDC models domain transfer patterns within a large number of domains using two distinct effects: the Isolated Domain Affinity Matrix for modeling non-interactive domain transfers, and the Hybrid Domain Affinity Matrix for considering dynamic domain synergy or interference under joint training. To integrate these two transfer effects, we introduce causal discovery to calculate a cohesion-based coefficient that adaptively balances their contributions. A Co-Optimized Dynamic Clustering algorithm iteratively optimizes target domain clustering and source domain selection for training. CDC significantly enhances performance across over 50 domains on public datasets and in industrial settings, achieving a 4.9% increase in online eCPM. Code is available at https://github.com/Chrissie-Law/Causal-Domain-Clustering-for-Multi-Domain-Recommendation",
        "translated": "多域推荐利用领域通用知识来提升跨多个领域的推荐效果。然而，随着平台扩展到数十甚至数百个场景，在统一模型中训练所有领域会导致性能下降，原因是域间差异显著。现有的域分组方法，无论是基于业务逻辑还是数据相似性，通常都无法捕捉到实现最佳分组所需的真实迁移关系。\n\n为了有效对域进行聚类，我们提出了因果域聚类（Causal Domain Clustering, CDC）方法。CDC 利用两种不同的效应来建模大量域中的域迁移模式：一是独立域亲和矩阵（Isolated Domain Affinity Matrix），用于建模非交互式域迁移；二是混合域亲和矩阵（Hybrid Domain Affinity Matrix），用于考虑联合训练下动态的域协同或干扰。为了整合这两种迁移效应，我们引入因果发现来计算一个基于凝聚力的系数，该系数能够自适应地平衡它们各自的贡献。一种协同优化动态聚类算法（Co-Optimized Dynamic Clustering algorithm）被用于迭代优化目标域聚类和训练时的源域选择。\n\nCDC 在公开数据集和工业环境中对超过50个域的性能都有显著提升，实现了在线eCPM 4.9%的增长。代码已在 https://github.com/Chrissie-Law/Causal-Domain-Clustering-for-Multi-Domain-Recommendation 上开源。"
    },
    {
        "title": "Shifting from Ranking to Set Selection for Retrieval Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2507.06838v1",
        "pub_date": "2025-07-09",
        "summary": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved passages are not only individually relevant but also collectively form a comprehensive set. Existing approaches primarily rerank top-k passages based on their individual relevance, often failing to meet the information needs of complex queries in multi-hop question answering. In this work, we propose a set-wise passage selection approach and introduce SETR, which explicitly identifies the information requirements of a query through Chain-of-Thought reasoning and selects an optimal set of passages that collectively satisfy those requirements. Experiments on multi-hop RAG benchmarks show that SETR outperforms both proprietary LLM-based rerankers and open-source baselines in terms of answer correctness and retrieval quality, providing an effective and efficient alternative to traditional rerankers in RAG systems. The code is available at https://github.com/LGAI-Research/SetR",
        "translated": "检索增强生成（RAG）中的检索必须确保检索到的段落不仅单独相关，而且共同构成一个全面的集合。现有方法主要基于段落的单独相关性对前k个段落进行重排序，这通常无法满足多跳问答中复杂查询的信息需求。\n\n在这项工作中，我们提出了一种集合式段落选择方法，并引入了SETR。SETR通过思维链（Chain-of-Thought）推理明确识别查询的信息需求，并选择一组能够共同满足这些需求的最优段落。在多跳RAG基准测试上的实验表明，SETR在答案正确性和检索质量方面均优于专有的大语言模型（LLM）重排序器和开源基线，为RAG系统中的传统重排序器提供了一种有效且高效的替代方案。\n\n代码已在 https://github.com/LGAI-Research/SetR 提供。"
    },
    {
        "title": "Temporal Information Retrieval via Time-Specifier Model Merging",
        "url": "http://arxiv.org/abs/2507.06782v1",
        "pub_date": "2025-07-09",
        "summary": "The rapid expansion of digital information and knowledge across structured and unstructured sources has heightened the importance of Information Retrieval (IR). While dense retrieval methods have substantially improved semantic matching for general queries, they consistently underperform on queries with explicit temporal constraints--often those containing numerical expressions and time specifiers such as ``in 2015.'' Existing approaches to Temporal Information Retrieval (TIR) improve temporal reasoning but often suffer from catastrophic forgetting, leading to reduced performance on non-temporal queries. To address this, we propose Time-Specifier Model Merging (TSM), a novel method that enhances temporal retrieval while preserving accuracy on non-temporal queries. TSM trains specialized retrievers for individual time specifiers and merges them in to a unified model, enabling precise handling of temporal constraints without compromising non-temporal retrieval. Extensive experiments on both temporal and non-temporal datasets demonstrate that TSM significantly improves performance on temporally constrained queries while maintaining strong results on non-temporal queries, consistently outperforming other baseline methods. Our code is available at https://github.com/seungyoonee/TSM .",
        "translated": "数字信息和知识在结构化和非结构化来源中的迅速扩张，使得信息检索（IR）的重要性日益凸显。尽管密集检索方法在通用查询的语义匹配方面已大幅提升，但它们在具有显式时间约束的查询上始终表现欠佳——这类查询通常包含数字表达式和诸如“in 2015”的时间限定词。现有的时序信息检索（TIR）方法虽然提升了时间推理能力，但却常常面临灾难性遗忘问题，导致在非时序查询上的性能降低。\n\n为解决此问题，我们提出了一种新颖的方法——时间限定词模型合并（Time-Specifier Model Merging, TSM），该方法在增强时序检索能力的同时，能保持对非时序查询的准确性。TSM 为每个独立的时间限定词训练专门的检索器，并将其合并到一个统一模型中，从而能够精确处理时间约束，同时不影响非时序检索。在时序和非时序数据集上进行的大量实验表明，TSM 显著提升了受时间约束查询的性能，同时在非时序查询上保持了强劲表现，并持续优于其他基线方法。我们的代码已在 https://github.com/seungyoonee/TSM 开源。"
    },
    {
        "title": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and\n  Context Aware Text Generation with LLMs",
        "url": "http://arxiv.org/abs/2507.06715v1",
        "pub_date": "2025-07-09",
        "summary": "Large language models (LLMs), including zero-shot and few-shot paradigms, have shown promising capabilities in clinical text generation. However, real-world applications face two key challenges: (1) patient data is highly unstructured, heterogeneous, and scattered across multiple note types and (2) clinical notes are often long and semantically dense, making naive prompting infeasible due to context length constraints and the risk of omitting clinically relevant information.   We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a domain-specific framework for structured and clinically grounded text generation using LLMs. It incorporates a novel hierarchical chunking strategy that respects clinical document structure and introduces a task-specific dual-stage retrieval mechanism. The global stage identifies relevant note types using evidence-based queries, while the local stage extracts high-value content within those notes creating relevance at both document and section levels.   We apply the system to generate structured progress notes for individual hospital visits using 15 clinical note types from the MIMIC-III dataset. Experiments show that it preserves temporal and semantic alignment across visits, achieving an average alignment score of 87.7%, surpassing the 80.7% baseline from real clinician-authored notes. The generated outputs also demonstrate high consistency across LLMs, reinforcing deterministic behavior essential for reproducibility, reliability, and clinical trust.",
        "translated": "大型语言模型（LLMs），包括零样本和少样本范式，在临床文本生成方面展现出良好的能力。然而，实际应用面临两个主要挑战：（1）患者数据高度非结构化、异构，并且分散在多种记录类型中；（2）临床记录通常很长且语义密集，这使得简单的提示不可行，原因在于上下文长度限制以及遗漏临床相关信息的风险。\n\n我们提出了CLI-RAG（临床知情检索增强生成），这是一个领域特定的框架，用于使用LLMs进行结构化和临床依据的文本生成。它采用了一种新颖的层次化分块策略，该策略尊重临床文档结构，并引入了任务特定的双阶段检索机制。全局阶段使用基于证据的查询来识别相关的记录类型，而局部阶段则从这些记录中提取高价值内容，从而在文档和章节层面都实现了相关性。\n\n我们将该系统应用于使用来自MIMIC-III数据集的15种临床记录类型，生成针对每次住院的结构化病程记录。实验结果显示，该系统能够保持跨就诊的时序和语义对齐，实现了87.7%的平均对齐分数，超过了来自真实临床医生编写记录的80.7%的基线。生成的输出在LLMs之间也表现出高度一致性，增强了确定性行为，这对于可复现性、可靠性和临床信任至关重要。"
    },
    {
        "title": "MS-DPPs: Multi-Source Determinantal Point Processes for Contextual\n  Diversity Refinement of Composite Attributes in Text to Image Retrieval",
        "url": "http://arxiv.org/abs/2507.06654v1",
        "pub_date": "2025-07-09",
        "summary": "Result diversification (RD) is a crucial technique in Text-to-Image Retrieval for enhancing the efficiency of a practical application. Conventional methods focus solely on increasing the diversity metric of image appearances. However, the diversity metric and its desired value vary depending on the application, which limits the applications of RD. This paper proposes a novel task called CDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims to refine the diversities of multiple attributes, according to the application's context. To address this task, we propose Multi-Source DPPs, a simple yet strong baseline that extends the Determinantal Point Process (DPP) to multi-sources. We model MS-DPP as a single DPP model with a unified similarity matrix based on a manifold representation. We also introduce Tangent Normalization to reflect contexts. Extensive experiments demonstrate the effectiveness of the proposed method. Our code is publicly available at https://github.com/NEC-N-SOGI/msdpp.",
        "translated": "**中文翻译：**\n\n结果多样性（Result Diversification, RD）是文本到图像检索中一项关键技术，旨在提高实际应用的效率。传统方法仅着重于提升图像外观的多样性度量。然而，多样性度量及其期望值因应用而异，这限制了RD的应用范围。\n\n本文提出一项新颖的任务，名为CDR-CA（Contextual Diversity Refinement of Composite Attributes，复合属性的上下文多样性细化）。CDR-CA旨在根据应用上下文，细化多个属性的多样性。为解决此任务，我们提出了多源行列式点过程（Multi-Source Determinantal Point Processes，简称MS-DPP），这是一个将行列式点过程（DPP）扩展到多源的简单而强大的基线方法。我们将MS-DPP建模为一个单一的DPP模型，该模型具有基于流形表示的统一相似性矩阵。我们还引入了切线归一化（Tangent Normalization）以反映上下文。大量实验证明了所提出方法的有效性。我们的代码已公开，可在 https://github.com/NEC-N-SOGI/msdpp 获取。"
    },
    {
        "title": "Impacts of Mainstream-Driven Algorithms on Recommendations for Children\n  Across Domains: A Reproducibility Study",
        "url": "http://arxiv.org/abs/2507.06596v1",
        "pub_date": "2025-07-09",
        "summary": "Children are often exposed to items curated by recommendation algorithms. Yet, research seldom considers children as a user group, and when it does, it is anchored on datasets where children are underrepresented, risking overlooking their interests, favoring those of the majority, i.e., mainstream users. Recently, Ungruh et al. demonstrated that children's consumption patterns and preferences differ from those of mainstream users, resulting in inconsistent recommendation algorithm performance and behavior for this user group. These findings, however, are based on two datasets with a limited child user sample. We reproduce and replicate this study on a wider range of datasets in the movie, music, and book domains, uncovering interaction patterns and aspects of child-recommender interactions consistent across domains, as well as those specific to some user samples in the data. We also extend insights from the original study with popularity bias metrics, given the interpretation of results from the original study. With this reproduction and extension, we uncover consumption patterns and differences between age groups stemming from intrinsic differences between children and others, and those unique to specific datasets or domains.",
        "translated": "儿童经常接触到由推荐算法精选的物品。然而，研究鲜少将儿童视为用户群体，即便考虑，也往往基于儿童代表性不足的数据集，这有忽视其兴趣、偏向多数用户（即主流用户）兴趣的风险。最近，Ungruh 等人表明，儿童的消费模式和偏好与主流用户不同，导致推荐算法对该用户群体的性能和行为表现出不一致性。然而，这些发现是基于仅包含有限儿童用户样本的两个数据集。我们在电影、音乐和图书领域更广泛的数据集上复现并复制了这项研究，揭示了儿童与推荐系统交互中跨领域一致的交互模式和方面，以及数据中某些用户样本所特有的模式和方面。鉴于对原始研究结果的解读，我们还引入流行度偏差指标，扩展了原始研究的见解。通过这项复现和扩展，我们揭示了不同年龄组之间的消费模式和差异，这些差异有些源于儿童与其他人之间的内在差异，有些则特定于某些数据集或领域。"
    },
    {
        "title": "DS@GT at CheckThat! 2025: Exploring Retrieval and Reranking Pipelines\n  for Scientific Claim Source Retrieval on Social Media Discourse",
        "url": "http://arxiv.org/abs/2507.06563v1",
        "pub_date": "2025-07-09",
        "summary": "Social media users often make scientific claims without citing where these claims come from, generating a need to verify these claims. This paper details work done by the DS@GT team for CLEF 2025 CheckThat! Lab Task 4b Scientific Claim Source Retrieval which seeks to find relevant scientific papers based on implicit references in tweets. Our team explored 6 different data augmentation techniques, 7 different retrieval and reranking pipelines, and finetuned a bi-encoder. Achieving an MRR@5 of 0.58, our team ranked 16th out of 30 teams for the CLEF 2025 CheckThat! Lab Task 4b, and improvement of 0.15 over the BM25 baseline of 0.43. Our code is available on Github at https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4b.",
        "translated": "社交媒体用户经常发表科学主张，但未注明其来源，这使得验证这些主张的需求应运而生。本文详细介绍了DS@GT团队为CLEF 2025 CheckThat! 实验室任务4b“科学主张来源检索”所做的工作，该任务旨在根据推文中隐含的引用来查找相关的科学论文。我们的团队探索了6种不同的数据增强技术、7种不同的检索和重排序流程，并对一个双编码器进行了微调。我们的团队在CLEF 2025 CheckThat! 实验室任务4b中获得了0.58的MRR@5（Mean Reciprocal Rank at 5），在30支参赛队伍中排名第16位，较BM25基线（0.43）提高了0.15。我们的代码已在GitHub上开源，链接为：https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4b。"
    },
    {
        "title": "SPEAR: Subset-sampled Performance Evaluation via Automated Ground Truth\n  Generation for RAG",
        "url": "http://arxiv.org/abs/2507.06554v1",
        "pub_date": "2025-07-09",
        "summary": "Retrieval-Augmented Generation (RAG) is a core approach for enhancing Large Language Models (LLMs), where the effectiveness of the retriever largely determines the overall response quality of RAG systems. Retrievers encompass a multitude of hyperparameters that significantly impact performance outcomes and demonstrate sensitivity to specific applications. Nevertheless, hyperparameter optimization entails prohibitively high computational expenses. Existing evaluation methods suffer from either prohibitive costs or disconnection from domain-specific scenarios. This paper proposes SEARA (Subset sampling Evaluation for Automatic Retriever Assessment), which addresses evaluation data challenges through subset sampling techniques and achieves robust automated retriever evaluation by minimal retrieval facts extraction and comprehensive retrieval metrics. Based on real user queries, this method enables fully automated retriever evaluation at low cost, thereby obtaining optimal retriever for specific business scenarios. We validate our method across classic RAG applications in rednote, including knowledge-based Q&amp;A system and retrieval-based travel assistant, successfully obtaining scenario-specific optimal retrievers.",
        "translated": "检索增强生成（RAG）是增强大型语言模型（LLM）的核心方法，其中检索器的有效性在很大程度上决定了RAG系统的整体响应质量。检索器包含大量超参数，这些超参数显著影响性能表现，并且对特定应用具有敏感性。然而，超参数优化伴随着高昂的计算开销。现有评估方法存在成本过高或与领域特定场景脱节的问题。本文提出了SEARA（子集采样自动检索器评估），该方法通过子集采样技术解决了评估数据挑战，并通过最小化检索事实提取和全面的检索指标实现了鲁棒的自动化检索器评估。基于真实用户查询，该方法能够以低成本实现全自动的检索器评估，从而为特定业务场景获得最优检索器。我们在rednote中的经典RAG应用（包括基于知识的问答系统和基于检索的旅行助手）中验证了我们的方法，并成功获得了场景特定的最优检索器。"
    },
    {
        "title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced\n  Planning, Navigation, and Dynamic Adaptation",
        "url": "http://arxiv.org/abs/2507.06993v1",
        "pub_date": "2025-07-09",
        "summary": "Traditional travel-planning systems are often static and fragmented, leaving them ill-equipped to handle real-world complexities such as evolving environmental conditions and unexpected itinerary disruptions. In this paper, we identify three gaps between existing service providers causing frustrating user experience: intelligent trip planning, precision \"last-100-meter\" navigation, and dynamic itinerary adaptation. We propose three cooperative agents: a Travel Planning Agent that employs grid-based spatial grounding and map analysis to help resolve complex multi-modal user queries; a Destination Assistant Agent that provides fine-grained guidance for the final navigation leg of each journey; and a Local Discovery Agent that leverages image embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to trip plan disruptions. With evaluations and experiments, our system demonstrates substantial improvements in query interpretation, navigation accuracy, and disruption resilience, underscoring its promise for applications from urban exploration to emergency response.",
        "translated": "传统的旅行规划系统通常是静态且碎片化的，这使得它们难以应对现实世界的复杂性，例如不断变化的环境条件和意料之外的行程中断。在本文中，我们指出了导致用户体验不佳的现有服务中的三个不足之处：智能旅行规划、精准的“最后一百米”导航以及动态行程调整。为此，我们提出了三个协作智能体：一个旅行规划智能体，它采用基于网格的空间理解和地图分析来帮助解决复杂的多模态用户查询；一个目的地辅助智能体，为每次旅程的最终导航阶段提供细粒度指导；以及一个本地探索智能体，它利用图像嵌入和检索增强生成（RAG）来检测并响应行程计划中断。通过评估和实验，我们的系统在查询解释、导航精度和中断韧性方面展现出显著改进，凸显了其在从城市探索到紧急响应等应用中的巨大潜力。"
    },
    {
        "title": "UniConv: Unifying Retrieval and Response Generation for Large Language\n  Models in Conversations",
        "url": "http://arxiv.org/abs/2507.07030v1",
        "pub_date": "2025-07-09",
        "summary": "The rapid advancement of conversational search systems revolutionizes how information is accessed by enabling the multi-turn interaction between the user and the system. Existing conversational search systems are usually built with two different models. This separation restricts the system from leveraging the intrinsic knowledge of the models simultaneously, which cannot ensure the effectiveness of retrieval benefiting the generation. The existing studies for developing unified models cannot fully address the aspects of understanding conversational context, managing retrieval independently, and generating responses. In this paper, we explore how to unify dense retrieval and response generation for large language models in conversation. We conduct joint fine-tuning with different objectives and design two mechanisms to reduce the inconsistency risks while mitigating data discrepancy. The evaluations on five conversational search datasets demonstrate that our unified model can mutually improve both tasks and outperform the existing baselines.",
        "translated": "对话式搜索系统的快速发展通过实现用户与系统之间的多轮交互，彻底革新了信息获取方式。现有对话式搜索系统通常由两种不同模型构建。这种分离限制了系统同时利用模型的内在知识，无法确保检索效果能有效反哺生成。现有关于开发统一模型的研究未能充分解决理解对话上下文、独立管理检索以及生成响应等问题。本文探索了如何在对话中为大语言模型（LLM）统一稠密检索与响应生成。我们采用不同目标进行联合微调，并设计了两种机制来降低不一致风险，同时缓解数据差异。在五个对话式搜索数据集上的评估表明，我们提出的统一模型能够相互提升两项任务，并超越现有基线。"
    },
    {
        "title": "Investigating the Robustness of Retrieval-Augmented Generation at the\n  Query Level",
        "url": "http://arxiv.org/abs/2507.06956v1",
        "pub_date": "2025-07-09",
        "summary": "Large language models (LLMs) are very costly and inefficient to update with new information. To address this limitation, retrieval-augmented generation (RAG) has been proposed as a solution that dynamically incorporates external knowledge during inference, improving factual consistency and reducing hallucinations. Despite its promise, RAG systems face practical challenges-most notably, a strong dependence on the quality of the input query for accurate retrieval. In this paper, we investigate the sensitivity of different components in the RAG pipeline to various types of query perturbations. Our analysis reveals that the performance of commonly used retrievers can degrade significantly even under minor query variations. We study each module in isolation as well as their combined effect in an end-to-end question answering setting, using both general-domain and domain-specific datasets. Additionally, we propose an evaluation framework to systematically assess the query-level robustness of RAG pipelines and offer actionable recommendations for practitioners based on the results of more than 1092 experiments we performed.",
        "translated": "大型语言模型（LLMs）在更新新信息时开销巨大且效率低下。为了解决这一局限性，检索增强生成（RAG）被提出作为一种解决方案，它能够在推理过程中动态整合外部知识，从而提高事实一致性并减少幻觉现象。尽管RAG系统前景广阔，但它也面临着实际挑战——尤其值得注意的是，其准确检索能力强烈依赖于输入查询的质量。\n\n在本文中，我们研究了RAG管线中不同组件对各种类型查询扰动的敏感性。我们的分析表明，即使在细微的查询变化下，常用检索器的性能也会显著下降。我们不仅独立研究了每个模块，还研究了它们在端到端问答场景下的组合效应，并使用了通用领域和领域特定数据集。此外，我们提出了一个评估框架，用于系统地评估RAG管线的查询级鲁棒性，并根据我们进行的1092次以上实验结果，为实践者提供了可操作的建议。"
    },
    {
        "title": "MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection",
        "url": "http://arxiv.org/abs/2507.06908v1",
        "pub_date": "2025-07-09",
        "summary": "The rapid expansion of memes on social media has highlighted the urgent need for effective approaches to detect harmful content. However, traditional data-driven approaches struggle to detect new memes due to their evolving nature and the lack of up-to-date annotated data. To address this issue, we propose MIND, a multi-agent framework for zero-shot harmful meme detection that does not rely on annotated data. MIND implements three key strategies: 1) We retrieve similar memes from an unannotated reference set to provide contextual information. 2) We propose a bi-directional insight derivation mechanism to extract a comprehensive understanding of similar memes. 3) We then employ a multi-agent debate mechanism to ensure robust decision-making through reasoned arbitration. Extensive experiments on three meme datasets demonstrate that our proposed framework not only outperforms existing zero-shot approaches but also shows strong generalization across different model architectures and parameter scales, providing a scalable solution for harmful meme detection. The code is available at https://github.com/destroy-lonely/MIND.",
        "translated": "社交媒体上梗图的迅猛发展凸显了对有效有害内容检测方法的迫切需求。然而，由于其不断演变的特性以及最新标注数据的缺乏，传统数据驱动方法难以检测新梗图。为解决这一问题，我们提出了MIND，一个无需标注数据的多智能体零样本有害梗图检测框架。MIND采用了三项关键策略：1) 我们从未标注的参考集中检索相似梗图以提供上下文信息。2) 我们提出了一个双向洞察推导机制，以获取对相似梗图的全面理解。3) 随后，我们采用多智能体辩论机制，通过理性仲裁确保鲁棒的决策。在三个梗图数据集上进行的大量实验表明，我们提出的框架不仅优于现有零样本方法，而且在不同模型架构和参数规模下都展现出强大的泛化能力，为有害梗图检测提供了一个可扩展的解决方案。代码已在https://github.com/destroy-lonely/MIND提供。"
    },
    {
        "title": "Measuring Hypothesis Testing Errors in the Evaluation of Retrieval\n  Systems",
        "url": "http://arxiv.org/abs/2507.07924v1",
        "pub_date": "2025-07-10",
        "summary": "The evaluation of Information Retrieval (IR) systems typically uses query-document pairs with corresponding human-labelled relevance assessments (qrels). These qrels are used to determine if one system is better than another based on average retrieval performance. Acquiring large volumes of human relevance assessments is expensive. Therefore, more efficient relevance assessment approaches have been proposed, necessitating comparisons between qrels to ascertain their efficacy. Discriminative power, i.e. the ability to correctly identify significant differences between systems, is important for drawing accurate conclusions on the robustness of qrels. Previous work has measured the proportion of pairs of systems that are identified as significantly different and has quantified Type I statistical errors. Type I errors lead to incorrect conclusions due to false positive significance tests. We argue that also identifying Type II errors (false negatives) is important as they lead science in the wrong direction. We quantify Type II errors and propose that balanced classification metrics, such as balanced accuracy, can be used to portray the discriminative power of qrels. We perform experiments using qrels generated using alternative relevance assessment methods to investigate measuring hypothesis testing errors in IR evaluation. We find that additional insights into the discriminative power of qrels can be gained by quantifying Type II errors, and that balanced classification metrics can be used to give an overall summary of discriminative power in one, easily comparable, number.",
        "translated": "信息检索（IR）系统的评估通常使用查询-文档对以及对应的人工标注相关性判断（qrels）。这些qrels用于基于平均检索性能来确定一个系统是否优于另一个系统。获取大量人工相关性判断的成本高昂，因此，研究者们提出了更高效的相关性判断方法，这使得需要对qrels进行比较以确定其有效性。判别力，即正确识别系统间显著差异的能力，对于得出关于qrels鲁棒性的准确结论至关重要。\n\n现有工作已经衡量了被识别为显著不同的系统对的比例，并量化了第一类统计错误。第一类错误（Type I errors）由于假阳性显著性检验而导致不正确的结论。我们认为，识别第二类错误（Type II errors，即假阴性）同样重要，因为它们会将科学引向错误的方向。我们量化了第二类错误，并提出可以使用平衡分类指标，例如平衡准确率，来刻画qrels的判别力。我们使用通过替代相关性判断方法生成的qrels进行实验，以研究在IR评估中测量假设检验误差。我们发现，通过量化第二类错误，可以对qrels的判别力获得更深入的见解，并且平衡分类指标可以以一个易于比较的单一数值形式，对判别力提供一个总体概述。"
    },
    {
        "title": "Plausible Counterfactual Explanations of Recommendations",
        "url": "http://arxiv.org/abs/2507.07919v1",
        "pub_date": "2025-07-10",
        "summary": "Explanations play a variety of roles in various recommender systems, from a legally mandated afterthought, through an integral element of user experience, to a key to persuasiveness. A natural and useful form of an explanation is the Counterfactual Explanation (CE). We present a method for generating highly plausible CEs in recommender systems and evaluate it both numerically and with a user study.",
        "translated": "解释在各类推荐系统中扮演着多种角色，从法律规定的附加考虑，到用户体验的核心要素，再到提升说服力的关键。反事实解释（CE）是一种自然且有效的解释形式。本文提出了一种在推荐系统中生成高度合理反事实解释（CE）的方法，并对其进行了数值评估和用户研究验证。"
    },
    {
        "title": "DTECT: Dynamic Topic Explorer &amp; Context Tracker",
        "url": "http://arxiv.org/abs/2507.07910v1",
        "pub_date": "2025-07-10",
        "summary": "The explosive growth of textual data over time presents a significant challenge in uncovering evolving themes and trends. Existing dynamic topic modeling techniques, while powerful, often exist in fragmented pipelines that lack robust support for interpretation and user-friendly exploration. We introduce DTECT (Dynamic Topic Explorer &amp; Context Tracker), an end-to-end system that bridges the gap between raw textual data and meaningful temporal insights. DTECT provides a unified workflow that supports data preprocessing, multiple model architectures, and dedicated evaluation metrics to analyze the topic quality of temporal topic models. It significantly enhances interpretability by introducing LLM-driven automatic topic labeling, trend analysis via temporally salient words, interactive visualizations with document-level summarization, and a natural language chat interface for intuitive data querying. By integrating these features into a single, cohesive platform, DTECT empowers users to more effectively track and understand thematic dynamics. DTECT is open-source and available at https://github.com/AdhyaSuman/DTECT.",
        "translated": "随着时间的推移，文本数据的爆炸式增长给揭示演变的主题和趋势带来了严峻挑战。现有的动态主题建模技术尽管功能强大，但通常以碎片化的形式存在，缺乏对解释和用户友好型探索的强大支持。我们推出DTECT（动态主题探索器与上下文追踪器），这是一个端到端系统，旨在弥合原始文本数据与有意义的时间洞察之间的鸿沟。\n\nDTECT提供了一个统一的工作流程，支持数据预处理、多种模型架构以及用于分析时间主题模型主题质量的专用评估指标。它通过引入大语言模型（LLM）驱动的自动化主题标注、通过时间上显著词语进行的趋势分析、结合文档级摘要的交互式可视化，以及用于直观数据查询的自然语言聊天界面，显著增强了可解释性。通过将这些功能集成到一个单一、内聚的平台中，DTECT使用户能够更有效地追踪和理解主题动态。DTECT是开源的，可通过https://github.com/AdhyaSuman/DTECT访问。"
    },
    {
        "title": "Document Similarity Enhanced IPS Estimation for Unbiased Learning to\n  Rank",
        "url": "http://arxiv.org/abs/2507.07909v1",
        "pub_date": "2025-07-10",
        "summary": "Learning to Rank (LTR) models learn from historical user interactions, such as user clicks. However, there is an inherent bias in the clicks of users due to position bias, i.e., users are more likely to click highly-ranked documents than low-ranked documents. To address this bias when training LTR models, many approaches from the literature re-weight the users' click data using Inverse Propensity Scoring (IPS). IPS re-weights the user's clicks proportionately to the position in the historical ranking that a document was placed when it was clicked since low-ranked documents are less likely to be seen by a user. In this paper, we argue that low-ranked documents that are similar to highly-ranked relevant documents are also likely to be relevant. Moreover, accounting for the similarity of low-ranked documents to highly ranked relevant documents when calculating IPS can more effectively mitigate the effects of position bias. Therefore, we propose an extension to IPS, called IPSsim, that takes into consideration the similarity of documents when estimating IPS. We evaluate our IPSsim estimator using two large publicly available LTR datasets under a number of simulated user click settings, and with different numbers of training clicks. Our experiments show that our IPSsim estimator is more effective than the existing IPS estimators for learning an unbiased LTR model, particularly in top-n settings when n &gt;= 30. For example, when n = 50, our IPSsim estimator achieves a statistically significant ~3% improvement (p &lt; 0.05) in terms of NDCG compared to the Doubly Robust estimator from the literature.",
        "translated": "排序学习（LTR）模型从历史用户交互（例如用户点击）中学习。然而，由于位置偏差，用户点击中存在固有的偏差，即用户更倾向于点击排名靠前的文档而非排名靠后的文档。为了在训练 LTR 模型时解决这种偏差，文献中的许多方法使用逆倾向性得分（IPS）对用户点击数据进行重新加权。IPS 会根据文档在被点击时所处的历史排名位置对用户点击进行重新加权，因为排名靠后的文档用户更不容易看到。\n\n在本文中，我们认为排名靠后但与排名靠前的相关文档相似的文档也可能相关。此外，在计算 IPS 时考虑到排名靠后的文档与排名靠前的相关文档之间的相似性，可以更有效地缓解位置偏差的影响。因此，我们提出了 IPS 的一个扩展，称为 IPSsim，它在估计 IPS 时考虑了文档的相似性。我们使用两个大型公开可用的 LTR 数据集，在多种模拟用户点击设置以及不同数量的训练点击下，评估了我们的 IPSsim 估计器。我们的实验表明，我们的 IPSsim 估计器在学习无偏的 LTR 模型方面比现有 IPS 估计器更有效，尤其是在 n 大于等于 30 的 top-n 设置中。例如，当 n = 50 时，与文献中的 Doubly Robust 估计器相比，我们的 IPSsim 估计器在 NDCG 方面实现了统计学上显著的约 3% 的提升（p < 0.05）。"
    },
    {
        "title": "Rethinking the Privacy of Text Embeddings: A Reproducibility Study of\n  \"Text Embeddings Reveal (Almost) As Much As Text\"",
        "url": "http://arxiv.org/abs/2507.07700v1",
        "pub_date": "2025-07-10",
        "summary": "Text embeddings are fundamental to many natural language processing (NLP) tasks, extensively applied in domains such as recommendation systems and information retrieval (IR). Traditionally, transmitting embeddings instead of raw text has been seen as privacy-preserving. However, recent methods such as Vec2Text challenge this assumption by demonstrating that controlled decoding can successfully reconstruct original texts from black-box embeddings. The unexpectedly strong results reported by Vec2Text motivated us to conduct further verification, particularly considering the typically non-intuitive and opaque structure of high-dimensional embedding spaces. In this work, we reproduce the Vec2Text framework and evaluate it from two perspectives: (1) validating the original claims, and (2) extending the study through targeted experiments. First, we successfully replicate the original key results in both in-domain and out-of-domain settings, with only minor discrepancies arising due to missing artifacts, such as model checkpoints and dataset splits. Furthermore, we extend the study by conducting a parameter sensitivity analysis, evaluating the feasibility of reconstructing sensitive inputs (e.g., passwords), and exploring embedding quantization as a lightweight privacy defense. Our results show that Vec2Text is effective under ideal conditions, capable of reconstructing even password-like sequences that lack clear semantics. However, we identify key limitations, including its sensitivity to input sequence length. We also find that Gaussian noise and quantization techniques can mitigate the privacy risks posed by Vec2Text, with quantization offering a simpler and more widely applicable solution. Our findings emphasize the need for caution in using text embeddings and highlight the importance of further research into robust defense mechanisms for NLP systems.",
        "translated": "文本嵌入是许多自然语言处理（NLP）任务的基础，广泛应用于推荐系统和信息检索（IR）等领域。传统上，传输嵌入（而非原始文本）被视为一种隐私保护措施。然而，Vec2Text 等近期方法通过证明受控解码能够成功地从黑盒嵌入中重构原始文本，从而挑战了这一假设。Vec2Text 报告的意想不到的强大结果，促使我们进行进一步验证，尤其考虑到高维嵌入空间通常非直观且不透明的结构。\n\n在这项工作中，我们复现了 Vec2Text 框架，并从两个角度对其进行了评估：(1) 验证原始主张，以及 (2) 通过有针对性的实验扩展研究。首先，我们在域内和域外设置中成功复现了原始的关键结果，仅因缺少模型检查点和数据集划分等工件而存在微小差异。此外，我们通过进行参数敏感性分析、评估重构敏感输入（如密码）的可行性，以及探索将嵌入量化作为一种轻量级隐私防御措施，从而扩展了这项研究。\n\n我们的结果表明，Vec2Text 在理想条件下是有效的，甚至能够重构缺乏明确语义的类似密码的序列。然而，我们也发现了一些主要的局限性，包括它对输入序列长度的敏感性。我们还发现，高斯噪声和量化技术可以减轻 Vec2Text 带来的隐私风险，其中量化提供了一种更简单、更广泛适用的解决方案。我们的发现强调了在使用文本嵌入时需要谨慎，并突出了进一步研究针对 NLP 系统的鲁棒防御机制的重要性。"
    },
    {
        "title": "The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English\n  Corpora",
        "url": "http://arxiv.org/abs/2507.07543v1",
        "pub_date": "2025-07-10",
        "summary": "Cross-lingual retrieval-augmented generation (RAG) is a critical capability for retrieving and generating answers across languages. Prior work in this context has mostly focused on generation and relied on benchmarks derived from open-domain sources, most notably Wikipedia. In such settings, retrieval challenges often remain hidden due to language imbalances, overlap with pretraining data, and memorized content. To address this gap, we study Arabic-English RAG in a domain-specific setting using benchmarks derived from real-world corporate datasets. Our benchmarks include all combinations of languages for the user query and the supporting document, drawn independently and uniformly at random. This enables a systematic study of multilingual retrieval behavior.   Our findings reveal that retrieval is a critical bottleneck in cross-lingual domain-specific scenarios, with significant performance drops occurring when the user query and supporting document languages differ. A key insight is that these failures stem primarily from the retriever's difficulty in ranking documents across languages. Finally, we propose a simple retrieval strategy that addresses this source of failure by enforcing equal retrieval from both languages, resulting in substantial improvements in cross-lingual and overall performance. These results highlight meaningful opportunities for improving multilingual retrieval, particularly in practical, real-world RAG applications.",
        "translated": "跨语言检索增强生成（RAG）是实现跨语言检索和生成答案的关键能力。在此背景下，以往的工作主要侧重于生成，并依赖于源自开放领域（尤其是维基百科）的基准。在此类设置中，由于语言不平衡、与预训练数据重叠以及模型记忆内容的存在，检索挑战往往被掩盖。为弥补这一空白，我们研究了在领域特定设置下的阿拉伯语-英语 RAG，并使用了源自真实企业数据集的基准。我们的基准包含了用户查询和支持文档的所有语言组合，这些组合都是独立且均匀随机抽取的。这使得对多语言检索行为进行系统研究成为可能。\n\n我们的研究结果表明，在跨语言领域特定场景中，检索是一个关键瓶颈；当用户查询和支持文档语言不同时，性能会显著下降。一个关键的洞察是，这些失败主要源于检索器在跨语言文档排序方面的困难。最后，我们提出了一种简单的检索策略，通过强制从两种语言中进行等量检索来解决这一失败根源，从而显著提升了跨语言和整体性能。这些结果突显了改进多语言检索的巨大潜力，尤其是在实际的、真实世界的 RAG 应用中。"
    },
    {
        "title": "NLGCL: Naturally Existing Neighbor Layers Graph Contrastive Learning for\n  Recommendation",
        "url": "http://arxiv.org/abs/2507.07522v1",
        "pub_date": "2025-07-10",
        "summary": "Graph Neural Networks (GNNs) are widely used in collaborative filtering to capture high-order user-item relationships. To address the data sparsity problem in recommendation systems, Graph Contrastive Learning (GCL) has emerged as a promising paradigm that maximizes mutual information between contrastive views. However, existing GCL methods rely on augmentation techniques that introduce semantically irrelevant noise and incur significant computational and storage costs, limiting effectiveness and efficiency.   To overcome these challenges, we propose NLGCL, a novel contrastive learning framework that leverages naturally contrastive views between neighbor layers within GNNs. By treating each node and its neighbors in the next layer as positive pairs, and other nodes as negatives, NLGCL avoids augmentation-based noise while preserving semantic relevance. This paradigm eliminates costly view construction and storage, making it computationally efficient and practical for real-world scenarios. Extensive experiments on four public datasets demonstrate that NLGCL outperforms state-of-the-art baselines in effectiveness and efficiency.",
        "translated": "图神经网络 (GNN) 广泛应用于协同过滤中，以捕获高阶用户-物品关系。为解决推荐系统中的数据稀疏性问题，图对比学习 (GCL) 已成为一种有前景的范式，它通过最大化对比视图之间的互信息来提升性能。然而，现有的 GCL 方法依赖于增强技术，这些技术引入了语义不相关的噪声，并带来了显著的计算和存储开销，从而限制了其有效性和效率。\n\n为了克服这些挑战，我们提出了 NLGCL，这是一种新颖的对比学习框架，它利用了 GNN 内部邻居层之间自然形成的对比视图。通过将每个节点及其在下一层中的邻居视为正样本对，并将其他节点视为负样本，NLGCL 避免了基于增强的噪声，同时保留了语义相关性。这种范式消除了耗时的视图构建和存储，使其计算高效，并对真实世界场景具有实用性。在四个公开数据集上的大量实验表明，NLGCL 在有效性和效率方面均优于最先进的基线方法。"
    },
    {
        "title": "When Graph Contrastive Learning Backfires: Spectral Vulnerability and\n  Defense in Recommendation",
        "url": "http://arxiv.org/abs/2507.07436v1",
        "pub_date": "2025-07-10",
        "summary": "Graph Contrastive Learning (GCL) has demonstrated substantial promise in enhancing the robustness and generalization of recommender systems, particularly by enabling models to leverage large-scale unlabeled data for improved representation learning. However, in this paper, we reveal an unexpected vulnerability: the integration of GCL inadvertently increases the susceptibility of a recommender to targeted promotion attacks. Through both theoretical investigation and empirical validation, we identify the root cause as the spectral smoothing effect induced by contrastive optimization, which disperses item embeddings across the representation space and unintentionally enhances the exposure of target items. Building on this insight, we introduce CLeaR, a bi-level optimization attack method that deliberately amplifies spectral smoothness, enabling a systematic investigation of the susceptibility of GCL-based recommendation models to targeted promotion attacks. Our findings highlight the urgent need for robust countermeasures; in response, we further propose SIM, a spectral irregularity mitigation framework designed to accurately detect and suppress targeted items without compromising model performance. Extensive experiments on multiple benchmark datasets demonstrate that, compared to existing targeted promotion attacks, GCL-based recommendation models exhibit greater susceptibility when evaluated with CLeaR, while SIM effectively mitigates these vulnerabilities.",
        "translated": "图对比学习（GCL）在增强推荐系统的鲁棒性和泛化能力方面展现出巨大潜力，尤其通过使模型能够利用大规模未标注数据以提升表示学习。然而，在本文中，我们揭示了一个出乎意料的脆弱性：GCL的集成无意中增加了推荐系统对定向推广攻击的易感性。通过理论研究和实证验证，我们确定其根本原因在于对比优化所导致的谱平滑效应，这使得项目嵌入在表示空间中分散，并无意中提高了目标项目的曝光度。基于这一洞察，我们引入了CLeaR，这是一种双层优化攻击方法，旨在故意放大谱平滑，从而能够系统地调查基于GCL的推荐模型对定向推广攻击的易感性。我们的研究结果凸显了对鲁棒性反制措施的迫切需求；对此，我们进一步提出了SIM，这是一种谱不规则性缓解框架，旨在在不损害模型性能的情况下准确检测和抑制目标项目。在多个基准数据集上进行的广泛实验表明，与现有定向推广攻击相比，基于GCL的推荐模型在通过CLeaR评估时表现出更大的易感性，而SIM则有效缓解了这些脆弱性。"
    },
    {
        "title": "A Language-Driven Framework for Improving Personalized Recommendations:\n  Merging LLMs with Traditional Algorithms",
        "url": "http://arxiv.org/abs/2507.07251v1",
        "pub_date": "2025-07-09",
        "summary": "Traditional recommendation algorithms are not designed to provide personalized recommendations based on user preferences provided through text, e.g., \"I enjoy light-hearted comedies with a lot of humor\". Large Language Models (LLMs) have emerged as one of the most promising tools for natural language processing in recent years. This research proposes a novel framework that mimics how a close friend would recommend items based on their knowledge of an individual's tastes. We leverage LLMs to enhance movie recommendation systems by refining traditional algorithm outputs and integrating them with language-based user preference inputs. We employ Singular Value Decomposition (SVD) or SVD++ algorithms to generate initial movie recommendations, implemented using the Surprise Python library and trained on the MovieLens-Latest-Small dataset. We compare the performance of the base algorithms with our LLM-enhanced versions using leave-one-out validation hit rates and cumulative hit rates. Additionally, to compare the performance of our framework against the current state-of-the-art recommendation systems, we use rating and ranking metrics with an item-based stratified 0.75 train, 0.25 test split. Our framework can generate preference profiles automatically based on users' favorite movies or allow manual preference specification for more personalized results. Using an automated approach, our framework overwhelmingly surpassed SVD and SVD++ on every evaluation metric used (e.g., improvements of up to ~6x in cumulative hit rate, ~3.7x in NDCG, etc.), albeit at the cost of a slight increase in computational overhead.",
        "translated": "传统推荐算法并非为处理用户通过文本表达的偏好而设计，例如“我喜欢轻松幽默的喜剧片”。近年来，大型语言模型（LLMs）已成为自然语言处理（NLP）领域最有前景的工具之一。本研究提出了一种新颖的框架，该框架模仿了亲密朋友根据对个人品味的了解进行物品推荐的方式。我们利用LLMs通过优化传统算法的输出，并将其与基于语言的用户偏好输入相结合，以增强电影推荐系统。我们采用奇异值分解（SVD）或 SVD++ 算法生成初始电影推荐，这些算法通过 Surprise Python 库实现，并在 MovieLens-Latest-Small 数据集上进行训练。我们使用留一法验证（leave-one-out validation）的命中率和累积命中率，比较了基础算法与我们LLM增强版本的性能。此外，为了将我们的框架性能与当前最先进的推荐系统进行比较，我们使用评分和排名指标，并采用基于物品的0.75训练集、0.25测试集分层划分。我们的框架可以根据用户喜欢的电影自动生成偏好画像，或允许手动指定偏好以获得更个性化的结果。采用自动化方法，我们的框架在所有使用的评估指标上都显著超越了SVD和SVD++（例如，累积命中率提升高达约6倍，NDCG提升约3.7倍等），尽管这会带来计算开销的略微增加。"
    },
    {
        "title": "EXPO: Stable Reinforcement Learning with Expressive Policies",
        "url": "http://arxiv.org/abs/2507.07986v1",
        "pub_date": "2025-07-10",
        "summary": "We study the problem of training and fine-tuning expressive policies with online reinforcement learning (RL) given an offline dataset. Training expressive policy classes with online RL present a unique challenge of stable value maximization. Unlike simpler Gaussian policies commonly used in online RL, expressive policies like diffusion and flow-matching policies are parameterized by a long denoising chain, which hinders stable gradient propagation from actions to policy parameters when optimizing against some value function. Our key insight is that we can address stable value maximization by avoiding direct optimization over value with the expressive policy and instead construct an on-the-fly RL policy to maximize Q-value. We propose Expressive Policy Optimization (EXPO), a sample-efficient online RL algorithm that utilizes an on-the-fly policy to maximize value with two parameterized policies -- a larger expressive base policy trained with a stable imitation learning objective and a light-weight Gaussian edit policy that edits the actions sampled from the base policy toward a higher value distribution. The on-the-fly policy optimizes the actions from the base policy with the learned edit policy and chooses the value maximizing action from the base and edited actions for both sampling and temporal-difference (TD) backup. Our approach yields up to 2-3x improvement in sample efficiency on average over prior methods both in the setting of fine-tuning a pretrained policy given offline data and in leveraging offline data to train online.",
        "translated": "我们研究了在给定离线数据集的情况下，结合在线强化学习（RL）训练和微调表达性策略的问题。结合在线RL训练表达性策略模型，在稳定价值最大化方面提出了独特的挑战。与在线RL中常用的简单高斯策略不同，像扩散策略和流匹配策略这样的表达性策略由长去噪链参数化，这在针对某个价值函数进行优化时，阻碍了从动作到策略参数的稳定梯度传播。我们的关键见解是，可以通过避免使用表达性策略直接优化价值，转而构建一个即时（on-the-fly）RL策略来最大化Q值，从而解决稳定价值最大化问题。\n\n我们提出了表达性策略优化（EXPO），这是一种样本高效的在线RL算法，它利用一个即时策略来最大化价值，该策略结合了两种参数化策略：一个更大的表达性基础策略（通过稳定的模仿学习目标进行训练），以及一个轻量级高斯编辑策略（用于修改从基础策略中采样的动作，使其趋向更高的价值分布）。该即时策略使用学习到的编辑策略优化来自基础策略的动作，并从基础动作和编辑后的动作中选择价值最大化的动作，用于采样和时序差分（TD）备份。我们的方法相较于现有方法，在给定离线数据微调预训练策略以及利用离线数据进行在线训练这两种场景下，样本效率平均提高了2-3倍。"
    },
    {
        "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents",
        "url": "http://arxiv.org/abs/2507.07957v1",
        "pub_date": "2025-07-10",
        "summary": "Although memory capabilities of AI agents are gaining increasing attention, existing solutions remain fundamentally limited. Most rely on flat, narrowly scoped memory components, constraining their ability to personalize, abstract, and reliably recall user-specific information over time. To this end, we introduce MIRIX, a modular, multi-agent memory system that redefines the future of AI memory by solving the field's most critical challenge: enabling language models to truly remember. Unlike prior approaches, MIRIX transcends text to embrace rich visual and multimodal experiences, making memory genuinely useful in real-world scenarios. MIRIX consists of six distinct, carefully structured memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and Knowledge Vault, coupled with a multi-agent framework that dynamically controls and coordinates updates and retrieval. This design enables agents to persist, reason over, and accurately retrieve diverse, long-term user data at scale. We validate MIRIX in two demanding settings. First, on ScreenshotVQA, a challenging multimodal benchmark comprising nearly 20,000 high-resolution computer screenshots per sequence, requiring deep contextual understanding and where no existing memory systems can be applied, MIRIX achieves 35% higher accuracy than the RAG baseline while reducing storage requirements by 99.9%. Second, on LOCOMO, a long-form conversation benchmark with single-modal textual input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing existing baselines. These results show that MIRIX sets a new performance standard for memory-augmented LLM agents. To allow users to experience our memory system, we provide a packaged application powered by MIRIX. It monitors the screen in real time, builds a personalized memory base, and offers intuitive visualization and secure local storage to ensure privacy.",
        "translated": "尽管AI智能体的记忆能力日益受到关注，但现有解决方案仍存在根本性局限。大多数依赖于扁平、范围狭窄的记忆组件，这限制了它们随着时间推移个性化、抽象化以及可靠回忆用户特定信息的能力。为此，我们提出MIRIX，一个模块化、多智能体记忆系统，它通过解决该领域最关键的挑战——使语言模型真正实现“记忆”——从而重新定义了AI记忆的未来。与现有方法不同，MIRIX超越了文本，拥抱丰富的视觉和多模态体验，使记忆在实际场景中真正有用。\n\nMIRIX由六种独特且精心设计的记忆类型构成：核心记忆（Core）、情景记忆（Episodic）、语义记忆（Semantic）、程序记忆（Procedural）、资源记忆（Resource Memory）和知识库（Knowledge Vault），并配备了一个多智能体框架，该框架动态控制和协调记忆的更新与检索。这种设计使得智能体能够大规模地持久化、推理并准确检索多样化、长期性的用户数据。\n\n我们在两个严苛的场景下验证了MIRIX的性能。首先，在ScreenshotVQA上，这是一个具有挑战性的多模态基准测试集，包含每个序列近20,000张高分辨率电脑截图，需要深入的上下文理解能力，且目前没有现有记忆系统可适用。在此基准上，MIRIX比RAG基线高出35%的准确率，同时将存储需求降低了99.9%。其次，在LOCOMO上，一个采用单模态文本输入的长篇对话基准测试集，MIRIX达到了85.4%的最新SOTA（State-of-the-Art）性能，远远超越了现有基线。这些结果表明，MIRIX为记忆增强型LLM智能体设定了新的性能标准。\n\n为了让用户体验我们的记忆系统，我们提供一个由MIRIX驱动的打包应用程序。该应用程序能实时监控屏幕，构建个性化的记忆库，并提供直观的可视化功能和安全的本地存储，以确保用户隐私。"
    },
    {
        "title": "DTECT: Dynamic Topic Explorer &amp; Context Tracker",
        "url": "http://arxiv.org/abs/2507.07910v1",
        "pub_date": "2025-07-10",
        "summary": "The explosive growth of textual data over time presents a significant challenge in uncovering evolving themes and trends. Existing dynamic topic modeling techniques, while powerful, often exist in fragmented pipelines that lack robust support for interpretation and user-friendly exploration. We introduce DTECT (Dynamic Topic Explorer &amp; Context Tracker), an end-to-end system that bridges the gap between raw textual data and meaningful temporal insights. DTECT provides a unified workflow that supports data preprocessing, multiple model architectures, and dedicated evaluation metrics to analyze the topic quality of temporal topic models. It significantly enhances interpretability by introducing LLM-driven automatic topic labeling, trend analysis via temporally salient words, interactive visualizations with document-level summarization, and a natural language chat interface for intuitive data querying. By integrating these features into a single, cohesive platform, DTECT empowers users to more effectively track and understand thematic dynamics. DTECT is open-source and available at https://github.com/AdhyaSuman/DTECT.",
        "translated": "文本数据随时间呈爆炸式增长，这给揭示不断演变的主题和趋势带来了重大挑战。现有的动态主题建模技术虽然功能强大，但通常存在于碎片化的流程中，缺乏对可解释性和用户友好探索的强大支持。为此，我们引入了 DTECT（动态主题探索器与上下文跟踪器），这是一个端到端系统，旨在弥合原始文本数据与有意义的时间洞察之间的鸿沟。DTECT 提供了一个统一的工作流，支持数据预处理、多种模型架构以及专用的评估指标，以分析时间主题模型的主题质量。它通过引入LLM驱动的自动主题标注、基于时间显著词的趋势分析、结合文档级摘要的交互式可视化，以及用于直观数据查询的自然语言聊天界面，显著增强了可解释性。通过将这些功能整合到一个统一的内聚平台中，DTECT 赋能用户更有效地追踪和理解主题动态。DTECT 是开源的，可在此处获取：https://github.com/AdhyaSuman/DTECT。"
    },
    {
        "title": "Improving Korean-English Cross-Lingual Retrieval: A Data-Centric Study\n  of Language Composition and Model Merging",
        "url": "http://arxiv.org/abs/2507.08480v1",
        "pub_date": "2025-07-11",
        "summary": "With the increasing utilization of multilingual text information, Cross-Lingual Information Retrieval (CLIR) has become a crucial research area. However, the impact of training data composition on both CLIR and Mono-Lingual Information Retrieval (IR) performance remains under-explored. To systematically investigate this data-centric aspect, we construct linguistically parallel Korean-English datasets and train retrieval models with various language combinations. Our experiments reveal that the language composition of training data significantly influences IR performance, exhibiting important inter-lingual correlations: CLIR performance improves with specific language pairs, while Mono-Lingual IR performance declines. Our work demonstrates that Model Merging can effectively mitigate this trade-off, achieving strong CLIR results while preserving Mono-Lingual IR capabilities. Our findings underscore the effects of linguistic configuration of training data on both CLIR and Mono-Lingual IR, and present Model Merging as a viable strategy to optimize performance across these tasks.",
        "translated": "随着多语言文本信息利用率的日益增长，跨语言信息检索（Cross-Lingual Information Retrieval, CLIR）已成为一个关键的研究领域。然而，训练数据构成对CLIR和单语言信息检索（Mono-Lingual Information Retrieval, IR）性能的影响尚未得到充分研究。\n\n为了系统地探究这一以数据为核心的问题，我们构建了语言对齐的韩语-英语数据集，并使用多种语言组合训练了检索模型。我们的实验揭示，训练数据的语言构成显著影响IR性能，并表现出重要的语际相关性：CLIR性能在特定语言对上有所提升，而单语言IR性能却有所下降。\n\n我们的工作表明，模型合并（Model Merging）可以有效缓解这种权衡取舍，在实现出色的CLIR结果的同时，保留了单语言IR能力。我们的研究结果强调了训练数据语言配置对CLIR和单语言IR两者的影响，并将模型合并提出为一种在这些任务中优化性能的可行策略。"
    },
    {
        "title": "CUE-RAG: Towards Accurate and Cost-Efficient Graph-Based RAG via\n  Multi-Partite Graph and Query-Driven Iterative Retrieval",
        "url": "http://arxiv.org/abs/2507.08445v1",
        "pub_date": "2025-07-11",
        "summary": "Despite the remarkable progress of Large Language Models (LLMs), their performance in question answering (QA) remains limited by the lack of domain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating external information, often from graph-structured data. However, existing graph-based RAG methods suffer from poor graph quality due to incomplete extraction and insufficient utilization of query information during retrieval. To overcome these limitations, we propose CUE-RAG, a novel approach that introduces (1) a multi-partite graph index incorporates text Chunks, knowledge Units, and Entities to capture semantic content at multiple levels of granularity, (2) a hybrid extraction strategy that reduces LLM token usage while still producing accurate and disambiguated knowledge units, and (3) Q-Iter, a query-driven iterative retrieval strategy that enhances relevance through semantic search and constrained graph traversal. Experiments on three QA benchmarks show that CUE-RAG significantly outperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy and 113.51% higher F1 score while reducing indexing costs by 72.58%. Remarkably, CUE-RAG matches or outperforms baselines even without using an LLM for indexing. These results demonstrate the effectiveness and cost-efficiency of CUE-RAG in advancing graph-based RAG systems.",
        "translated": "尽管大语言模型（LLMs）取得了显著进展，但其在问答（QA）方面的性能仍受限于缺乏领域特定和最新知识。检索增强生成（RAG）通过整合外部信息（通常来自图结构数据）来解决这一局限。然而，现有基于图的RAG方法存在图质量不佳的问题，这源于提取不完整以及在检索过程中对查询信息利用不足。\n\n为克服这些局限，我们提出了一种名为 CUE-RAG 的新颖方法，它引入了：(1) 一个多方图索引，整合了文本块（Chunks）、知识单元（knowledge Units）和实体（Entities），以在多个粒度级别捕获语义内容；(2) 一种混合提取策略，该策略在减少LLM token使用量的同时，仍能生成准确且消歧的知识单元；以及 (3) Q-Iter，一种查询驱动的迭代检索策略，通过语义搜索和受限图遍历来增强相关性。\n\n在三个QA基准数据集上的实验表明，CUE-RAG 显著优于最先进的基线方法，在将索引成本降低72.58%的同时，准确率最高提升99.33%，F1分数最高提升113.51%。值得注意的是，即使不使用LLM进行索引，CUE-RAG也能与基线方法持平或超越它们。这些结果证明了 CUE-RAG 在推动基于图的RAG系统方面的有效性和成本效益。"
    },
    {
        "title": "DS@GT at LongEval: Evaluating Temporal Performance in Web Search Systems\n  and Topics with Two-Stage Retrieval",
        "url": "http://arxiv.org/abs/2507.08360v1",
        "pub_date": "2025-07-11",
        "summary": "Information Retrieval (IR) models are often trained on static datasets, making them vulnerable to performance degradation as web content evolves. The DS@GT competition team participated in the Longitudinal Evaluation of Model Performance (LongEval) lab at CLEF 2025, which evaluates IR systems across temporally distributed web snapshots. Our analysis of the Qwant web dataset includes exploratory data analysis with topic modeling over time. The two-phase retrieval system employs sparse keyword searches, utilizing query expansion and document reranking. Our best system achieves an average NDCG@10 of 0.296 across the entire training and test dataset, with an overall best score of 0.395 on 2023-05. The accompanying source code for this paper is at https://github.com/dsgt-arc/longeval-2025",
        "translated": "信息检索 (IR) 模型通常在静态数据集上训练，这使得它们在网络内容不断演变时容易出现性能衰减。DS@GT 竞赛团队参加了 CLEF 2025 的模型性能纵向评估 (LongEval) 实验室，该实验室旨在评估跨时间分布的网络快照上的信息检索系统。我们对 Qwant 网络数据集的分析包括探索性数据分析，并辅以随时间演变的主题建模。该两阶段检索系统采用稀疏关键词搜索，并利用查询扩展和文档重排序技术。我们表现最佳的系统在整个训练集和测试集上实现了平均 NDCG@10 为 0.296 的分数，并在 2023 年 5 月取得了 0.395 的最佳整体得分。本文的配套源代码位于 https://github.com/dsgt-arc/longeval-2025。"
    },
    {
        "title": "Distillation versus Contrastive Learning: How to Train Your Rerankers",
        "url": "http://arxiv.org/abs/2507.08336v1",
        "pub_date": "2025-07-11",
        "summary": "Training text rerankers is crucial for information retrieval. Two primary strategies are widely used: contrastive learning (optimizing directly on ground-truth labels) and knowledge distillation (transferring knowledge from a larger reranker). While both have been studied in the literature, a clear comparison of their effectiveness for training cross-encoder rerankers under practical conditions is needed.   This paper empirically compares these strategies by training rerankers of different sizes and architectures using both methods on the same data, with a strong contrastive learning model acting as the distillation teacher. Our results show that knowledge distillation generally yields better in-domain and out-of-domain ranking performance than contrastive learning when distilling from a larger teacher model. This finding is consistent across student model sizes and architectures. However, distilling from a teacher of the same capacity does not provide the same advantage, particularly for out-of-domain tasks. These findings offer practical guidance for choosing a training strategy based on available teacher models. Therefore, we recommend using knowledge distillation to train smaller rerankers if a larger, more powerful teacher is accessible; in its absence, contrastive learning provides a strong and more reliable alternative otherwise.",
        "translated": "文本重排序器的训练对于信息检索至关重要。目前广泛采用两种主要策略：对比学习（直接基于真实标签进行优化）和知识蒸馏（从更大的重排序器模型中迁移知识）。尽管这两种方法在文献中均有研究，但仍需在实际条件下，对它们在训练交叉编码器重排序器方面的有效性进行明确比较。\n\n本文通过在相同数据集上使用这两种方法训练不同规模和架构的重排序器，并以一个强大的对比学习模型作为知识蒸馏的教师模型，从而对这些策略进行了实证比较。我们的结果表明，当从一个更大的教师模型进行蒸馏时，知识蒸馏通常比对比学习能产生更好的域内和域外排序性能。这一发现对于不同规模和架构的学生模型都保持一致。然而，从容量相同的教师模型进行蒸馏，并不能提供同样的优势，尤其是在域外任务上。这些研究结果为根据可用的教师模型选择训练策略提供了实用指导。因此，我们建议，如果有一个更大、更强大的教师模型可用，应使用知识蒸馏来训练小型重排序器；否则，在没有此类教师模型的情况下，对比学习则是一个强大且更可靠的替代方案。"
    },
    {
        "title": "Towards Efficient Quantity Retrieval from Text:an Approach via\n  Description Parsing and Weak Supervision",
        "url": "http://arxiv.org/abs/2507.08322v1",
        "pub_date": "2025-07-11",
        "summary": "Quantitative facts are continually generated by companies and governments, supporting data-driven decision-making. While common facts are structured, many long-tail quantitative facts remain buried in unstructured documents, making them difficult to access. We propose the task of Quantity Retrieval: given a description of a quantitative fact, the system returns the relevant value and supporting evidence. Understanding quantity semantics in context is essential for this task. We introduce a framework based on description parsing that converts text into structured (description, quantity) pairs for effective retrieval. To improve learning, we construct a large paraphrase dataset using weak supervision based on quantity co-occurrence. We evaluate our approach on a large corpus of financial annual reports and a newly annotated quantity description dataset. Our method significantly improves top-1 retrieval accuracy from 30.98 percent to 64.66 percent.",
        "translated": "公司和政府不断生成定量事实，支持数据驱动的决策制定。尽管常见事实是结构化的，但许多长尾定量事实仍埋藏在非结构化文档中，使其难以获取。我们提出“数量检索”任务：给定对某个定量事实的描述，系统返回相关数值和支持证据。理解上下文中的数量语义对于此任务至关重要。我们引入了一个基于描述解析的框架，该框架将文本转换为结构化的（描述，数量）对，以实现有效检索。为了改进学习，我们基于数量共现，利用弱监督构建了一个大型释义数据集。我们在一大规模金融年报语料库和一个新标注的数量描述数据集上评估了我们的方法。结果显示，我们的方法将 Top-1 检索准确率从 30.98% 显著提高到 64.66%。"
    },
    {
        "title": "Overview of the TREC 2021 deep learning track",
        "url": "http://arxiv.org/abs/2507.08191v1",
        "pub_date": "2025-07-10",
        "summary": "This is the third year of the TREC Deep Learning track. As in previous years, we leverage the MS MARCO datasets that made hundreds of thousands of human annotated training labels available for both passage and document ranking tasks. In addition, this year we refreshed both the document and the passage collections which also led to a nearly four times increase in the document collection size and nearly $16$ times increase in the size of the passage collection. Deep neural ranking models that employ large scale pretraininig continued to outperform traditional retrieval methods this year. We also found that single stage retrieval can achieve good performance on both tasks although they still do not perform at par with multistage retrieval pipelines. Finally, the increase in the collection size and the general data refresh raised some questions about completeness of NIST judgments and the quality of the training labels that were mapped to the new collections from the old ones which we discuss in this report.",
        "translated": "这是TREC深度学习赛道的第三年。与往年一样，我们利用MS MARCO数据集，该数据集为段落和文档排序任务提供了数十万个人工标注的训练标签。此外，今年我们更新了文档和段落集合，这使得文档集合规模增加了近四倍，段落集合规模增加了近16倍。采用大规模预训练的深度神经网络排序模型今年继续优于传统检索方法。我们还发现单阶段检索在这两项任务上都能取得良好性能，尽管其表现仍未能与多阶段检索管道媲美。最后，集合规模的增加和整体数据更新引发了一些问题，关于NIST判定的完整性以及从旧集合映射到新集合的训练标签的质量，我们将在本报告中讨论这些问题。"
    },
    {
        "title": "GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs",
        "url": "http://arxiv.org/abs/2507.08107v1",
        "pub_date": "2025-07-10",
        "summary": "We propose a new approach for generating SPARQL queries on RDF knowledge graphs from natural language questions or keyword queries, using a large language model. Our approach does not require fine-tuning. Instead, it uses the language model to explore the knowledge graph by strategically executing SPARQL queries and searching for relevant IRIs and literals. We evaluate our approach on a variety of benchmarks (for knowledge graphs of different kinds and sizes) and language models (of different scales and types, commercial as well as open-source) and compare it with existing approaches. On Wikidata we reach state-of-the-art results on multiple benchmarks, despite the zero-shot setting. On Freebase we come close to the best few-shot methods. On other, less commonly evaluated knowledge graphs and benchmarks our approach also performs well overall. We conduct several additional studies, like comparing different ways of searching the graphs, incorporating a feedback mechanism, or making use of few-shot examples.",
        "translated": "我们提出了一种基于大语言模型的新方法，用于从自然语言问题或关键词查询生成RDF知识图谱上的SPARQL查询。我们的方法无需微调，而是利用大语言模型通过策略性地执行SPARQL查询并搜索相关的IRI和字面量来探索知识图谱。\n\n我们在一系列基准测试（涵盖不同种类和规模的知识图谱）以及不同规模和类型（包括商业模型和开源模型）的大语言模型上，对所提出的方法进行了评估，并与现有方法进行了比较。在Wikidata上，尽管是在零样本设置下，我们的方法在多个基准测试上达到了最先进（state-of-the-art）的结果。在Freebase上，我们的方法也接近了最好的少量样本（few-shot）方法的表现。在其他较少评估的知识图谱和基准测试上，我们的方法总体表现也良好。我们还进行了几项额外研究，包括比较不同的图搜索方式、引入反馈机制，以及利用少量样本示例等。"
    },
    {
        "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout\n  in Distance Learning",
        "url": "http://arxiv.org/abs/2507.10421v1",
        "pub_date": "2025-07-14",
        "summary": "School dropout is a serious problem in distance learning, where early detection is crucial for effective intervention and student perseverance. Predicting student dropout using available educational data is a widely researched topic in learning analytics. Our partner's distance learning platform highlights the importance of integrating diverse data sources, including socio-demographic data, behavioral data, and sentiment analysis, to accurately predict dropout risks. In this paper, we introduce a novel model that combines sentiment analysis of student comments using the Bidirectional Encoder Representations from Transformers (BERT) model with socio-demographic and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We fine-tuned BERT on student comments to capture nuanced sentiments, which were then merged with key features selected using feature importance techniques in XGBoost. Our model was tested on unseen data from the next academic year, achieving an accuracy of 84\\%, compared to 82\\% for the baseline model. Additionally, the model demonstrated superior performance in other metrics, such as precision and F1-score. The proposed method could be a vital tool in developing personalized strategies to reduce dropout rates and encourage student perseverance",
        "translated": "学生辍学是远程学习中的一个严重问题，早期预警对于有效干预和学生持续学习至关重要。利用现有教育数据预测学生辍学是学习分析领域一个广泛研究的课题。我们的合作方远程学习平台强调了整合多样化数据源的重要性，包括社会人口统计数据、行为数据和情感分析，以准确预测辍学风险。\n\n在本文中，我们提出了一种新颖模型，它将利用基于Transformer的双向编码表示模型（BERT）对学生评论进行情感分析，并结合通过极端梯度提升（XGBoost）技术分析的社会人口统计数据和行为数据。我们基于学生评论对BERT进行了微调，以捕捉细微情感，然后将这些情感与利用XGBoost中特征重要性技术选取的关键特征合并。我们的模型在来自下一个学年的未见数据上进行了测试，其准确率达到84%，而基线模型为82%。此外，该模型在其他指标上，例如精确率和F1分数，也表现出更优异的性能。所提出的方法可能成为制定个性化策略的关键工具，以减少辍学率并鼓励学生持续学习。"
    },
    {
        "title": "Am I on the Right Track? What Can Predicted Query Performance Tell Us\n  about the Search Behaviour of Agentic RAG",
        "url": "http://arxiv.org/abs/2507.10411v1",
        "pub_date": "2025-07-14",
        "summary": "Agentic Retrieval-Augmented Generation (RAG) is a new paradigm where the reasoning model decides when to invoke a retriever (as a \"tool\") when answering a question. This paradigm, exemplified by recent research works such as Search-R1, enables the model to decide when to search and obtain external information. However, the queries generated by such Agentic RAG models and the role of the retriever in obtaining high-quality answers remain understudied. To this end, this initial study examines the applicability of query performance prediction (QPP) within the recent Agentic RAG models Search-R1 and R1-Searcher. We find that applying effective retrievers can achieve higher answer quality within a shorter reasoning process. Moreover, the QPP estimates of the generated queries, used as an approximation of their retrieval quality, are positively correlated with the quality of the final answer. Ultimately, our work is a step towards adaptive retrieval within Agentic RAG, where QPP is used to inform the model if the retrieved results are likely to be useful.",
        "translated": "代理检索增强生成（Agentic Retrieval-Augmented Generation, RAG）是一种新的范式，其中推理模型在回答问题时，能够自主决定何时调用检索器（作为“工具”）。这种范式以Search-R1等近期研究工作为代表，使模型能够自主决定何时进行搜索并获取外部信息。然而，此类代理RAG模型所生成的查询，以及检索器在获取高质量答案中的作用，目前仍未得到充分研究。\n\n为此，本初步研究旨在探讨查询性能预测（Query Performance Prediction, QPP）在近期代理RAG模型Search-R1和R1-Searcher中的适用性。我们发现，应用高效的检索器能够在更短的推理过程中实现更高的答案质量。此外，将生成的查询的QPP估计值（用作其检索质量的近似值）与最终答案的质量进行关联，发现二者呈正相关。最终，我们的工作是迈向代理RAG中自适应检索的重要一步，其中QPP被用于告知模型检索到的结果是否可能有用。"
    },
    {
        "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources",
        "url": "http://arxiv.org/abs/2507.10403v1",
        "pub_date": "2025-07-14",
        "summary": "Retrieving relevant imagery from vast satellite archives is crucial for applications like disaster response and long-term climate monitoring. However, most text-to-image retrieval systems are limited to RGB data, failing to exploit the unique physical information captured by other sensors, such as the all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the spectral signatures in optical multispectral data. To bridge this gap, we introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1 SAR and Sentinel-2 multispectral images paired with structured textual annotations for land cover, land use, and crisis events harmonized from authoritative land cover systems (CORINE and Dynamic World) and crisis-specific sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining), a novel framework that uses text as a bridge to align unpaired optical and SAR images into a unified embedding space. Our experiments show that CLOSP achieves a new state-of-the-art, improving retrieval nDGC by 54% over existing models. Additionally, we find that the unified training strategy overcomes the inherent difficulty of interpreting SAR imagery by transferring rich semantic knowledge from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which integrates geographic coordinates into our framework, creates a powerful trade-off between generality and specificity: while the CLOSP excels at general semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving location-dependent crisis events and rare geographic features. This work highlights that the integration of diverse sensor data and geographic context is essential for unlocking the full potential of remote sensing archives.",
        "translated": "从海量卫星影像档案中检索相关影像，对于灾害响应和长期气候监测等应用至关重要。然而，大多数文本到图像检索系统仅限于RGB数据，未能充分利用其他传感器捕获的独特物理信息，例如合成孔径雷达（SAR）的全天候结构敏感性或光学多光谱数据中的光谱特征。为此，我们引入了CrisisLandMark，这是一个新的大规模语料库，包含超过647,000张Sentinel-1 SAR和Sentinel-2多光谱影像，并配有结构化的文本标注，用于描述土地覆盖、土地利用和危机事件，这些标注从权威的土地覆盖系统（CORINE和Dynamic World）以及危机特定来源中整合而来。\n\n接着，我们提出了CLOSP（Contrastive Language Optical SAR Pretraining），这是一个新颖的框架，它利用文本作为桥梁，将未配对的光学和SAR影像对齐到一个统一的嵌入空间中。我们的实验表明，CLOSP取得了新的最先进水平，相较于现有模型，将检索nDGC提高了54%。此外，我们发现统一的训练策略通过间接交互，从光学领域转移丰富的语义知识，从而克服了解读SAR影像的固有难题。\n\n进一步地，将地理坐标整合到我们框架中的GeoCLOSP，在通用性和特异性之间实现了强大的权衡：尽管CLOSP擅长处理通用语义任务，但GeoCLOSP则成为一个专业专家，用于检索依赖位置的危机事件和稀有地理特征。这项工作强调，整合多样传感器数据和地理上下文对于释放遥感档案的全部潜力至关重要。"
    },
    {
        "title": "Riding the Carousel: The First Extensive Eye Tracking Analysis of\n  Browsing Behavior in Carousel Recommenders",
        "url": "http://arxiv.org/abs/2507.10135v1",
        "pub_date": "2025-07-14",
        "summary": "Carousels have become the de-facto interface in online services. However, there is a lack of research in carousels, particularly examining how recommender systems may be designed differently than the traditional single-list interfaces. One of the key elements for understanding how to design a system for a particular interface is understanding how users browse. For carousels, users may browse in a number of different ways due to the added complexity of multiple topic defined-lists and swiping to see more items.   Eye tracking is the key to understanding user behavior by providing valuable, direct information on how users see and navigate. In this work, we provide the first extensive analysis of the eye tracking behavior in carousel recommenders under the free-browsing setting. To understand how users browse, we examine the following research questions : 1) where do users start browsing, 2) how do users transition from item to item within the same carousel and across carousels, and 3) how does genre preference impact transitions?   This work addresses a gap in the field and provides the first extensive empirical results of eye tracked browsing behavior in carousels for improving recommenders. Taking into account the insights learned from the above questions, our final contribution is to provide suggestions to help carousel recommender system designers optimize their systems for user browsing behavior. The most important suggestion being to reorder the ranked item positions to account for browsing after swiping.These contributions aim not only to help improve current systems, but also to encourage and allow the design of new user models, systems, and metrics that are better suited to the complexity of carousel interfaces.",
        "translated": "轮播图已成为在线服务中事实上的主流界面。然而，针对轮播图的研究却很匮乏，尤其是缺乏探讨推荐系统如何能与传统的单列表界面有不同设计的研究。理解如何为特定界面设计系统的关键要素之一是了解用户如何进行浏览。对于轮播图，因其包含多个主题分类列表以及滑动查看更多项目的额外复杂性，用户可能会以多种不同方式进行浏览。眼动追踪是理解用户行为的关键，因为它能提供关于用户如何查看和导航的有价值的直接信息。在这项工作中，我们首次对自由浏览设置下的轮播推荐系统中的眼动行为进行了广泛分析。为了理解用户如何浏览，我们探讨了以下研究问题：1) 用户从何处开始浏览，2) 用户如何在同一轮播图内以及跨轮播图在项目间进行转换，以及 3) 类型偏好如何影响转换？\n\n本工作填补了该领域的一个空白，并首次提供了用于改进推荐系统的轮播图中眼动追踪浏览行为的广泛实证结果。考虑到从上述问题中获得的见解，我们最后的贡献是提供建议，以帮助轮播推荐系统设计者优化其系统，使其更好地适应用户浏览行为。其中最重要的建议是重新排序项目的排名位置，以适应用户在滑动后的浏览行为。这些贡献不仅旨在帮助改进现有系统，而且旨在鼓励并允许设计出更适合轮播图界面复杂性的新型用户模型、系统和评估指标。"
    },
    {
        "title": "User Long-Term Multi-Interest Retrieval Model for Recommendation",
        "url": "http://arxiv.org/abs/2507.10097v1",
        "pub_date": "2025-07-14",
        "summary": "User behavior sequence modeling, which captures user interest from rich historical interactions, is pivotal for industrial recommendation systems. Despite breakthroughs in ranking-stage models capable of leveraging ultra-long behavior sequences with length scaling up to thousands, existing retrieval models remain constrained to sequences of hundreds of behaviors due to two main challenges. One is strict latency budget imposed by real-time service over large-scale candidate pool. The other is the absence of target-aware mechanisms and cross-interaction architectures, which prevent utilizing ranking-like techniques to simplify long sequence modeling. To address these limitations, we propose a new framework named User Long-term Multi-Interest Retrieval Model(ULIM), which enables thousand-scale behavior modeling in retrieval stages. ULIM includes two novel components: 1)Category-Aware Hierarchical Dual-Interest Learning partitions long behavior sequences into multiple category-aware subsequences representing multi-interest and jointly optimizes long-term and short-term interests within specific interest cluster. 2)Pointer-Enhanced Cascaded Category-to-Item Retrieval introduces Pointer-Generator Interest Network(PGIN) for next-category prediction, followed by next-item retrieval upon the top-K predicted categories. Comprehensive experiments on Taobao dataset show that ULIM achieves substantial improvement over state-of-the-art methods, and brings 5.54% clicks, 11.01% orders and 4.03% GMV lift for Taobaomiaosha, a notable mini-app of Taobao.",
        "translated": "用户行为序列建模通过捕捉用户丰富历史交互中的兴趣，对工业推荐系统至关重要。尽管排序阶段模型在利用长度达数千的超长行为序列方面取得了突破，但现有召回模型由于两大主要挑战，仍局限于数百个行为的序列。其一是实时服务在大型候选池上施加的严格延迟预算。另一个是缺乏目标感知机制和跨交互架构，这阻碍了利用类似排序的技术来简化长序列建模。\n\n为解决这些限制，我们提出了一个名为用户长期多兴趣召回模型（ULIM）的新框架，该框架能够在召回阶段实现千级别行为建模。ULIM包含两个新颖组件：1) **类别感知分层双兴趣学习**：将长行为序列划分为多个代表多兴趣的类别感知子序列，并在特定兴趣簇内共同优化长期和短期兴趣。2) **指针增强级联类别到物品召回**：引入指针生成兴趣网络（PGIN）进行下一类别预测，随后基于预测出的Top-K类别进行下一物品召回。\n\n在淘宝数据集上的综合实验表明，ULIM相比最先进的方法取得了显著改进，并为淘宝的一个知名小程序——淘宝秒杀带来了5.54%的点击量、11.01%的订单量和4.03%的GMV提升。"
    },
    {
        "title": "PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware\n  Query Optimization",
        "url": "http://arxiv.org/abs/2507.10057v1",
        "pub_date": "2025-07-14",
        "summary": "Scientific paper retrieval, particularly framed as document-to-document retrieval, aims to identify relevant papers in response to a long-form query paper, rather than a short query string. Previous approaches to this task have focused on abstracts, embedding them into dense vectors as surrogates for full documents and calculating similarity across them, although abstracts provide only sparse and high-level summaries. To address this, we propose PRISM, a novel document-to-document retrieval method that introduces multiple, fine-grained representations for both the query and candidate papers. In particular, each query paper is decomposed into multiple aspect-specific views and individually embedded, which are then matched against candidate papers similarity segmented to consider their multifaceted dimensions. Moreover, we present SciFullBench, a novel benchmark in which the complete and segmented context of full papers for both queries and candidates is available. Then, experimental results show that PRISM improves performance by an average of 4.3% over existing retrieval baselines.",
        "translated": "科学论文检索，特别是当其被定义为文档到文档检索（document-to-document retrieval）时，旨在针对一篇长篇查询论文而非简短查询字符串，来识别相关的论文。以往针对该任务的方法侧重于使用论文摘要，将其嵌入为稠密向量作为全文的替代，并通过计算它们之间的相似度进行检索，尽管摘要仅能提供稀疏且高层次的总结。为解决此问题，我们提出了PRISM，一种新颖的文档到文档检索方法，它为查询论文和候选论文引入了多个细粒度表示。具体来说，每篇查询论文被分解为多个特定方面的视图并分别嵌入，然后与经相似性分段以考量其多方面维度的候选论文进行匹配。此外，我们提出了SciFullBench，这是一个新颖的基准，其中包含查询和候选论文的完整全文及其分段上下文。实验结果表明，PRISM相较于现有检索基线，性能平均提升了4.3%。"
    },
    {
        "title": "SLIF-MR: Self-loop Iterative Fusion of Heterogeneous Auxiliary\n  Information for Multimodal Recommendation",
        "url": "http://arxiv.org/abs/2507.09998v1",
        "pub_date": "2025-07-14",
        "summary": "Knowledge graphs (KGs) and multimodal item information, which respectively capture relational and attribute features, play a crucial role in improving recommender system accuracy. Recent studies have attempted to integrate them via multimodal knowledge graphs (MKGs) to further enhance recommendation performance. However, existing methods typically freeze the MKG structure during training, which limits the full integration of structural information from heterogeneous graphs (e.g., KG and user-item interaction graph), and results in sub-optimal performance. To address this challenge, we propose a novel framework, termed Self-loop Iterative Fusion of Heterogeneous Auxiliary Information for Multimodal Recommendation (SLIF-MR), which leverages item representations from previous training epoch as feedback signals to dynamically optimize the heterogeneous graph structures composed of KG, multimodal item feature graph, and user-item interaction graph. Through this iterative fusion mechanism, both user and item representations are refined, thus improving the final recommendation performance. Specifically, based on the feedback item representations, SLIF-MR constructs an item-item correlation graph, then integrated into the establishment process of heterogeneous graphs as additional new structural information in a self-loop manner. Consequently, the internal structures of heterogeneous graphs are updated with the feedback item representations during training. Moreover, a semantic consistency learning strategy is proposed to align heterogeneous item representations across modalities. The experimental results show that SLIF-MR significantly outperforms existing methods, particularly in terms of accuracy and robustness.",
        "translated": "知识图谱（KGs）和多模态物品信息分别捕获关系特征和属性特征，在提升推荐系统准确性方面发挥着关键作用。近期研究尝试通过多模态知识图谱（MKGs）对其进行整合，以进一步提升推荐性能。然而，现有方法通常在训练过程中冻结MKG结构，这限制了异构图（如知识图谱和用户-物品交互图）结构信息的充分整合，并导致次优性能。\n\n为解决这一挑战，我们提出了一种新颖的框架，命名为“异构辅助信息自循环迭代融合多模态推荐框架”（SLIF-MR）。该框架利用来自前一训练周期的物品表示作为反馈信号，动态优化由知识图谱、多模态物品特征图和用户-物品交互图构成的异构图结构。通过这种迭代融合机制，用户和物品表示均得到精炼，从而提升最终的推荐性能。具体而言，基于反馈的物品表示，SLIF-MR构建了一个物品-物品相关图，然后将其作为额外的新结构信息，以自循环的方式整合到异构图的构建过程中。因此，异构图的内部结构在训练过程中随着反馈的物品表示而更新。此外，我们提出了一种语义一致性学习策略，用于对齐跨模态的异构物品表示。实验结果表明，SLIF-MR显著优于现有方法，尤其是在准确性和鲁棒性方面。"
    },
    {
        "title": "Non-parametric Graph Convolution for Re-ranking in Recommendation\n  Systems",
        "url": "http://arxiv.org/abs/2507.09969v1",
        "pub_date": "2025-07-14",
        "summary": "Graph knowledge has been proven effective in enhancing item rankings in recommender systems (RecSys), particularly during the retrieval stage. However, its application in the ranking stage, especially when richer contextual information in user-item interactions is available, remains underexplored. A major challenge lies in the substantial computational cost associated with repeatedly retrieving neighborhood information from billions of items stored in distributed systems. This resource-intensive requirement makes it difficult to scale graph-based methods in practical RecSys. To bridge this gap, we first demonstrate that incorporating graphs in the ranking stage improves ranking qualities. Notably, while the improvement is evident, we show that the substantial computational overheads entailed by graphs are prohibitively expensive for real-world recommendations. In light of this, we propose a non-parametric strategy that utilizes graph convolution for re-ranking only during test time. Our strategy circumvents the notorious computational overheads from graph convolution during training, and utilizes structural knowledge hidden in graphs on-the-fly during testing. It can be used as a plug-and-play module and easily employed to enhance the ranking ability of various ranking layers of a real-world RecSys with significantly reduced computational overhead. Through comprehensive experiments across four benchmark datasets with varying levels of sparsity, we demonstrate that our strategy yields noticeable improvements (i.e., 8.1% on average) during testing time with little to no additional computational overheads (i.e., 0.5 on average). Code: https://github.com/zyouyang/RecSys2025_NonParamGC.git",
        "translated": "图谱知识已被证明在增强推荐系统（RecSys）的物品排序方面非常有效，尤其是在召回阶段。然而，其在排序阶段的应用，特别是在用户-物品交互中可获得更丰富上下文信息的情况下，仍未得到充分探索。一个主要挑战在于，从分布式系统中存储的数十亿物品中重复检索邻居信息所伴随的巨大计算开销。这种资源密集型要求使得基于图的方法难以在实际推荐系统中扩展。\n\n为弥补这一空白，我们首先证明了在排序阶段引入图结构确实能提升排序质量。值得注意的是，尽管这种提升显而易见，但我们表明图结构所带来的巨大计算开销对于实际推荐而言是高昂得令人无法承受的。鉴于此，我们提出了一种非参数策略，该策略仅在测试时利用图卷积进行重排序。我们的策略规避了训练阶段图卷积带来的巨大计算开销，并在测试时即时地利用了隐藏在图中的结构知识。它可以用作一个即插即用模块，并能轻松应用于提升真实推荐系统中各种排序层的排序能力，同时显著降低计算开销。\n\n通过在四个不同稀疏程度的基准数据集上进行全面实验，我们证明了我们的策略在测试阶段带来了显著的提升（平均8.1%），而几乎没有额外的计算开销（平均0.5）。\n\n代码：https://github.com/zyouyang/RecSys2025_NonParamGC.git"
    },
    {
        "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for\n  Rehearsal-Free Generative Retrieval over Dynamic Corpora",
        "url": "http://arxiv.org/abs/2507.09924v1",
        "pub_date": "2025-07-14",
        "summary": "Continually updating model-based indexes in generative retrieval with new documents remains challenging, as full retraining is computationally expensive and impractical under resource constraints. We propose MixLoRA-DSI, a novel framework that combines an expandable mixture of Low-Rank Adaptation experts with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead of allocating new experts for each new corpus, our proposed expansion strategy enables sublinear parameter growth by selectively introducing new experts only when significant number of OOD documents are detected. Experiments on NQ320k and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update baselines, with minimal parameter overhead and substantially lower training costs.",
        "translated": "在生成式检索中，当引入新文档时，持续更新基于模型的索引仍具挑战性，因为全面再训练计算成本高昂，且在资源受限的情况下不切实际。我们提出了 MixLoRA-DSI，一个新颖的框架，它结合了可扩展的低秩适应（Low-Rank Adaptation, LoRA）专家混合模型和逐层域外数据（out-of-distribution, OOD）驱动的扩展策略。我们提出的扩展策略与为每个新语料库分配新专家不同，它通过仅在检测到大量域外文档时选择性地引入新专家，从而实现了亚线性参数增长。在 NQ320k 和 MS MARCO Passage 数据集上的实验表明，MixLoRA-DSI 在参数开销最小、训练成本显著降低的情况下，性能优于全模型更新的基线方法。"
    },
    {
        "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex\n  Scientific Question Answering in Ecology",
        "url": "http://arxiv.org/abs/2507.10522v1",
        "pub_date": "2025-07-14",
        "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system for automated scientific synthesis that supports recursive, depth- and breadth-controlled exploration of original research questions -- enhancing search diversity and nuance in the retrieval of relevant scientific literature. Unlike conventional retrieval-augmented generation pipelines, DeepResearch enables user-controllable synthesis with transparent reasoning and parameter-driven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor. Applied to 49 ecological research questions, DeepResearch achieves up to a 21-fold increase in source integration and a 14.9-fold rise in sources integrated per 1,000 words. High-parameter settings yield expert-level analytical depth and contextual diversity.   Source code available at: https://github.com/sciknoworg/deep-research.",
        "translated": "我们引入了 DeepResearch$^{\\text{Eco}}$，这是一种新颖的、基于智能体（agentic）的大语言模型（LLM）系统，专为自动化科学综合而设计。它支持对原创研究问题进行递归式、深度和广度可控的探索，从而增强相关科学文献检索的搜索多样性和精妙性。\n\n与传统的检索增强生成（RAG）流程不同，DeepResearch 实现了用户可控的综合，具备透明的推理过程和参数驱动的可配置性，从而促进了领域特定证据的高通量整合，同时保持了分析的严谨性。将 DeepResearch 应用于49个生态学研究问题时，其来源整合量增加了高达21倍，每千字整合的来源数量增加了14.9倍。在高参数设置下，系统产生了专家级的分析深度和上下文多样性。\n\n源代码可在此处获取：https://github.com/sciknoworg/deep-research。"
    },
    {
        "title": "KG-Attention: Knowledge Graph-Guided Attention at Test-Time via\n  Bidirectional Information Aggregation",
        "url": "http://arxiv.org/abs/2507.08704v1",
        "pub_date": "2025-07-11",
        "summary": "Knowledge graphs (KGs) play a critical role in enhancing large language models (LLMs) by introducing structured and grounded knowledge into the learning process. However, most existing KG-enhanced approaches rely on parameter-intensive fine-tuning, which risks catastrophic forgetting and degrades the pretrained model's generalization. Moreover, they exhibit limited adaptability to real-time knowledge updates due to their static integration frameworks. To address these issues, we introduce the first test-time KG-augmented framework for LLMs, built around a dedicated knowledge graph-guided attention (KGA) module that enables dynamic knowledge fusion without any parameter updates. The proposed KGA module augments the standard self-attention mechanism with two synergistic pathways: outward and inward aggregation. Specifically, the outward pathway dynamically integrates external knowledge into input representations via input-driven KG fusion. This inward aggregation complements the outward pathway by refining input representations through KG-guided filtering, suppressing task-irrelevant signals and amplifying knowledge-relevant patterns. Importantly, while the outward pathway handles knowledge fusion, the inward path selects the most relevant triples and feeds them back into the fusion process, forming a closed-loop enhancement mechanism. By synergistically combining these two pathways, the proposed method supports real-time knowledge fusion exclusively at test-time, without any parameter modification. Extensive experiments on five benchmarks verify the comparable knowledge fusion performance of KGA.",
        "translated": "知识图谱（KGs）通过在学习过程中引入结构化、接地气的知识，在增强大语言模型（LLMs）方面发挥着关键作用。然而，现有的大多数知识图谱增强方法都依赖于参数密集型微调，这不仅存在灾难性遗忘的风险，还会损害预训练模型的泛化能力。此外，由于其静态集成框架，这些方法对实时知识更新的适应性有限。\n\n为解决这些问题，我们提出首个针对大语言模型的测试时知识图谱增强框架。该框架围绕一个专门设计的知识图谱引导注意力（KGA）模块构建，能够在无需任何参数更新的情况下实现动态知识融合。所提出的KGA模块通过“向外聚合”和“向内聚合”两个协同通路，对标准自注意力机制进行了增强。具体而言，向外通路通过输入驱动的知识图谱融合，将外部知识动态地集成到输入表示中。向内聚合则通过知识图谱引导的过滤，对输入表示进行精炼，抑制任务无关信号，并增强知识相关模式，以此补充向外通路。值得注意的是，向外通路负责知识融合，而向内通路则负责选择最相关的三元组并将其反馈回融合过程，从而形成一个闭环增强机制。通过协同结合这两个通路，所提出的方法仅在测试时便可支持实时知识融合，且无需对模型参数进行任何修改。在五个基准数据集上进行的广泛实验验证了KGA具有可与现有方法媲美的知识融合性能。"
    },
    {
        "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
        "url": "http://arxiv.org/abs/2507.10524v1",
        "pub_date": "2025-07-14",
        "summary": "Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.",
        "translated": "语言模型的规模化发展带来了令人印象深刻的能力，但随之而来的计算和内存需求使得训练和部署成本高昂。现有的效率提升方法通常仅关注参数共享或自适应计算，如何同时实现两者仍是一个悬而未决的问题。\n\n我们引入了**递归混合模型（Mixture-of-Recursions, MoR）**，这是一个统一框架，在单个递归Transformer中结合了这两种效率提升途径。MoR通过在递归步骤中重用共享层栈来提高参数效率，同时，轻量级路由器通过动态为单个令牌（token）分配不同的递归深度来实现自适应的令牌级推理。这使得MoR能够将二次复杂度的注意力计算仅集中在给定递归深度下仍活跃的令牌之间，并通过选择性缓存这些令牌的键值对（KV对）来进一步提升内存访问效率。除了这些核心机制，我们还提出了一种KV共享变体，它重用第一次递归中的KV对，旨在减少预填充延迟和内存占用。\n\n在1.35亿至17亿参数的模型规模范围内，MoR形成了一个新的帕累托前沿：在相同的训练浮点运算数（FLOPs）和更小的模型尺寸下，它显著降低了验证困惑度，提高了少样本准确率，同时与传统及现有递归基线模型相比，实现了更高的吞吐量。这些成果表明，MoR是实现大型模型性能而无需承担高昂成本的有效途径。"
    },
    {
        "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data",
        "url": "http://arxiv.org/abs/2507.12425v1",
        "pub_date": "2025-07-16",
        "summary": "Organizations increasingly rely on proprietary enterprise data, including HR records, structured reports, and tabular documents, for critical decision-making. While Large Language Models (LLMs) have strong generative capabilities, they are limited by static pretraining, short context windows, and challenges in processing heterogeneous data formats. Conventional Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but often struggle with structured and semi-structured data.   This work proposes an advanced RAG framework that combines hybrid retrieval strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by metadata-aware filtering with SpaCy NER and cross-encoder reranking. The framework applies semantic chunking to maintain textual coherence and retains tabular data structures to preserve row-column integrity. Quantized indexing optimizes retrieval efficiency, while human-in-the-loop feedback and conversation memory improve adaptability.   Experiments on enterprise datasets show notable improvements: Precision@5 increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74), and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness (4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale. These results demonstrate the framework's effectiveness in delivering accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work includes extending to multimodal data and integrating agent-based retrieval. The source code will be released at https://github.com/CheerlaChandana/Enterprise-Chatbot",
        "translated": "组织越来越依赖专有企业数据，包括人力资源记录、结构化报告和表格文档，以支持关键决策。尽管大语言模型（LLM）具有强大的生成能力，但它们受限于静态预训练、短上下文窗口以及处理异构数据格式的挑战。传统的检索增强生成（RAG）框架虽然解决了其中一些不足，但通常难以处理结构化和半结构化数据。\n\n本工作提出了一种高级RAG框架，该框架结合了使用稠密嵌入（all-mpnet-base-v2）和BM25的混合检索策略，并通过SpaCy命名实体识别（NER）进行的元数据感知过滤以及交叉编码器重排序得到了增强。该框架应用语义分块以保持文本连贯性，并保留表格数据结构以维护行列完整性。量化索引优化了检索效率，而人在回路反馈和会话记忆则提高了适应性。\n\n在企业数据集上的实验显示出显著改进：精确率@5增加了15%（90对比75），召回率@5增加了13%（87对比74），平均倒数排名（MRR）增加了16%（0.85对比0.69）。定性评估结果显示，在5点李克特量表上，忠实性（4.6对比3.0）、完整性（4.2对比2.5）和相关性（4.5对比3.2）均获得了更高的分数。这些结果表明，该框架在为企业任务提供准确、全面且上下文相关的响应方面是有效的。未来的工作包括扩展到多模态数据和集成基于代理的检索。源代码将发布于 https://github.com/CheerlaChandana/Enterprise-Chatbot。"
    },
    {
        "title": "Developing Visual Augmented Q&amp;A System using Scalable Vision Embedding\n  Retrieval &amp; Late Interaction Re-ranker",
        "url": "http://arxiv.org/abs/2507.12378v1",
        "pub_date": "2025-07-16",
        "summary": "Traditional information extraction systems face challenges with text only language models as it does not consider infographics (visual elements of information) such as tables, charts, images etc. often used to convey complex information to readers. Multimodal LLM (MLLM) face challenges of finding needle in the haystack problem i.e., either longer context length or substantial number of documents as search space. Late interaction mechanism over visual language models has shown state of the art performance in retrieval-based vision augmented Q&amp;A tasks. There are yet few challenges using it for RAG based multi-modal Q&amp;A. Firstly, many popular and widely adopted vector databases do not support native multi-vector retrieval. Secondly, late interaction requires computation which inflates space footprint and can hinder enterprise adoption. Lastly, the current state of late interaction mechanism does not leverage the approximate neighbor search indexing methods for large speed ups in retrieval process. This paper explores a pragmatic approach to make vision retrieval process scalable and efficient without compromising on performance quality. We propose multi-step custom implementation utilizing widely adopted hybrid search (metadata &amp; embedding) and state of the art late interaction re-ranker to retrieve best matching pages. Finally, MLLM are prompted as reader to generate answers from contextualized best matching pages. Through experiments, we observe that the proposed design is scalable (significant speed up) and stable (without degrading performance quality), hence can be used as production systems at enterprises.",
        "translated": "传统信息抽取系统面临挑战，因为它们仅基于文本的语言模型不考虑信息图（信息的视觉元素），例如表格、图表、图像等，这些元素常用于向读者传达复杂信息。多模态大语言模型（MLLM）面临着大海捞针的挑战，即在作为搜索空间的文档中，上下文长度过长或文档数量巨大。基于视觉语言模型的晚期交互机制已在基于检索的视觉增强问答任务中展现出最先进的性能。\n\n然而，将其用于基于RAG的多模态问答仍存在一些挑战。首先，许多流行且广泛采用的向量数据库不支持原生多向量检索。其次，晚期交互需要大量计算，这会增加空间占用并可能阻碍企业采用。最后，晚期交互机制的现状尚未利用近似邻域搜索索引方法，以实现检索过程的大幅加速。\n\n本文探讨了一种务实的方法，旨在使视觉检索过程具有可伸缩性和高效性，同时不牺牲性能质量。我们提出了一种多步骤定制实现方案，该方案利用广泛采用的混合搜索（元数据和嵌入）以及最先进的晚期交互重排器来检索最佳匹配页面。最后，MLLM被用作阅读器，从语境化的最佳匹配页面中生成答案。通过实验，我们观察到所提出的设计具有可伸缩性（显著加速）且稳定（不降低性能质量），因此可作为企业生产系统使用。"
    },
    {
        "title": "Looking for Fairness in Recommender Systems",
        "url": "http://arxiv.org/abs/2507.12242v1",
        "pub_date": "2025-07-16",
        "summary": "Recommender systems can be found everywhere today, shaping our everyday experience whenever we're consuming content, ordering food, buying groceries online, or even just reading the news. Let's imagine we're in the process of building a recommender system to make content suggestions to users on social media. When thinking about fairness, it becomes clear there are several perspectives to consider: the users asking for tailored suggestions, the content creators hoping for some limelight, and society at large, navigating the repercussions of algorithmic recommendations. A shared fairness concern across all three is the emergence of filter bubbles, a side-effect that takes place when recommender systems are almost \"too good\", making recommendations so tailored that users become inadvertently confined to a narrow set of opinions/themes and isolated from alternative ideas. From the user's perspective, this is akin to manipulation. From the small content creator's perspective, this is an obstacle preventing them access to a whole range of potential fans. From society's perspective, the potential consequences are far-reaching, influencing collective opinions, social behavior and political decisions. How can our recommender system be fine-tuned to avoid the creation of filter bubbles, and ensure a more inclusive and diverse content landscape? Approaching this problem involves defining one (or more) performance metric to represent diversity, and tweaking our recommender system's performance through the lens of fairness. By incorporating this metric into our evaluation framework, we aim to strike a balance between personalized recommendations and the broader societal goal of fostering rich and varied cultures and points of view.",
        "translated": "今日推荐系统无处不在，无论我们是在消费内容、订餐、在线购物，还是仅仅阅读新闻，它们都在塑造着我们的日常体验。设想我们正在构建一个推荐系统，为社交媒体用户提供内容建议。在考量公平性时，显然需要从多个角度出发：寻求个性化建议的用户、渴望获得关注的内容创作者，以及身处算法推荐影响之下的整个社会。\n\n这三方共同关注的一个公平性问题是“信息茧房”的出现。当推荐系统“过于精准”，推荐内容高度定制化时，用户会不知不觉地局限于狭隘的观点和主题，与替代性思想隔绝，这便是信息茧房的副作用。从用户的角度来看，这类似于一种操控。对小型内容创作者而言，这阻碍了他们接触到广泛的潜在粉丝。而从社会层面看，其潜在影响是深远的，会影响集体观点、社会行为乃至政治决策。\n\n我们的推荐系统应如何进行精细调整，才能避免信息茧房的产生，并确保一个更具包容性和多样性的内容生态？解决此问题的方法在于定义一个（或多个）衡量多样性的性能指标，并从公平性的角度优化推荐系统的表现。通过将此指标纳入我们的评估框架，我们旨在个性化推荐与促进丰富多元文化及观点这一更广泛的社会目标之间取得平衡。"
    },
    {
        "title": "Sparse Autoencoders for Sequential Recommendation Models: Interpretation\n  and Flexible Control",
        "url": "http://arxiv.org/abs/2507.12202v1",
        "pub_date": "2025-07-16",
        "summary": "Many current state-of-the-art models for sequential recommendations are based on transformer architectures. Interpretation and explanation of such black box models is an important research question, as a better understanding of their internals can help understand, influence, and control their behavior, which is very important in a variety of real-world applications. Recently sparse autoencoders (SAE) have been shown to be a promising unsupervised approach for extracting interpretable features from language models. These autoencoders learn to reconstruct hidden states of the transformer's internal layers from sparse linear combinations of directions in their activation space.   This paper is focused on the application of SAE to the sequential recommendation domain. We show that this approach can be successfully applied to the transformer trained on a sequential recommendation task: learned directions turn out to be more interpretable and monosemantic than the original hidden state dimensions. Moreover, we demonstrate that the features learned by SAE can be used to effectively and flexibly control the model's behavior, providing end-users with a straightforward method to adjust their recommendations to different custom scenarios and contexts.",
        "translated": "许多当前最先进的序列推荐模型都基于Transformer架构。对这类黑盒模型的解释和可解释性是一个重要的研究问题，因为更好地理解其内部机制有助于理解、影响和控制它们的行为，这在各种实际应用中都非常重要。近期，稀疏自编码器（SAE）已被证明是一种很有前景的无监督方法，可用于从语言模型中提取可解释特征。这些自编码器学习通过其激活空间中方向的稀疏线性组合来重建Transformer内部层的隐状态。\n\n本文主要关注将SAE应用于序列推荐领域。我们表明，这种方法可以成功应用于在序列推荐任务上训练的Transformer：结果发现，所学习到的方向比原始隐状态维度更具可解释性和单义性。此外，我们证明SAE学习到的特征可以有效且灵活地控制模型的行为，为最终用户提供了一种直接的方法，以根据不同的自定义场景和上下文调整其推荐。"
    },
    {
        "title": "SIEVE: Effective Filtered Vector Search with Collection of Indexes",
        "url": "http://arxiv.org/abs/2507.11907v1",
        "pub_date": "2025-07-16",
        "summary": "Many real-world tasks such as recommending videos with the kids tag can be reduced to finding most similar vectors associated with hard predicates. This task, filtered vector search, is challenging as prior state-of-the-art graph-based (unfiltered) similarity search techniques quickly degenerate when hard constraints are considered. That is, effective graph-based filtered similarity search relies on sufficient connectivity for reaching the most similar items within just a few hops. To consider predicates, recent works propose modifying graph traversal to visit only the items that may satisfy predicates. However, they fail to offer the just-a-few-hops property for a wide range of predicates: they must restrict predicates significantly or lose efficiency if only a small fraction of items satisfy predicates.   We propose an opposite approach: instead of constraining traversal, we build many indexes each serving different predicate forms. For effective construction, we devise a three-dimensional analytical model capturing relationships among index size, search time, and recall, with which we follow a workload-aware approach to pack as many useful indexes as possible into a collection. At query time, the analytical model is employed yet again to discern the one that offers the fastest search at a given recall. We show superior performance and support on datasets with varying selectivities and forms: our approach achieves up to 8.06x speedup while having as low as 1% build time versus other indexes, with less than 2.15x memory of a standard HNSW graph and modest knowledge of past workloads.",
        "translated": "许多真实世界的任务，例如推荐带有“儿童”标签的视频，都可以简化为查找满足严格谓词的最相似向量。这项任务，即过滤向量搜索，极具挑战性，因为当考虑严格约束时，先前最先进的基于图的（未过滤）相似性搜索技术会迅速失效/性能急剧下降。也就是说，有效的基于图的过滤相似性搜索依赖于足够的连接性，以确保在少数几跳内即可抵达最相似的项目。为考虑谓词，近期研究提出修改图遍历算法，使其仅访问可能满足谓词的项目。然而，对于广泛的谓词，它们无法提供“少数几跳”的特性：当只有一小部分项目满足谓词时，它们必须大幅限制谓词范围，否则将损失效率。\n\n我们提出了一种对立方法：我们不限制图遍历，而是构建多个索引，每个索引服务于不同的谓词形式。为了实现有效的索引构建，我们设计了一个三维分析模型，用于捕捉索引大小、搜索时间和召回率之间的关系。基于此模型，我们采用一种工作负载感知方法，将尽可能多的有用索引集成到索引集合中。在查询时，该分析模型再次被应用，以甄选出在给定召回率下能提供最快搜索的索引。我们的方法在具有不同选择性和形式的数据集上展示了卓越的性能和广泛的支持：相较于其他索引，我们的方法实现了高达8.06倍的加速，构建时间却低至1%，内存使用量不到标准HNSW图的2.15倍，且仅需少量关于历史工作负载的先验知识。"
    },
    {
        "title": "Context-Aware Search and Retrieval Over Erasure Channels",
        "url": "http://arxiv.org/abs/2507.11894v1",
        "pub_date": "2025-07-16",
        "summary": "This paper introduces and analyzes a search and retrieval model that adopts key semantic communication principles from retrieval-augmented generation. We specifically present an information-theoretic analysis of a remote document retrieval system operating over a symbol erasure channel. The proposed model encodes the feature vector of a query, derived from term-frequency weights of a language corpus by using a repetition code with an adaptive rate dependent on the contextual importance of the terms. At the decoder, we select between two documents based on the contextual closeness of the recovered query. By leveraging a jointly Gaussian approximation for both the true and reconstructed similarity scores, we derive an explicit expression for the retrieval error probability, i.e., the probability under which the less similar document is selected. Numerical simulations on synthetic and real-world data (Google NQ) confirm the validity of the analysis. They further demonstrate that assigning greater redundancy to critical features effectively reduces the error rate, highlighting the effectiveness of semantic-aware feature encoding in error-prone communication settings.",
        "translated": "本文介绍并分析了一种搜索与检索模型，该模型采用了检索增强生成（RAG）中的关键语义通信原则。我们具体提出了一个在符号擦除信道上运行的远程文档检索系统的信息论分析。所提出的模型对查询的特征向量进行编码，该特征向量源自语言语料库的词频权重，通过使用具有自适应速率的重复码实现，该速率取决于词语的上下文重要性。在解码器端，我们根据恢复查询的上下文接近度在两篇文档之间进行选择。通过利用真实和重建相似度分数的联合高斯近似，我们推导出了检索错误概率的显式表达式，即选择相似度较低文档的概率。在合成数据和真实世界数据（Google NQ）上的数值模拟证实了分析的有效性。它们进一步表明，为关键特征分配更大的冗余能有效降低错误率，从而强调了语义感知特征编码在易出错的通信环境中的有效性。"
    },
    {
        "title": "Similarity-Guided Diffusion for Contrastive Sequential Recommendation",
        "url": "http://arxiv.org/abs/2507.11866v1",
        "pub_date": "2025-07-16",
        "summary": "In sequential recommendation systems, data augmentation and contrastive learning techniques have recently been introduced using diffusion models to achieve robust representation learning. However, most of the existing approaches use random augmentation, which risk damaging the contextual information of the original sequence. Accordingly, we propose a Similarity-Guided Diffusion for Contrastive Sequential Recommendation. Our method leverages the similarity between item embedding vectors to generate semantically consistent noise. Moreover, we utilize high confidence score in the denoising process to select our augmentation positions. This approach more effectively reflects contextual and structural information compared to augmentation at random positions. From a contrastive learning perspective, the proposed augmentation technique provides more discriminative positive and negative samples, simultaneously improving training efficiency and recommendation performance. Experimental results on five benchmark datasets show that SimDiffRec outperforms the existing baseline models.",
        "translated": "序列推荐系统中，近期利用扩散模型引入了数据增强和对比学习技术，以实现鲁棒的表示学习。然而，大多数现有方法采用随机增强，这有损害原始序列上下文信息的风险。为此，我们提出了一种用于对比序列推荐的相似性引导扩散方法 (SimDiffRec)。本文方法利用物品嵌入向量之间的相似性，以生成语义一致的噪声。此外，我们在去噪过程中利用高置信度分数来选择增强位置。相比于在随机位置进行增强，这种方法能更有效地反映上下文和结构信息。从对比学习的角度来看，所提出的增强技术提供了更具判别性的正负样本，同时提升了训练效率和推荐性能。在五个基准数据集上的实验结果表明，SimDiffRec 的性能优于现有基线模型。"
    },
    {
        "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data",
        "url": "http://arxiv.org/abs/2507.12425v1",
        "pub_date": "2025-07-16",
        "summary": "Organizations increasingly rely on proprietary enterprise data, including HR records, structured reports, and tabular documents, for critical decision-making. While Large Language Models (LLMs) have strong generative capabilities, they are limited by static pretraining, short context windows, and challenges in processing heterogeneous data formats. Conventional Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but often struggle with structured and semi-structured data.   This work proposes an advanced RAG framework that combines hybrid retrieval strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by metadata-aware filtering with SpaCy NER and cross-encoder reranking. The framework applies semantic chunking to maintain textual coherence and retains tabular data structures to preserve row-column integrity. Quantized indexing optimizes retrieval efficiency, while human-in-the-loop feedback and conversation memory improve adaptability.   Experiments on enterprise datasets show notable improvements: Precision@5 increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74), and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness (4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale. These results demonstrate the framework's effectiveness in delivering accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work includes extending to multimodal data and integrating agent-based retrieval. The source code will be released at https://github.com/CheerlaChandana/Enterprise-Chatbot",
        "translated": "组织日益依赖专有企业数据，包括人力资源记录、结构化报告和表格文档，用于关键决策。尽管大型语言模型（LLMs）具有强大的生成能力，但它们受限于静态预训练、短上下文窗口以及处理异构数据格式的挑战。传统的检索增强生成（RAG）框架虽然弥补了部分不足，但通常难以处理结构化和半结构化数据。\n\n本文提出了一个先进的RAG框架，该框架结合了使用密集嵌入（all-mpnet-base-v2）和BM25的混合检索策略，并通过SpaCy NER的元数据感知过滤和交叉编码器重排序进行了增强。该框架应用语义分块以保持文本连贯性，并保留表格数据结构以确保行-列完整性。量化索引优化了检索效率，同时人机协作反馈和对话记忆提高了框架的适应性。\n\n在企业数据集上的实验显示出显著改进：Precision@5 提高了15%（90% 对比 75%），Recall@5 提高了13%（87% 对比 74%），平均倒数排名（MRR）提高了16%（0.85 对比 0.69）。定性评估显示，在5点李克特量表上，忠实性（4.6 对比 3.0）、完整性（4.2 对比 2.5）和相关性（4.5 对比 3.2）均获得更高分数。这些结果证明了该框架在为企业任务提供准确、全面和上下文相关响应方面的有效性。未来工作包括扩展到多模态数据和集成基于代理的检索。源代码将在 https://github.com/CheerlaChandana/Enterprise-Chatbot 发布。"
    },
    {
        "title": "Mixture of Raytraced Experts",
        "url": "http://arxiv.org/abs/2507.12419v1",
        "pub_date": "2025-07-16",
        "summary": "We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts (MoE) architecture which can dynamically select sequences of experts, producing computational graphs of variable width and depth. Existing MoE architectures generally require a fixed amount of computation for a given sample. Our approach, in contrast, yields predictions with increasing accuracy as the computation cycles through the experts' sequence. We train our model by iteratively sampling from a set of candidate experts, unfolding the sequence akin to how Recurrent Neural Networks are trained. Our method does not require load-balancing mechanisms, and preliminary experiments show a reduction in training epochs of 10\\% to 40\\% with a comparable/higher accuracy. These results point to new research directions in the field of MoEs, allowing the design of potentially faster and more expressive models. The code is available at https://github.com/nutig/RayTracing",
        "translated": "我们引入了“射线追踪专家混合模型”（Mixture of Raytraced Experts，简称 MoRE），这是一种堆叠式专家混合 (MoE) 架构，能够动态选择专家序列，从而生成可变宽度和深度的计算图。现有的 MoE 架构通常对给定样本需要固定的计算量。相比之下，我们的方法能够随着计算在专家序列中的循环，逐步提高预测精度。\n\n我们通过从一组候选专家中迭代采样来训练模型，其序列展开方式类似于循环神经网络 (RNN) 的训练。我们的方法无需负载均衡机制，并且初步实验表明，在获得可比或更高精度的同时，训练周期减少了10%至40%。这些结果为 MoE 领域指明了新的研究方向，有助于设计出潜在更快、表达能力更强的模型。代码已在 https://github.com/nutig/RayTracing 提供。"
    },
    {
        "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in\n  Composed Image Retrieval",
        "url": "http://arxiv.org/abs/2507.12416v1",
        "pub_date": "2025-07-16",
        "summary": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference image and accompanying text describing desired modifications. However, existing CIR methods only focus on retrieving the target image and disregard the relevance of other images. This limitation arises because most methods employing contrastive learning-which treats the target image as positive and all other images in the batch as negatives-can inadvertently include false negatives. This may result in retrieving irrelevant images, reducing user satisfaction even when the target image is retrieved. To address this issue, we propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which optimizes a reward model objective to reduce false negatives. Additionally, we introduce a hard negative sampling strategy that selects images positioned between two steep drops in relevance scores following the target image, to effectively filter false negatives. In order to evaluate CIR models on their alignment with human satisfaction, we create Human-Preference FashionIQ (HP-FashionIQ), a new dataset that explicitly captures user preferences beyond target retrieval. Extensive experiments demonstrate that QuRe achieves state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting the strongest alignment with human preferences on the HP-FashionIQ dataset. The source code is available at https://github.com/jackwaky/QuRe.",
        "translated": "组合图像检索（CIR）根据参考图像和附带的描述所需修改的文本来检索相关图像。然而，现有CIR方法仅关注检索目标图像，却忽视了其他图像的相关性。这种局限性源于大多数采用对比学习的方法会无意中包含假负样本，这些方法将目标图像视为正样本，并将批次中所有其他图像视为负样本。这可能导致检索到不相关的图像，从而降低用户满意度，即便目标图像已被检索到。\n\n为了解决这个问题，我们提出了通过难负样本采样实现查询相关检索（QuRe）方法，该方法优化了一个奖励模型目标以减少假负样本。此外，我们引入了一种难负样本采样策略，该策略选择位于目标图像之后相关性分数两次急剧下降之间的图像，以有效过滤假负样本。为了评估CIR模型与人类满意度的对齐程度，我们创建了人类偏好FashionIQ（HP-FashionIQ）数据集，这是一个明确捕捉超越目标检索的用户偏好的新数据集。大量实验表明，QuRe在FashionIQ和CIRR数据集上达到了最先进的性能，同时在HP-FashionIQ数据集上展现出与人类偏好的最强对齐。源代码可在 https://github.com/jackwaky/QuRe 获取。"
    },
    {
        "title": "Language Models Improve When Pretraining Data Matches Target Tasks",
        "url": "http://arxiv.org/abs/2507.12466v1",
        "pub_date": "2025-07-16",
        "summary": "Every data selection method inherently has a target. In practice, these targets often emerge implicitly through benchmark-driven iteration: researchers develop selection strategies, train models, measure benchmark performance, then refine accordingly. This raises a natural question: what happens when we make this optimization explicit? To explore this, we propose benchmark-targeted ranking (BETR), a simple method that selects pretraining documents based on similarity to benchmark training examples. BETR embeds benchmark examples and a sample of pretraining documents in a shared space, scores this sample by similarity to benchmarks, then trains a lightweight classifier to predict these scores for the full corpus. We compare data selection methods by training over 500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to them. From this, we find that simply aligning pretraining data to evaluation benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline (4.7x over unfiltered data) and improves performance on 9 out of 10 tasks across all scales. BETR also generalizes well: when targeting a diverse set of benchmarks disjoint from our evaluation suite, it still matches or outperforms baselines. Our scaling analysis further reveals a clear trend: larger models require less aggressive filtering. Overall, our findings show that directly matching pretraining data to target tasks precisely shapes model capabilities and highlight that optimal selection strategies must adapt to model scale.",
        "translated": "任何数据选择方法都内在地包含一个目标。在实践中，这些目标通常通过基准驱动的迭代隐式地显现：研究人员开发选择策略、训练模型、衡量基准性能，然后据此进行调整优化。这自然引出了一个问题：当我们使这种优化变得显式时，结果会怎样？\n\n为此，我们提出了一种名为“基准目标排序”（Benchmark-Targeted Ranking, BETR）的简单方法，它根据预训练文档与基准训练示例的相似性来选择文档。BETR 将基准示例和部分预训练文档嵌入到共享空间中，根据其与基准的相似性对该样本进行评分，然后训练一个轻量级分类器来预测整个语料库的这些分数。\n\n我们通过训练超过 500 个计算量从 $10^{19}$ 到 $10^{22}$ FLOPs 的模型，并拟合它们的缩放定律，来比较不同的数据选择方法。结果发现，通过 BETR 简单地将预训练数据与评估基准对齐，相较于 DCLM-Baseline 实现了 2.1 倍的计算效率提升（比未过滤数据提升 4.7 倍），并在所有模型规模下，10 个任务中有 9 个的性能得到提升。BETR 也表现出良好的泛化能力：即使针对与我们评估套件不相交的多元化基准集，它仍能与基线方法持平或超越。我们的缩放分析进一步揭示了一个清晰的趋势：更大的模型需要更宽松的数据过滤策略。\n\n总之，我们的研究结果表明，将预训练数据与目标任务直接匹配能够精确地塑造模型能力，并强调了最佳数据选择策略必须适应模型规模。"
    },
    {
        "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data",
        "url": "http://arxiv.org/abs/2507.12425v1",
        "pub_date": "2025-07-16",
        "summary": "Organizations increasingly rely on proprietary enterprise data, including HR records, structured reports, and tabular documents, for critical decision-making. While Large Language Models (LLMs) have strong generative capabilities, they are limited by static pretraining, short context windows, and challenges in processing heterogeneous data formats. Conventional Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but often struggle with structured and semi-structured data.   This work proposes an advanced RAG framework that combines hybrid retrieval strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by metadata-aware filtering with SpaCy NER and cross-encoder reranking. The framework applies semantic chunking to maintain textual coherence and retains tabular data structures to preserve row-column integrity. Quantized indexing optimizes retrieval efficiency, while human-in-the-loop feedback and conversation memory improve adaptability.   Experiments on enterprise datasets show notable improvements: Precision@5 increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74), and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness (4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale. These results demonstrate the framework's effectiveness in delivering accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work includes extending to multimodal data and integrating agent-based retrieval. The source code will be released at https://github.com/CheerlaChandana/Enterprise-Chatbot",
        "translated": "组织日益依赖包括人力资源记录、结构化报告和表格文档在内的专有企业数据，以支持关键决策。尽管大型语言模型（LLM）具有强大的生成能力，但它们受限于静态预训练、短上下文窗口以及处理异构数据格式的挑战。传统的检索增强生成（RAG）框架虽然解决了其中一些不足，但在处理结构化和半结构化数据时往往表现不佳。\n\n本工作提出了一个先进的RAG框架，该框架结合了使用密集嵌入（all-mpnet-base-v2）和BM25的混合检索策略，并通过SpaCy NER实现的元数据感知过滤和交叉编码器重排序进行了增强。该框架应用语义分块以保持文本连贯性，并保留表格数据结构以确保行-列完整性。量化索引优化了检索效率，同时人工参与反馈和对话记忆提高了框架的适应性。\n\n在企业数据集上的实验显示出显著的改进：Precision@5 提高了15%（90对比75），Recall@5 提高了13%（87对比74），平均倒数排名（Mean Reciprocal Rank）提高了16%（0.85对比0.69）。定性评估结果显示，在5点李克特量表上，框架在忠实性（4.6对比3.0）、完整性（4.2对比2.5）和相关性（4.5对比3.2）方面获得了更高的分数。这些结果证明了该框架在为企业任务提供准确、全面且上下文相关响应方面的有效性。未来工作包括将框架扩展到多模态数据，并集成基于代理的检索。源代码将发布在 https://github.com/CheerlaChandana/Enterprise-Chatbot。"
    },
    {
        "title": "Developing Visual Augmented Q&amp;A System using Scalable Vision Embedding\n  Retrieval &amp; Late Interaction Re-ranker",
        "url": "http://arxiv.org/abs/2507.12378v1",
        "pub_date": "2025-07-16",
        "summary": "Traditional information extraction systems face challenges with text only language models as it does not consider infographics (visual elements of information) such as tables, charts, images etc. often used to convey complex information to readers. Multimodal LLM (MLLM) face challenges of finding needle in the haystack problem i.e., either longer context length or substantial number of documents as search space. Late interaction mechanism over visual language models has shown state of the art performance in retrieval-based vision augmented Q&amp;A tasks. There are yet few challenges using it for RAG based multi-modal Q&amp;A. Firstly, many popular and widely adopted vector databases do not support native multi-vector retrieval. Secondly, late interaction requires computation which inflates space footprint and can hinder enterprise adoption. Lastly, the current state of late interaction mechanism does not leverage the approximate neighbor search indexing methods for large speed ups in retrieval process. This paper explores a pragmatic approach to make vision retrieval process scalable and efficient without compromising on performance quality. We propose multi-step custom implementation utilizing widely adopted hybrid search (metadata &amp; embedding) and state of the art late interaction re-ranker to retrieve best matching pages. Finally, MLLM are prompted as reader to generate answers from contextualized best matching pages. Through experiments, we observe that the proposed design is scalable (significant speed up) and stable (without degrading performance quality), hence can be used as production systems at enterprises.",
        "translated": "传统信息抽取系统在处理纯文本语言模型时面临挑战，因为它们无法考虑信息图表（信息的视觉元素），如表格、图表、图像等，这些元素常用于向读者传达复杂信息。多模态大型语言模型（MLLM）则面临“大海捞针”问题，即上下文长度过长或搜索空间中文档数量庞大。基于视觉语言模型的后期交互机制，在基于检索的视觉增强问答任务中展现了最先进的性能。然而，将其应用于基于RAG的多模态问答时，仍存在一些挑战。首先，许多流行且广泛采用的向量数据库不支持原生的多向量检索。其次，后期交互机制需要大量计算，这会增加其空间占用，并可能阻碍企业采纳。最后，当前的后期交互机制尚未充分利用近似邻域搜索（ANN）索引方法来大幅提升检索速度。\n\n本文旨在探索一种务实的方法，以在不牺牲性能质量的前提下，实现视觉检索过程的可扩展性和高效性。我们提出一种多步骤定制实现方案，该方案利用广泛采用的混合搜索（元数据和嵌入）以及最先进的后期交互重排序器来检索最佳匹配页面。最后，多模态大型语言模型（MLLM）被用作“阅读器”，从上下文相关的最佳匹配页面中生成答案。通过实验，我们发现所提出的设计具有良好的可扩展性（显著提速）和稳定性（不降低性能质量），因此可作为企业的生产系统使用。"
    },
    {
        "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User\n  Demographics",
        "url": "http://arxiv.org/abs/2507.12372v1",
        "pub_date": "2025-07-16",
        "summary": "Large language models (LLMs) have traditionally relied on static training data, limiting their knowledge to fixed snapshots. Recent advancements, however, have equipped LLMs with web browsing capabilities, enabling real time information retrieval and multi step reasoning over live web content. While prior studies have demonstrated LLMs ability to access and analyze websites, their capacity to directly retrieve and analyze social media data remains unexplored. Here, we evaluate whether web browsing LLMs can infer demographic attributes of social media users given only their usernames. Using a synthetic dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international participants, we show that these models can access social media content and predict user demographics with reasonable accuracy. Analysis of the synthetic dataset further reveals how LLMs parse and interpret social media profiles, which may introduce gender and political biases against accounts with minimal activity. While this capability holds promise for computational social science in the post API era, it also raises risks of misuse particularly in information operations and targeted advertising underscoring the need for safeguards. We recommend that LLM providers restrict this capability in public facing applications, while preserving controlled access for verified research purposes.",
        "translated": "大型语言模型（LLMs）传统上依赖静态训练数据，将其知识限制在固定的时间点快照。然而，最近的进展使LLMs具备了网络浏览能力，从而能够实现实时信息检索和基于实时网络内容的多步推理。尽管此前的研究已表明LLMs能够访问和分析网站，但它们直接检索和分析社交媒体数据的能力仍未得到探索。\n\n在本文中，我们评估了具备网络浏览能力的LLMs能否仅根据社交媒体用户的用户名推断其人口统计属性。利用包含48个X（Twitter）账户的合成数据集和一个包含1,384名国际参与者的调查数据集，我们发现这些模型能够访问社交媒体内容，并以合理的准确度预测用户的人口统计信息。对合成数据集的分析进一步揭示了LLMs如何解析和解释社交媒体资料，这可能会对活动量极小的账户引入性别和政治偏见。\n\n尽管这项能力为后API时代的计算社会科学带来了希望，但它也带来了滥用的风险，特别是在信息操作和定向广告方面，这凸显了采取保障措施的必要性。我们建议LLM提供商在面向公众的应用中限制这项能力，同时为经过验证的研究用途保留受控访问权限。"
    },
    {
        "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests\n  through Debate",
        "url": "http://arxiv.org/abs/2507.12370v1",
        "pub_date": "2025-07-16",
        "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in understanding and generating human language, contributing to more natural interactions with complex systems. However, they face challenges such as ambiguity in user requests processed by LLMs. To address these challenges, this paper introduces and evaluates a multi-agent debate framework designed to enhance detection and resolution capabilities beyond single models. The framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and Mistral-7B variants) and a dataset with diverse ambiguities. The debate framework markedly enhanced the performance of Llama3-8B and Mistral-7B variants over their individual baselines, with Mistral-7B-led debates achieving a notable 76.7% success rate and proving particularly effective for complex ambiguities and efficient consensus. While acknowledging varying model responses to collaborative strategies, these findings underscore the debate framework's value as a targeted method for augmenting LLM capabilities. This work offers important insights for developing more robust and adaptive language understanding systems by showing how structured debates can lead to improved clarity in interactive systems.",
        "translated": "大语言模型（LLM）在理解和生成人类语言方面展现出显著能力，有助于与复杂系统进行更自然的交互。然而，它们面临挑战，例如由LLM处理的用户请求中存在的歧义。为解决这些挑战，本文介绍并评估了一个多智能体辩论框架，该框架旨在提升超越单一模型的检测和消歧能力。该框架由三种LLM架构（Llama3-8B、Gemma2-9B 和 Mistral-7B 变体）以及一个包含多样化歧义的数据集组成。辩论框架显著提升了 Llama3-8B 和 Mistral-7B 变体相较于其各自基线的性能，其中以 Mistral-7B 为主的辩论达到了显著的 76.7% 的成功率，并证明在处理复杂歧义和实现高效共识方面特别有效。尽管不同模型对协作策略的响应存在差异，但这些发现凸显了辩论框架作为一种增强LLM能力的有针对性方法的价值。本研究通过展示结构化辩论如何促进交互系统中的明确性，为开发更鲁棒和自适应的语言理解系统提供了重要见解。"
    },
    {
        "title": "Nonlinear Concept Erasure: a Density Matching Approach",
        "url": "http://arxiv.org/abs/2507.12341v1",
        "pub_date": "2025-07-16",
        "summary": "Ensuring that neural models used in real-world applications cannot infer sensitive information, such as demographic attributes like gender or race, from text representations is a critical challenge when fairness is a concern. We address this issue through concept erasure, a process that removes information related to a specific concept from distributed representations while preserving as much of the remaining semantic information as possible. Our approach involves learning an orthogonal projection in the embedding space, designed to make the class-conditional feature distributions of the discrete concept to erase indistinguishable after projection. By adjusting the rank of the projector, we control the extent of information removal, while its orthogonality ensures strict preservation of the local structure of the embeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves state-of-the-art performance in nonlinear erasure of a discrete attribute on classic natural language processing benchmarks. Furthermore, we demonstrate that $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear classifiers, thereby promoting fairness.",
        "translated": "确保用于实际应用的神经网络模型无法从文本表示中推断出敏感信息（例如性别或种族等人口统计学属性），是关乎公平性的一个关键挑战。我们通过概念擦除来解决这个问题，这是一种从分布式表示中移除与特定概念相关的信息，同时尽可能保留其余语义信息的过程。我们的方法涉及在嵌入空间中学习一个正交投影，旨在使要擦除的离散概念的类条件特征分布在投影后变得不可区分。通过调整投影器的秩，我们可以控制信息移除的程度，而其正交性则确保严格保留嵌入的局部结构。我们的方法被命名为 $\\overline{\\mathrm{L}}$EOPARD，在经典自然语言处理基准测试中，针对离散属性的非线性擦除方面，实现了最先进的性能。此外，我们证明 $\\overline{\\mathrm{L}}$EOPARD 能够有效减轻深度非线性分类器中的偏见，从而促进公平性。"
    },
    {
        "title": "SGCL: Unifying Self-Supervised and Supervised Learning for Graph\n  Recommendation",
        "url": "http://arxiv.org/abs/2507.13336v1",
        "pub_date": "2025-07-17",
        "summary": "Recommender systems (RecSys) are essential for online platforms, providing personalized suggestions to users within a vast sea of information. Self-supervised graph learning seeks to harness high-order collaborative filtering signals through unsupervised augmentation on the user-item bipartite graph, primarily leveraging a multi-task learning framework that includes both supervised recommendation loss and self-supervised contrastive loss. However, this separate design introduces additional graph convolution processes and creates inconsistencies in gradient directions due to disparate losses, resulting in prolonged training times and sub-optimal performance. In this study, we introduce a unified framework of Supervised Graph Contrastive Learning for recommendation (SGCL) to address these issues. SGCL uniquely combines the training of recommendation and unsupervised contrastive losses into a cohesive supervised contrastive learning loss, aligning both tasks within a single optimization direction for exceptionally fast training. Extensive experiments on three real-world datasets show that SGCL outperforms state-of-the-art methods, achieving superior accuracy and efficiency.",
        "translated": "推荐系统（RecSys）对于在线平台至关重要，它们能在海量信息中为用户提供个性化推荐。自监督图学习旨在通过对用户-项目二分图进行无监督增强来利用高阶协同过滤信号，其主要通过一个包含监督推荐损失和自监督对比损失的多任务学习框架来实现。然而，这种分离设计引入了额外的图卷积过程，并由于不同的损失函数而导致梯度方向不一致，从而造成训练时间延长和次优性能。在本研究中，我们提出了一种用于推荐的统一的监督图对比学习框架（SGCL）来解决这些问题。SGCL 独特地将推荐损失和无监督对比损失的训练整合为一个内聚的监督对比学习损失，使两项任务在单一优化方向上对齐，从而实现了极快的训练速度。在三个真实世界数据集上进行的大量实验表明，SGCL 优于最先进的方法，实现了卓越的准确性和效率。"
    },
    {
        "title": "Efficiently Constructing Sparse Navigable Graphs",
        "url": "http://arxiv.org/abs/2507.13296v1",
        "pub_date": "2025-07-17",
        "summary": "Graph-based nearest neighbor search methods have seen a surge of popularity in recent years, offering state-of-the-art performance across a wide variety of applications. Central to these methods is the task of constructing a sparse navigable search graph for a given dataset endowed with a distance function. Unfortunately, doing so is computationally expensive, so heuristics are universally used in practice.   In this work, we initiate the study of fast algorithms with provable guarantees for search graph construction. For a dataset with $n$ data points, the problem of constructing an optimally sparse navigable graph can be framed as $n$ separate but highly correlated minimum set cover instances. This yields a naive $O(n^3)$ time greedy algorithm that returns a navigable graph whose sparsity is at most $O(\\log n)$ higher than optimal. We improve significantly on this baseline, taking advantage of correlation between the set cover instances to leverage techniques from streaming and sublinear-time set cover algorithms. Combined with problem-specific pre-processing techniques, we present an $\\tilde{O}(n^2)$ time algorithm for constructing an $O(\\log n)$-approximate sparsest navigable graph under any distance function.   The runtime of our method is optimal up to logarithmic factors under the Strong Exponential Time Hypothesis via a reduction from Monochromatic Closest Pair. Moreover, we prove that, as with general set cover, obtaining better than an $O(\\log n)$-approximation is NP-hard, despite the significant additional structure present in the navigable graph problem. Finally, we show that our techniques can also beat cubic time for the closely related and practically important problems of constructing $\\alpha$-shortcut reachable and $\\tau$-monotonic graphs, which are also used for nearest neighbor search. For such graphs, we obtain $\\tilde{O}(n^{2.5})$ time or better algorithms.",
        "translated": "基于图的最近邻搜索方法近年来迅速兴起，在各种应用中展现出最先进的性能。这些方法的核心任务是为给定数据集（通常赋予一个距离函数）构建一个稀疏可导航的搜索图。不幸的是，这一过程计算开销巨大，因此实践中普遍采用启发式方法。\n\n在这项工作中，我们着手研究具有可证明保证的快速搜索图构建算法。对于一个包含 $n$ 个数据点的数据集，构建最优稀疏可导航图的问题可以被框架为 $n$ 个相互独立但高度相关的最小集合覆盖实例。这会得到一个朴素的 $O(n^3)$ 时间贪心算法，其返回的可导航图的稀疏性比最优稀疏性至多高出 $O(\\log n)$ 倍。我们显著改进了这一基线，利用集合覆盖实例间的相关性，并借鉴了流式和亚线性时间集合覆盖算法中的技术。结合问题特定的预处理技术，我们提出了一个 $\\tilde{O}(n^2)$ 时间算法，用于在任何距离函数下构建 $O(\\log n)$-近似的最稀疏可导航图。\n\n根据强指数时间假设（Strong Exponential Time Hypothesis，简称 SETH），通过从单色最近点对问题的归约，我们方法的运行时间在对数因子意义下是最优的。此外，我们证明，如同一般的集合覆盖问题一样，即使可导航图问题中存在显著的额外结构，获得优于 $O(\\log n)$-近似的结果仍是 NP-难的。最后，我们展示了我们的技术也可以在密切相关且具有实际重要性的问题上击败立方时间复杂度，例如构建 $\\alpha$-捷径可达图和 $\\tau$-单调图，这些图也常用于最近邻搜索。对于这类图，我们获得了 $\\tilde{O}(n^{2.5})$ 时间或更优的算法。"
    },
    {
        "title": "Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for\n  Human Capital Management",
        "url": "http://arxiv.org/abs/2507.13275v1",
        "pub_date": "2025-07-17",
        "summary": "Advances in natural language processing and large language models are driving a major transformation in Human Capital Management, with a growing interest in building smart systems based on language technologies for talent acquisition, upskilling strategies, and workforce planning. However, the adoption and progress of these technologies critically depend on the development of reliable and fair models, properly evaluated on public data and open benchmarks, which have so far been unavailable in this domain.   To address this gap, we present TalentCLEF 2025, the first evaluation campaign focused on skill and job title intelligence. The lab consists of two tasks: Task A - Multilingual Job Title Matching, covering English, Spanish, German, and Chinese; and Task B - Job Title-Based Skill Prediction, in English. Both corpora were built from real job applications, carefully anonymized, and manually annotated to reflect the complexity and diversity of real-world labor market data, including linguistic variability and gender-marked expressions.   The evaluations included monolingual and cross-lingual scenarios and covered the evaluation of gender bias.   TalentCLEF attracted 76 registered teams with more than 280 submissions. Most systems relied on information retrieval techniques built with multilingual encoder-based models fine-tuned with contrastive learning, and several of them incorporated large language models for data augmentation or re-ranking. The results show that the training strategies have a larger effect than the size of the model alone. TalentCLEF provides the first public benchmark in this field and encourages the development of robust, fair, and transferable language technologies for the labor market.",
        "translated": "自然语言处理和大型语言模型的进步正在推动人力资本管理（HCM）领域的一场重大变革，人们对基于语言技术构建智能系统以实现人才招聘、技能提升策略和劳动力规划的兴趣日益增长。然而，这些技术的采用和进展，关键取决于开发出可靠且公平的模型，这些模型需在公共数据和开放基准上进行适当评估，而这些资源迄今在该领域尚属空白。\n\n为了弥补这一空白，我们推出了TalentCLEF 2025，这是首次专注于技能和职位智能的评估活动。该实验室包含两项任务：任务A——多语言职位名称匹配（涵盖英语、西班牙语、德语和中文）；以及任务B——基于职位名称的技能预测（使用英语）。两个语料库均基于真实的求职申请构建，经过仔细匿名化处理并进行人工标注，以反映真实劳动力市场数据的复杂性和多样性，包括语言变异性和带有性别偏见的表达。评估内容包括单语言和跨语言场景，并涵盖了性别偏见的评估。\n\nTalentCLEF吸引了76个注册团队，提交了280多份结果。大多数系统依赖于信息检索技术，结合通过对比学习微调的多语言编码器模型构建，其中一些系统还融入了大型语言模型用于数据增强或重排序。结果表明，训练策略的影响大于模型规模本身。TalentCLEF提供了该领域首个公共基准，并鼓励开发鲁棒、公平且可迁移的语言技术，以应用于劳动力市场。"
    },
    {
        "title": "SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated\n  Summaries For Scientific Abstracts",
        "url": "http://arxiv.org/abs/2507.13105v1",
        "pub_date": "2025-07-17",
        "summary": "We introduce SemCSE, an unsupervised method for learning semantic embeddings of scientific texts. Building on recent advances in contrastive learning for text embeddings, our approach leverages LLM-generated summaries of scientific abstracts to train a model that positions semantically related summaries closer together in the embedding space. This resulting objective ensures that the model captures the true semantic content of a text, in contrast to traditional citation-based approaches that do not necessarily reflect semantic similarity. To validate this, we propose a novel benchmark designed to assess a model's ability to understand and encode the semantic content of scientific texts, demonstrating that our method enforces a stronger semantic separation within the embedding space. Additionally, we evaluate SemCSE on the comprehensive SciRepEval benchmark for scientific text embeddings, where it achieves state-of-the-art performance among models of its size, thus highlighting the benefits of a semantically focused training approach.",
        "translated": "我们引入了 SemCSE，这是一种用于学习科学文本语义嵌入的无监督方法。该方法基于文本嵌入领域对比学习的最新进展，利用大型语言模型（LLM）生成的科学摘要来训练模型，使语义相关的摘要在嵌入空间中彼此更靠近。由此产生的目标确保模型能够捕获文本的真实语义内容，这与不一定反映语义相似性的传统基于引用的方法形成对比。\n\n为了验证这一点，我们提出了一个新颖的基准，旨在评估模型理解和编码科学文本语义内容的能力，并证明我们的方法在嵌入空间中实现了更强的语义分离。此外，我们还在针对科学文本嵌入的综合性 SciRepEval 基准上评估了 SemCSE，它在同等规模的模型中实现了最先进的性能，从而凸显了以语义为中心的训练方法的优势。"
    },
    {
        "title": "Generative Multi-Target Cross-Domain Recommendation",
        "url": "http://arxiv.org/abs/2507.12871v1",
        "pub_date": "2025-07-17",
        "summary": "Recently, there has been a surge of interest in Multi-Target Cross-Domain Recommendation (MTCDR), which aims to enhance recommendation performance across multiple domains simultaneously. Existing MTCDR methods primarily rely on domain-shared entities (\\eg users or items) to fuse and transfer cross-domain knowledge, which may be unavailable in non-overlapped recommendation scenarios. Some studies model user preferences and item features as domain-sharable semantic representations, which can be utilized to tackle the MTCDR task. Nevertheless, they often require extensive auxiliary data for pre-training. Developing more effective solutions for MTCDR remains an important area for further exploration.   Inspired by recent advancements in generative recommendation, this paper introduces GMC, a generative paradigm-based approach for multi-target cross-domain recommendation. The core idea of GMC is to leverage semantically quantized discrete item identifiers as a medium for integrating multi-domain knowledge within a unified generative model. GMC first employs an item tokenizer to generate domain-shared semantic identifiers for each item, and then formulates item recommendation as a next-token generation task by training a domain-unified sequence-to-sequence model. To further leverage the domain information to enhance performance, we incorporate a domain-aware contrastive loss into the semantic identifier learning, and perform domain-specific fine-tuning on the unified recommender. Extensive experiments on five public datasets demonstrate the effectiveness of GMC compared to a range of baseline methods.",
        "translated": "近期，多目标跨域推荐（Multi-Target Cross-Domain Recommendation, MTCDR）引起了广泛关注，该方法旨在同时提升多个领域的推荐性能。现有MTCDR方法主要依赖于域共享实体（例如用户或物品）来融合并迁移跨域知识，但这在非重合推荐场景中可能不可用。一些研究将用户偏好和物品特征建模为域可共享的语义表示，可用于解决MTCDR任务。然而，它们通常需要大量的辅助数据进行预训练。因此，为MTCDR开发更有效的解决方案仍然是值得进一步探索的重要领域。\n\n受生成式推荐最新进展的启发，本文提出GMC，一种基于生成范式的多目标跨域推荐方法。GMC的核心思想是利用语义量化的离散物品标识符作为媒介，在统一的生成模型中整合多域知识。GMC首先采用一个物品分词器（item tokenizer）为每个物品生成域共享的语义标识符，然后通过训练一个域统一的序列到序列模型，将物品推荐表述为一个下一个令牌生成（next-token generation）任务。为了进一步利用领域信息提升性能，我们在语义标识符学习中引入了域感知对比损失（domain-aware contrastive loss），并对统一推荐器进行域特定微调。在五个公共数据集上进行的大量实验表明，与一系列基线方法相比，GMC具有显著的有效性。"
    },
    {
        "title": "Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better\n  Understand Public Concerns about Vaccines",
        "url": "http://arxiv.org/abs/2507.12840v1",
        "pub_date": "2025-07-17",
        "summary": "Vaccine hesitancy threatens public health, leading to delayed or rejected vaccines. Social media is a vital source for understanding public concerns, and traditional methods like topic modelling often struggle to capture nuanced opinions. Though trained for query answering, large Language Models (LLMs) often miss current events and community concerns. Additionally, hallucinations in LLMs can compromise public health communication. To address these limitations, we developed a tool (VaxPulse Query Corner) using the Retrieval Augmented Generation technique. It addresses complex queries about public vaccine concerns on various online platforms, aiding public health administrators and stakeholders in understanding public concerns and implementing targeted interventions to boost vaccine confidence. Analysing 35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and relevance (0.94).",
        "translated": "疫苗犹豫对公共健康构成威胁，导致疫苗接种延迟或被拒绝。社交媒体是了解公众担忧的重要来源，但主题建模等传统方法往往难以捕捉细致入微的观点。尽管大型语言模型（LLMs）经过了查询回答的训练，但它们常常遗漏时事和社区关注点。此外，LLMs中的幻觉可能损害公共健康传播。\n\n为解决这些局限性，我们开发了一款采用检索增强生成（RAG）技术的工具（VaxPulse Query Corner）。该工具能够处理关于各种在线平台上公众疫苗担忧的复杂查询，从而帮助公共卫生管理人员和利益相关者了解公众担忧，并实施有针对性的干预措施以提高疫苗信心。通过分析35,103条关于带状疱疹疫苗（Shingrix）的社交媒体帖子，该工具在回答忠实度上达到了0.96，在相关性上达到了0.94。"
    },
    {
        "title": "PinFM: Foundation Model for User Activity Sequences at a Billion-scale\n  Visual Discovery Platform",
        "url": "http://arxiv.org/abs/2507.12704v1",
        "pub_date": "2025-07-17",
        "summary": "User activity sequences have emerged as one of the most important signals in recommender systems. We present a foundational model, PinFM, for understanding user activity sequences across multiple applications at a billion-scale visual discovery platform. We pretrain a transformer model with 20B+ parameters using extensive user activity data, then fine-tune it for specific applications, efficiently coupling it with existing models. While this pretraining-and-fine-tuning approach has been popular in other domains, such as Vision and NLP, its application in industrial recommender systems presents numerous challenges. The foundational model must be scalable enough to score millions of items every second while meeting tight cost and latency constraints imposed by these systems. Additionally, it should capture the interactions between user activities and other features and handle new items that were not present during the pretraining stage.   We developed innovative techniques to address these challenges. Our infrastructure and algorithmic optimizations, such as the Deduplicated Cross-Attention Transformer (DCAT), improved our throughput by 600% on Pinterest internal data. We demonstrate that PinFM can learn interactions between user sequences and candidate items by altering input sequences, leading to a 20% increase in engagement with new items. PinFM is now deployed to help improve the experience of more than a half billion users across various applications.",
        "translated": "用户活动序列已成为推荐系统中最重要的信号之一。我们提出了一个基础模型PinFM，用于在一个十亿级规模的视觉发现平台中理解跨多个应用的用户活动序列。我们使用大量的用户活动数据预训练了一个拥有200亿以上参数的Transformer模型，然后针对特定应用进行微调，并有效地将其与现有模型结合。\n\n尽管这种预训练-微调方法在其他领域（如计算机视觉和自然语言处理）已广泛流行，但其在工业级推荐系统中的应用面临诸多挑战。该基础模型必须具有足够的扩展性，能够每秒对数百万个物品进行评分，同时满足这些系统施加的严格成本和延迟约束。此外，它应能捕获用户活动与其他特征之间的交互，并处理在预训练阶段未出现过的新物品。\n\n我们开发了创新技术来解决这些挑战。我们的基础设施和算法优化，例如去重交叉注意力Transformer (DCAT)，将我们在Pinterest内部数据上的吞吐量提高了600%。我们证明PinFM能够通过改变输入序列来学习用户序列与候选物品之间的交互，从而使新物品的互动率提高了20%。PinFM现已部署，旨在帮助改善超过5亿用户在各种应用中的体验。"
    },
    {
        "title": "A Survey of Context Engineering for Large Language Models",
        "url": "http://arxiv.org/abs/2507.13334v1",
        "pub_date": "2025-07-17",
        "summary": "The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.",
        "translated": "大语言模型（LLM）的性能从根本上取决于推理过程中提供的上下文信息。本综述介绍了上下文工程（Context Engineering），这是一门超越简单提示词设计的正式学科，旨在系统性地优化为LLM提供的信息载荷。我们提出了一个全面的分类体系，将上下文工程分解为：其基础组件，以及将这些组件集成到智能系统中的复杂实现。我们首先探讨了基础组件：上下文检索与生成、上下文处理和上下文管理。接着，我们探讨了这些组件如何通过架构集成，来创建复杂的系统实现：检索增强生成（RAG）、记忆系统与工具集成推理，以及多智能体系统。通过对1300多篇研究论文的系统性分析，本综述不仅为该领域确立了技术路线图，还揭示了一个关键的研究空白：模型能力之间存在根本不对称性。当前模型在先进上下文工程的增强下，在理解复杂上下文方面展现出卓越的能力，但在生成同样复杂、长篇幅的输出方面却表现出显著局限性。解决这一空白是未来研究的首要任务。最终，本综述为推动上下文感知人工智能发展的研究人员和工程师提供了一个统一的框架。"
    },
    {
        "title": "Social and Political Framing in Search Engine Results",
        "url": "http://arxiv.org/abs/2507.13325v1",
        "pub_date": "2025-07-17",
        "summary": "Search engines play a crucial role in shaping public discourse by influencing how information is accessed and framed. While prior research has extensively examined various dimensions of search bias -- such as content prioritization, indexical bias, political polarization, and sources of bias -- an important question remains underexplored: how do search engines and ideologically-motivated user queries contribute to bias in search results. This study analyzes the outputs of major search engines using a dataset of political and social topics. The findings reveal that search engines not only prioritize content in ways that reflect underlying biases but also that ideologically-driven user queries exacerbate these biases, resulting in the amplification of specific narratives. Moreover, significant differences were observed across search engines in terms of the sources they prioritize. These results suggest that search engines may play a pivotal role in shaping public perceptions by reinforcing ideological divides, thereby contributing to the broader issue of information polarization.",
        "translated": "搜索引擎通过影响信息的获取和呈现方式，在塑造公共话语方面发挥着关键作用。尽管先前研究已广泛探讨了搜索偏见的各个维度——包括内容优先级、索引偏见、政治极化以及偏见来源——但一个重要问题仍未得到充分探究：搜索引擎本身以及受意识形态驱动的用户查询，如何共同导致搜索结果中产生偏见。本研究利用一个涵盖政治和社会主题的数据集，分析了主要搜索引擎的输出结果。研究结果揭示，搜索引擎不仅以反映潜在偏见的方式对内容进行优先排序，而且受意识形态驱动的用户查询会加剧这些偏见，从而导致特定叙事被放大。此外，在优先考虑的信息来源方面，不同搜索引擎之间也存在显著差异。这些结果表明，搜索引擎可能通过强化意识形态分歧，在塑造公众认知方面扮演着关键角色，从而加剧了信息极化这一更广泛的问题。"
    },
    {
        "title": "Machine-Readable Ads: Accessibility and Trust Patterns for AI Web Agents\n  interacting with Online Advertisements",
        "url": "http://arxiv.org/abs/2507.12844v1",
        "pub_date": "2025-07-17",
        "summary": "Autonomous multimodal language models are rapidly evolving into web agents that can browse, click, and purchase items on behalf of users, posing a threat to display advertising designed for human eyes. Yet little is known about how these agents interact with ads or which design principles ensure reliable engagement. To address this, we ran a controlled experiment using a faithful clone of the news site TT.com, seeded with diverse ads: static banners, GIFs, carousels, videos, cookie dialogues, and paywalls. We ran 300 initial trials plus follow-ups using the Document Object Model (DOM)-centric Browser Use framework with GPT-4o, Claude 3.7 Sonnet, Gemini 2.0 Flash, and the pixel-based OpenAI Operator, across 10 realistic user tasks. Our results show these agents display severe satisficing: they never scroll beyond two viewports and ignore purely visual calls to action, clicking banners only when semantic button overlays or off-screen text labels are present. Critically, when sweepstake participation required a purchase, GPT-4o and Claude 3.7 Sonnet subscribed in 100% of trials, and Gemini 2.0 Flash in 70%, revealing gaps in cost-benefit analysis. We identified five actionable design principles-semantic overlays, hidden labels, top-left placement, static frames, and dialogue replacement, that make human-centric creatives machine-detectable without harming user experience. We also evaluated agent trustworthiness through \"behavior patterns\" such as cookie consent handling and subscription choices, highlighting model-specific risk boundaries and the urgent need for robust trust evaluation frameworks in real-world advertising.",
        "translated": "自主多模态语言模型正迅速发展成为网络代理，能够代表用户浏览、点击和购买商品，这对此类为人眼设计的展示广告构成了威胁。然而，关于这些代理如何与广告互动，以及哪些设计原则能确保可靠的互动，人们知之甚少。为此，我们使用新闻网站TT.com的一个高保真克隆版进行了一项对照实验，并在其中植入了多种类型的广告：静态横幅、GIF动图、轮播图、视频、Cookie对话框和付费墙。我们进行了300次初始试验以及后续试验，采用了以文档对象模型（DOM）为中心的浏览器使用框架，结合了GPT-4o、Claude 3.7 Sonnet、Gemini 2.0 Flash和基于像素的OpenAI Operator模型，针对10个真实用户任务进行了测试。\n\n我们的结果显示，这些代理表现出严重的“满足即可”（satisficing）倾向：它们从未滚动超过两个视口，并忽略纯视觉的行动号召，仅当存在语义按钮覆盖层或屏幕外文本标签时才点击横幅。关键的是，当抽奖活动要求购买时，GPT-4o和Claude 3.7 Sonnet在100%的试验中都进行了订阅，而Gemini 2.0 Flash在70%的试验中进行了订阅，这揭示了它们在成本效益分析中存在的缺陷。我们确定了五项可操作的设计原则——语义覆盖、隐藏标签、左上角放置、静态框架和对话框替换——这些原则能够使以人为中心的创意内容在不损害用户体验的情况下被机器检测到。我们还通过Cookie同意处理和订阅选择等“行为模式”评估了代理的可信度，强调了模型特有的风险界限，以及在现实世界广告中建立鲁棒信任评估框架的迫切需求。"
    },
    {
        "title": "HapticCap: A Multimodal Dataset and Task for Understanding User\n  Experience of Vibration Haptic Signals",
        "url": "http://arxiv.org/abs/2507.13318v1",
        "pub_date": "2025-07-17",
        "summary": "Haptic signals, from smartphone vibrations to virtual reality touch feedback, can effectively convey information and enhance realism, but designing signals that resonate meaningfully with users is challenging. To facilitate this, we introduce a multimodal dataset and task, of matching user descriptions to vibration haptic signals, and highlight two primary challenges: (1) lack of large haptic vibration datasets annotated with textual descriptions as collecting haptic descriptions is time-consuming, and (2) limited capability of existing tasks and models to describe vibration signals in text. To advance this area, we create HapticCap, the first fully human-annotated haptic-captioned dataset, containing 92,070 haptic-text pairs for user descriptions of sensory, emotional, and associative attributes of vibrations. Based on HapticCap, we propose the haptic-caption retrieval task and present the results of this task from a supervised contrastive learning framework that brings together text representations within specific categories and vibrations. Overall, the combination of language model T5 and audio model AST yields the best performance in the haptic-caption retrieval task, especially when separately trained for each description category.",
        "translated": "触觉信号，从智能手机振动到虚拟现实触觉反馈，能有效传达信息并增强真实感，但设计能真正引起用户共鸣的信号极具挑战。为此，我们引入了一个多模态数据集和任务，旨在将用户描述与振动触觉信号进行匹配，并强调了两个主要挑战：(1) 缺乏用文本描述标注的大规模触觉振动数据集，因为收集触觉描述非常耗时；(2) 现有任务和模型在文本描述振动信号方面的能力有限。为推进这一领域，我们创建了HapticCap，这是首个完全由人工标注的触觉-文本描述数据集，包含92,070个触觉-文本对，用于描述振动的感官、情感和联想属性。基于HapticCap，我们提出了触觉-描述检索任务，并展示了在一个监督对比学习框架下获得的该任务结果，该框架将特定类别内的文本表示与振动相结合。总体而言，语言模型T5和音频模型AST的组合在触觉-描述检索任务中取得了最佳性能，尤其是在针对每个描述类别进行单独训练时。"
    },
    {
        "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts\n  (PLABA) track",
        "url": "http://arxiv.org/abs/2507.14096v1",
        "pub_date": "2025-07-18",
        "summary": "Objective: Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary. Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems.   Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2). For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references. Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts.   Results: Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers. In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity. Automatic, reference-based metrics generally did not correlate well with manual judgments. In Task 2, systems struggled with identifying difficult terms and classifying how to replace them. When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity.   Conclusion: The PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools.",
        "translated": "**目标：** 语言模型的最新进展显示出潜力，能够将面向专业人士的生物医学文献改编为通俗易懂的语言，从而方便患者和护理人员获取。然而，它们的不可预测性，再加上在此领域内可能造成的严重危害，意味着有必要进行严格的评估。我们设立此赛道的目的旨在促进相关研究，并对最有前途的系统进行高质量评估。\n\n**方法：** 我们在2023年和2024年的文本检索会议（TREC）上举办了生物医学摘要通俗化改编（PLABA）赛道。任务包括对摘要进行完整、句子级别的重写（任务1），以及识别和替换难懂的术语（任务2）。对于任务1的自动评估，我们开发了一套包含四份专业撰写参考文本的数据集。任务1和任务2的提交内容都获得了生物医学专家的广泛人工评估。\n\n**结果：** 来自12个国家的12支队伍参与了该赛道，其所使用的模型类型从多层感知机到大型预训练Transformer模型不等。在任务1的人工评估中，表现最佳的模型在事实准确性和完整性方面可媲美人类水平，但在简洁性和简练性方面则不然。基于参考文本的自动评估指标通常与人工判断的相关性不佳。在任务2中，系统在识别难懂术语以及对其替换方式进行分类方面遇到了困难。然而，在生成替换内容时，基于大语言模型的系统在人工评估的准确性、完整性和简洁性方面表现良好，但在简练性方面则不然。\n\n**结论：** PLABA赛道展现了利用大型语言模型改编生物医学文献以供公众阅读的潜力，同时也凸显了它们的不足以及对改进自动基准测试工具的需求。"
    },
    {
        "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of\n  Heterogeneous Clinical Notes Across Hospital Visits",
        "url": "http://arxiv.org/abs/2507.14079v1",
        "pub_date": "2025-07-18",
        "summary": "Progress notes are among the most clinically meaningful artifacts in an Electronic Health Record (EHR), offering temporally grounded insights into a patient's evolving condition, treatments, and care decisions. Despite their importance, they are severely underrepresented in large-scale EHR datasets. For instance, in the widely used Medical Information Mart for Intensive Care III (MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress notes, leaving gaps in longitudinal patient narratives. In contrast, the dataset contains a diverse array of other note types, each capturing different aspects of care.   We present DENSE (Documenting Evolving Progress Notes from Scattered Evidence), a system designed to align with clinical documentation workflows by simulating how physicians reference past encounters while drafting progress notes. The system introduces a fine-grained note categorization and a temporal alignment mechanism that organizes heterogeneous notes across visits into structured, chronological inputs. At its core, DENSE leverages a clinically informed retrieval strategy to identify temporally and semantically relevant content from both current and prior visits. This retrieved evidence is used to prompt a large language model (LLM) to generate clinically coherent and temporally aware progress notes.   We evaluate DENSE on a curated cohort of patients with multiple visits and complete progress note documentation. The generated notes demonstrate strong longitudinal fidelity, achieving a temporal alignment ratio of $1.089$, surpassing the continuity observed in original notes. By restoring narrative coherence across fragmented documentation, our system supports improved downstream tasks such as summarization, predictive modeling, and clinical decision support, offering a scalable solution for LLM-driven note synthesis in real-world healthcare settings.",
        "translated": "病程记录是电子健康记录（EHR）中临床意义最重大的资料之一，它提供了基于时间的、关于患者不断变化的病情、治疗和护理决策的深刻洞察。尽管其重要性，但在大规模EHR数据集中，病程记录的代表性严重不足。例如，在广泛使用的重症监护医学信息库III (MIMIC-III) 数据集中，仅约8.56%的住院就诊包含病程记录，这在纵向患者病程叙述中留下了空白。相比之下，该数据集包含了多种多样的其他类型记录，每种记录都捕捉了护理的不同方面。\n\n我们提出了DENSE（从分散证据中记录演进的病程记录），该系统旨在通过模拟医生在起草病程记录时如何参考过去的就诊记录，从而与临床文档工作流程保持一致。该系统引入了细粒度的记录分类和时间对齐机制，可将跨就诊的异构记录组织成结构化的、按时间顺序的输入。其核心是，DENSE利用一种临床知情的检索策略，从当前和之前的就诊记录中识别时间上和语义上相关的内容。这些检索到的证据用于提示大型语言模型（LLM），以生成临床连贯且具有时间意识的病程记录。\n\n我们在一组经过精心筛选的、包含多次就诊且病程记录文档完整的患者队列上评估了DENSE。生成的记录显示出强大的纵向保真度，实现了1.089的时间对齐比，超越了原始记录中观察到的连续性。通过恢复碎片化文档的叙述连贯性，我们的系统支持改进的下游任务，例如总结、预测建模和临床决策支持，为真实世界的医疗环境中的LLM驱动的记录合成提供了一个可扩展的解决方案。"
    },
    {
        "title": "DUALRec: A Hybrid Sequential and Language Model Framework for\n  Context-Aware Movie Recommendation",
        "url": "http://arxiv.org/abs/2507.13957v1",
        "pub_date": "2025-07-18",
        "summary": "The modern recommender systems are facing an increasing challenge of modelling and predicting the dynamic and context-rich user preferences. Traditional collaborative filtering and content-based methods often struggle to capture the temporal patternings and evolving user intentions. While Large Language Models (LLMs) have gained gradual attention in recent years, by their strong semantic understanding and reasoning abilities, they are not inherently designed to model chronologically evolving user preference and intentions. On the other hand, for sequential models like LSTM (Long-Short-Term-Memory) which is good at capturing the temporal dynamics of user behaviour and evolving user preference over time, but still lacks a rich semantic understanding for comprehensive recommendation generation. In this study, we propose DUALRec (Dynamic User-Aware Language-based Recommender), a novel recommender that leverages the complementary strength of both models, which combines the temporal modelling abilities of LSTM networks with semantic reasoning power of the fine-tuned Large Language Models. The LSTM component will capture users evolving preference through their viewing history, while the fine-tuned LLM variants will leverage these temporal user insights to generate next movies that users might enjoy. Experimental results on MovieLens-1M dataset shows that the DUALRec model outperforms a wide range of baseline models, with comprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted Cumulative Gain (NDCG@k), and genre similarity metrics. This research proposes a novel architecture that bridges the gap between temporal sequence modeling and semantic reasoning, and offers a promising direction for developing more intelligent and context-aware recommenders.",
        "translated": "现代推荐系统正面临着建模和预测动态且上下文丰富的用户偏好日益严峻的挑战。传统的协同过滤和基于内容的推荐方法往往难以捕捉时间模式和不断演变的用户意图。尽管大型语言模型（LLMs）近年来凭借其强大的语义理解和推理能力逐渐受到关注，但它们并非天生就旨在建模按时间顺序演变的用户偏好和意图。另一方面，像LSTM（长短期记忆网络）这样的序列模型虽然擅长捕捉用户行为的时序动态和随时间演变的用户偏好，但仍缺乏丰富的语义理解能力，无法进行全面的推荐生成。\n\n在本研究中，我们提出DUALRec（动态用户感知语言推荐系统），这是一种新颖的推荐系统，融合了两种模型的互补优势，即结合了LSTM网络的时序建模能力和微调大型语言模型的语义推理能力。LSTM组件将通过用户的观看历史捕捉其不断演变的偏好，而经过微调的LLM变体将利用这些时序用户洞察来生成用户可能喜欢的下一部电影。在MovieLens-1M数据集上的实验结果表明，DUALRec模型在性能上优于多种基线模型，通过命中率（HR@k）、归一化折现累积增益（NDCG@k）和类型相似度指标等综合评估矩阵进行衡量。这项研究提出了一种新颖的架构，弥合了时序序列建模和语义推理之间的鸿沟，并为开发更智能、更具上下文感知能力的推荐系统提供了有前景的方向。"
    },
    {
        "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
        "url": "http://arxiv.org/abs/2507.13933v1",
        "pub_date": "2025-07-18",
        "summary": "Increasingly, web content is automatically generated by large language models (LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs plagiarize and hallucinate, LLM-dominant content can be unreliable and unethical. Yet, websites rarely disclose such content, and human readers struggle to distinguish it. Thus, we must develop reliable detectors for LLM-dominant content. However, state-of-the-art LLM detectors are insufficient, because they perform well mainly on clean, prose-like text, while web content has complex markup and diverse genres.   We propose a highly reliable, scalable pipeline that classifies entire websites. Instead of naively classifying text extracted from each page, we classify each site based on an LLM text detector's outputs of multiple prose-like pages. We train and evaluate our detector by collecting 2 distinct ground truth datasets totaling 120 sites, and obtain 100% accuracies testing across them. In the wild, we detect a sizable portion of sites as LLM-dominant among 10k sites in search engine results and 10k in Common Crawl archives. We find LLM-dominant sites are growing in prevalence and rank highly in search results, raising questions about their impact on end users and the overall Web ecosystem.",
        "translated": "随着时间的推移，越来越多的网络内容是由大型语言模型（LLMs）自动生成，且很少有人工干预。我们将此类内容称为“LLM主导型”内容。由于LLMs存在抄袭和“幻觉”现象，LLM主导型内容可能不可靠且不道德。然而，网站鲜少披露此类内容，人类读者也难以区分。因此，我们必须开发针对LLM主导型内容的高可靠性检测器。然而，最先进的LLM检测器仍显不足，因为它们主要在纯净的散文体文本上表现良好，而网络内容通常具有复杂的标记和多样的体裁。\n\n我们提出了一种高度可靠、可扩展的流水线，用于对整个网站进行分类。我们并非简单地对从每个页面提取的文本进行分类，而是根据LLM文本检测器对来自多个散文体页面的输出结果来对每个网站进行分类。我们通过收集总计120个网站的两个不同真值数据集来训练和评估我们的检测器，并在跨数据集测试中取得了100%的准确率。在实际应用中，我们从1万个搜索引擎结果网站和1万个Common Crawl存档网站中，检测到相当一部分网站属于LLM主导型。我们发现LLM主导型网站的普及率日益增长，且在搜索结果中排名靠前，这引发了关于它们对最终用户和整个网络生态系统影响的疑问。"
    },
    {
        "title": "PARK: Personalized academic retrieval with knowledge-graphs",
        "url": "http://arxiv.org/abs/2507.13910v1",
        "pub_date": "2025-07-18",
        "summary": "Academic Search is a search task aimed to manage and retrieve scientific documents like journal articles and conference papers. Personalization in this context meets individual researchers' needs by leveraging, through user profiles, the user related information (e.g. documents authored by a researcher), to improve search effectiveness and to reduce the information overload. While citation graphs are a valuable means to support the outcome of recommender systems, their use in personalized academic search (with, e.g. nodes as papers and edges as citations) is still under-explored.   Existing personalized models for academic search often struggle to fully capture users' academic interests. To address this, we propose a two-step approach: first, training a neural language model for retrieval, then converting the academic graph into a knowledge graph and embedding it into a shared semantic space with the language model using translational embedding techniques. This allows user models to capture both explicit relationships and hidden structures in citation graphs and paper content. We evaluate our approach in four academic search domains, outperforming traditional graph-based and personalized models in three out of four, with up to a 10\\% improvement in MAP@100 over the second-best model. This highlights the potential of knowledge graph-based user models to enhance retrieval effectiveness.",
        "translated": "学术搜索是一项旨在管理和检索科学文献（如期刊文章和会议论文）的检索任务。在此背景下，个性化通过利用用户画像中的用户相关信息（例如，研究者撰写的文档），来满足个体研究者的需求，以提高搜索效率并减少信息过载。尽管引用图是支持推荐系统结果的宝贵手段，但它们在个性化学术搜索（例如，节点为论文，边为引用关系）中的应用仍未得到充分探索。现有针对学术搜索的个性化模型往往难以充分捕捉用户的学术兴趣。\n\n为此，我们提出了一种两步法：首先，训练一个用于检索的神经网络语言模型；然后，将学术图谱转换为知识图谱，并采用平移嵌入技术将其与语言模型一同嵌入到共享的语义空间中。这使得用户模型能够捕捉引用图和论文内容中的显式关系和隐藏结构。我们在四个学术搜索领域评估了我们的方法，结果显示在其中三个领域中，我们的方法表现优于传统的基于图和个性化模型，相较于次优模型，MAP@100指标提升高达10%。这凸显了基于知识图谱的用户模型在增强检索效率方面的潜力。"
    },
    {
        "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data\n  Memorization and Knowledge Injection",
        "url": "http://arxiv.org/abs/2507.13859v1",
        "pub_date": "2025-07-18",
        "summary": "Nowadays, the importance of software with natural-language user interfaces cannot be underestimated. In particular, in Question Answering (QA) systems, generating a SPARQL query for a given natural-language question (often named Query Building) from the information retrieved from the same question is the central task of QA systems working over Knowledge Graphs (KGQA). Due to the rise of Large Language Models (LLMs), they are considered a well-suited method to increase the quality of the question-answering functionality, as there is still a lot of room for improvement, aiming for enhanced quality and trustworthiness. However, LLMs are trained on web data, where researchers have no control over whether the benchmark or the knowledge graph was already included in the training data. In this paper, we introduce a novel method that evaluates the quality of LLMs by generating a SPARQL query from a natural-language question under various conditions: (1) zero-shot SPARQL generation, (2) with knowledge injection, and (3) with \"anonymized\" knowledge injection. This enables us, for the first time, to estimate the influence of the training data on the QA quality improved by LLMs. Ultimately, this will help to identify how portable a method is or whether good results might mostly be achieved because a benchmark was already included in the training data (cf. LLM memorization). The developed method is portable, robust, and supports any knowledge graph; therefore, it could be easily applied to any KGQA or LLM, s.t., generating consistent insights into the actual LLM capabilities is possible.",
        "translated": "如今，具有自然语言用户界面的软件其重要性不容小觑。特别地，在问答（QA）系统中，从给定自然语言问题中检索到的信息来为其生成SPARQL查询（通常称作“查询构建”），是知识图谱问答（KGQA）系统的核心任务。随着大语言模型（LLMs）的兴起，它们被认为是提升问答功能质量的一种非常适合的方法，因为在提高质量和可信度方面仍有很大的改进空间。然而，LLMs是在网络数据上进行训练的，研究人员无法控制基准测试或知识图谱是否已包含在训练数据中。在本文中，我们提出了一种新颖的方法，通过在各种条件下从自然语言问题生成SPARQL查询来评估LLM的质量，这些条件包括：(1) 零样本SPARQL生成，(2) 知识注入，以及(3) “匿名化”知识注入。这使我们首次能够评估训练数据对LLM提升的问答质量的影响。最终，这将有助于弄清一种方法的可移植性如何，或者好的结果是否主要因为基准测试已包含在训练数据中而获得（参见LLM记忆化）。所开发的方法具有可移植性、鲁棒性，并支持任何知识图谱；因此，它可以轻松应用于任何知识图谱问答系统或大语言模型，从而能够对LLM的实际能力产生一致的洞察。"
    },
    {
        "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
        "url": "http://arxiv.org/abs/2507.13822v1",
        "pub_date": "2025-07-18",
        "summary": "Drug side effects are a major global health concern, necessitating advanced methods for their accurate detection and analysis. While Large Language Models (LLMs) offer promising conversational interfaces, their inherent limitations, including reliance on black-box training data, susceptibility to hallucinations, and lack of domain-specific knowledge, hinder their reliability in specialized fields like pharmacovigilance. To address this gap, we propose two architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which integrate comprehensive drug side effect knowledge into a Llama 3 8B language model. Through extensive evaluations on 19,520 drug side effect associations (covering 976 drugs and 3,851 side effect terms), our results demonstrate that GraphRAG achieves near-perfect accuracy in drug side effect retrieval. This framework offers a highly accurate and scalable solution, signifying a significant advancement in leveraging LLMs for critical pharmacovigilance applications.",
        "translated": "药物副作用是一个重大的全球健康问题，因此需要先进的方法进行准确检测和分析。尽管大语言模型（LLMs）提供了前景广阔的对话界面，但其固有的局限性，包括对黑盒训练数据的依赖、易产生幻觉以及缺乏领域特定知识，影响了它们在药物警戒等专业领域的可靠性。为弥补这一不足，我们提出了两种架构：检索增强生成（RAG）和GraphRAG，它们将全面的药物副作用知识整合到Llama 3 8B语言模型中。通过对19,520个药物副作用关联（涵盖976种药物和3,851个副作用术语）进行广泛评估，我们的结果表明GraphRAG在药物副作用检索中实现了近乎完美的准确率。该框架提供了一个高度准确且可扩展的解决方案，标志着在利用大语言模型（LLMs）处理关键药物警戒应用方面的一项重大进展。"
    },
    {
        "title": "Point of Interest Recommendation: Pitfalls and Viable Solutions",
        "url": "http://arxiv.org/abs/2507.13725v1",
        "pub_date": "2025-07-18",
        "summary": "Point of interest (POI) recommendation can play a pivotal role in enriching tourists' experiences by suggesting context-dependent and preference-matching locations and activities, such as restaurants, landmarks, itineraries, and cultural attractions. Unlike some more common recommendation domains (e.g., music and video), POI recommendation is inherently high-stakes: users invest significant time, money, and effort to search, choose, and consume these suggested POIs. Despite the numerous research works in the area, several fundamental issues remain unresolved, hindering the real-world applicability of the proposed approaches. In this paper, we discuss the current status of the POI recommendation problem and the main challenges we have identified. The first contribution of this paper is a critical assessment of the current state of POI recommendation research and the identification of key shortcomings across three main dimensions: datasets, algorithms, and evaluation methodologies. We highlight persistent issues such as the lack of standardized benchmark datasets, flawed assumptions in the problem definition and model design, and inadequate treatment of biases in the user behavior and system performance. The second contribution is a structured research agenda that, starting from the identified issues, introduces important directions for future work related to multistakeholder design, context awareness, data collection, trustworthiness, novel interactions, and real-world evaluation.",
        "translated": "兴趣点 (POI) 推荐在丰富游客体验方面可发挥关键作用，它能推荐与上下文相关并与偏好匹配的地点和活动，例如餐厅、地标、行程和文化景点。与一些更常见的推荐领域（如音乐和视频）不同，POI 推荐本质上投入巨大、事关重大：用户需要投入大量时间、金钱和精力来搜索、选择和消费这些被推荐的 POI。尽管该领域已有诸多研究成果，但一些根本性问题仍未解决，阻碍了所提出方法在实际世界中的应用。在本文中，我们讨论了 POI 推荐问题的现状以及我们所发现的主要挑战。本文的第一个贡献是对 POI 推荐研究现状进行批判性审视，并从数据集、算法和评估方法三个主要维度识别了关键不足。我们强调了长期存在的问题，例如缺乏标准化的基准数据集、问题定义和模型设计中存在的有缺陷的假设，以及对用户行为和系统性能中偏差处理不足。第二个贡献是一个结构化的研究议程，它从已识别的问题出发，提出了未来工作的重要方向，包括多方利益相关者设计、上下文感知、数据收集、可信度、新颖的交互方式和真实世界评估。"
    },
    {
        "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical\n  Perspective",
        "url": "http://arxiv.org/abs/2507.14121v1",
        "pub_date": "2025-07-18",
        "summary": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in neural computation that offer a mathematically grounded alternative to standard neural networks. This study presents an empirical evaluation of KANs in context of class imbalanced classification, using ten benchmark datasets. We observe that KANs can inherently perform well on raw imbalanced data more effectively than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However, conventional imbalance strategies fundamentally conflict with KANs mathematical structure as resampling and focal loss implementations significantly degrade KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from prohibitive computational costs without proportional performance gains. Statistical validation confirms that MLPs with imbalance techniques achieve equivalence with KANs (|d| &lt; 0.08 across metrics) at minimal resource costs. These findings reveal that KANs represent a specialized solution for raw imbalanced data where resources permit. But their severe performance-resource tradeoffs and incompatibility with standard resampling techniques currently limits practical deployment. We identify critical research priorities as developing KAN specific architectural modifications for imbalance learning, optimizing computational efficiency, and theoretical reconciling their conflict with data augmentation. This work establishes foundational insights for next generation KAN architectures in imbalanced classification scenarios.",
        "translated": "柯尔莫哥洛夫-阿诺德网络（KANs）是神经计算领域近期的一项架构创新，为标准神经网络提供了一种具有数学基础的替代方案。本研究对KANs在类别不平衡分类背景下的性能进行了实证评估，使用了十个基准数据集。我们发现，KANs在处理原始不平衡数据时，无需任何重采样策略即可比多层感知机（MLPs）表现出固有的更优性能。然而，传统的类别不平衡策略与KANs的数学结构存在根本性冲突，因为重采样和焦点损失的实现会显著降低KANs的性能，同时仅能给MLPs带来微不足道的益处。\n\n关键的是，KANs面临着高昂的计算成本，而其性能提升却不成比例。统计验证证实，采用不平衡技术的MLPs能够在极低的资源成本下，与KANs在性能上达到等效（各项指标的|d| < 0.08）。这些发现表明，KANs是处理原始不平衡数据的一种专用解决方案，但其适用前提是资源允许。然而，其严峻的性能-资源权衡以及与标准重采样技术的不兼容性，目前限制了其实际部署。我们确定了关键的研究重点，包括开发针对不平衡学习的KANs专用架构修改、优化计算效率，以及从理论上协调其与数据增强的冲突。本工作为下一代KANs架构在不平衡分类场景中的发展奠定了基础性见解。"
    },
    {
        "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts\n  (PLABA) track",
        "url": "http://arxiv.org/abs/2507.14096v1",
        "pub_date": "2025-07-18",
        "summary": "Objective: Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary. Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems.   Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2). For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references. Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts.   Results: Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers. In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity. Automatic, reference-based metrics generally did not correlate well with manual judgments. In Task 2, systems struggled with identifying difficult terms and classifying how to replace them. When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity.   Conclusion: The PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools.",
        "translated": "**目标：** 语言模型的最新进展已显示出将面向专业人士的生物医学文献改编为通俗语言的潜力，使其可供患者和护理人员理解。然而，其固有的不可预测性，以及该领域高度的潜在危害，意味着有必要进行严格的评估。我们设置该专题的目标是激发相关研究，并对最有前景的系统提供高质量的评估。\n\n**方法：** 我们在2023年和2024年的文本检索大会（TREC）上主办了生物医学摘要通俗化改编（Plain Language Adaptation of Biomedical Abstracts, PLABA）专题。任务包括摘要的完整句级重写（任务1），以及识别和替换难懂术语（任务2）。为了对任务1进行自动化评估，我们开发了一套四份由专业人员编写的参考文本。任务1和任务2的提交结果均由生物医学专家进行了广泛的人工评估。\n\n**结果：** 来自12个国家的12支团队参与了该专题，其所使用的模型涵盖了从多层感知机到大型预训练Transformer的各种类型。在对任务1的人工评估中，表现最佳的模型在事实准确性和完整性方面可与人类水平媲美，但在简洁性和精炼性方面则未能达到人类水平。自动化、基于参考文本的度量指标通常与人工判断的相关性不佳。在任务2中，系统在识别难懂术语以及分类如何替换这些术语方面遇到了困难。然而，在生成替换文本时，基于大型语言模型（LLM）的系统在人工评估的准确性、完整性和简洁性方面表现良好，但在精炼性方面则表现不佳。\n\n**结论：** PLABA专题展现了利用大型语言模型将生物医学文献改编为面向公众的文本的潜力，同时也揭示了它们的不足，并强调了对改进自动化基准测试工具的需求。"
    },
    {
        "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of\n  Heterogeneous Clinical Notes Across Hospital Visits",
        "url": "http://arxiv.org/abs/2507.14079v1",
        "pub_date": "2025-07-18",
        "summary": "Progress notes are among the most clinically meaningful artifacts in an Electronic Health Record (EHR), offering temporally grounded insights into a patient's evolving condition, treatments, and care decisions. Despite their importance, they are severely underrepresented in large-scale EHR datasets. For instance, in the widely used Medical Information Mart for Intensive Care III (MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress notes, leaving gaps in longitudinal patient narratives. In contrast, the dataset contains a diverse array of other note types, each capturing different aspects of care.   We present DENSE (Documenting Evolving Progress Notes from Scattered Evidence), a system designed to align with clinical documentation workflows by simulating how physicians reference past encounters while drafting progress notes. The system introduces a fine-grained note categorization and a temporal alignment mechanism that organizes heterogeneous notes across visits into structured, chronological inputs. At its core, DENSE leverages a clinically informed retrieval strategy to identify temporally and semantically relevant content from both current and prior visits. This retrieved evidence is used to prompt a large language model (LLM) to generate clinically coherent and temporally aware progress notes.   We evaluate DENSE on a curated cohort of patients with multiple visits and complete progress note documentation. The generated notes demonstrate strong longitudinal fidelity, achieving a temporal alignment ratio of $1.089$, surpassing the continuity observed in original notes. By restoring narrative coherence across fragmented documentation, our system supports improved downstream tasks such as summarization, predictive modeling, and clinical decision support, offering a scalable solution for LLM-driven note synthesis in real-world healthcare settings.",
        "translated": "病程记录是电子健康档案（EHR）中具有重要临床意义的文档，能提供关于患者不断变化的病情、治疗和护理决策的时间维度上的深入了解。尽管其重要性，但在大规模EHR数据集中，病程记录的代表性却严重不足。例如，在广泛使用的重症监护医疗信息市场III（MIMIC-III）数据集中，仅约有8.56%的住院就诊包含病程记录，这在患者的纵向叙事中留下了空白。相比之下，该数据集包含各种其他类型的记录，每种都捕捉到不同的护理方面。\n\n我们提出了DENSE（Documenting Evolving Progress Notes from Scattered Evidence）系统，该系统旨在通过模拟医生在起草病程记录时如何参考过去的就诊记录，以与临床文档工作流程保持一致。该系统引入了细粒度记录分类和时间对齐机制，能够将跨就诊的异构记录组织成结构化、按时间顺序的输入。DENSE的核心在于，它利用临床知情的检索策略，从当前和之前的就诊中识别出时间上和语义上相关的内容。这些检索到的证据用于提示大型语言模型（LLM），以生成临床连贯且具有时间感知能力的病程记录。\n\n我们在经过精心筛选的、包含多次就诊且病程记录文档完整的患者队列上对DENSE进行了评估。生成的记录显示出强大的纵向保真度，时间对齐率达到1.089，超过了原始记录中观察到的连续性。通过恢复支离破碎文档之间的叙事连贯性，我们的系统支持改进的下游任务，例如总结、预测建模和临床决策支持，为现实世界医疗环境中的LLM驱动记录合成提供了可扩展的解决方案。"
    },
    {
        "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of\n  Heterogeneous Clinical Notes Across Hospital Visits",
        "url": "http://arxiv.org/abs/2507.14079v1",
        "pub_date": "2025-07-18",
        "summary": "Progress notes are among the most clinically meaningful artifacts in an Electronic Health Record (EHR), offering temporally grounded insights into a patient's evolving condition, treatments, and care decisions. Despite their importance, they are severely underrepresented in large-scale EHR datasets. For instance, in the widely used Medical Information Mart for Intensive Care III (MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress notes, leaving gaps in longitudinal patient narratives. In contrast, the dataset contains a diverse array of other note types, each capturing different aspects of care.   We present DENSE (Documenting Evolving Progress Notes from Scattered Evidence), a system designed to align with clinical documentation workflows by simulating how physicians reference past encounters while drafting progress notes. The system introduces a fine-grained note categorization and a temporal alignment mechanism that organizes heterogeneous notes across visits into structured, chronological inputs. At its core, DENSE leverages a clinically informed retrieval strategy to identify temporally and semantically relevant content from both current and prior visits. This retrieved evidence is used to prompt a large language model (LLM) to generate clinically coherent and temporally aware progress notes.   We evaluate DENSE on a curated cohort of patients with multiple visits and complete progress note documentation. The generated notes demonstrate strong longitudinal fidelity, achieving a temporal alignment ratio of $1.089$, surpassing the continuity observed in original notes. By restoring narrative coherence across fragmented documentation, our system supports improved downstream tasks such as summarization, predictive modeling, and clinical decision support, offering a scalable solution for LLM-driven note synthesis in real-world healthcare settings.",
        "translated": "进展记录是电子健康记录（EHR）中临床上最具意义的记录类型之一，它提供基于时间的深入洞察，展现了患者不断变化的病情、治疗方案和护理决策。尽管它们的重要性不言而喻，但在大规模EHR数据集中，进展记录却严重不足。例如，在广泛使用的重症监护医学信息集III (MIMIC-III) 数据集中，仅约有8.56%的住院就诊包含进展记录，这在患者的纵向叙述中留下了空白。相比之下，该数据集包含多种其他类型的记录，每种记录都捕捉到护理的不同方面。\n\n我们提出了DENSE（从分散证据中记录演进的进展记录），一个旨在模拟医生在起草进展记录时如何参考过去的就诊记录，从而与临床文档工作流程对齐的系统。该系统引入了细粒度记录分类和时间对齐机制，能够将不同就诊中异构的记录组织成结构化、时序化的输入。DENSE 的核心在于，它利用一种临床知情检索策略，从当前和之前就诊中识别出时间上和语义上相关的内容。这些检索到的证据用于提示大型语言模型（LLM）生成临床连贯且时间感知的进展记录。\n\n我们在由具有多次就诊且拥有完整进展记录的患者组成的精选队列上评估了DENSE。生成的记录展现出强大的纵向保真度，实现了1.089的时间对齐比率，超越了原始记录中观察到的连续性。通过恢复碎片化文档中的叙述连贯性，我们的系统支持改进摘要、预测建模和临床决策支持等下游任务，为真实世界医疗环境中LLM驱动的记录合成提供了一个可扩展的解决方案。"
    },
    {
        "title": "Efficient Temporal Tokenization for Mobility Prediction with Large\n  Language Models",
        "url": "http://arxiv.org/abs/2507.14017v1",
        "pub_date": "2025-07-18",
        "summary": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a framework that leverages large language models (LLMs) as spatio-temporal predictors and trajectory reasoners. RHYTHM partitions trajectories into daily segments encoded as discrete tokens with hierarchical attention, capturing both daily and weekly dependencies while substantially reducing the sequence length. Token representations are enriched with pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability to capture interdependencies without extensive computational overhead. By freezing the LLM backbone, RHYTHM achieves significant computational efficiency. Evaluation on three real-world datasets demonstrates a 2.4% improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in training time compared to state-of-the-art methods.",
        "translated": "我们引入了RHYTHM（基于层次化时间分词的人类移动性推理），这是一个利用大语言模型（LLMs）作为时空预测器和轨迹推理器的框架。RHYTHM将轨迹划分为每日片段，并将其编码为具有层次化注意力机制的离散token，从而捕获每日和每周依赖关系，同时显著缩短了序列长度。通过冻结的LLM，Token表示通过预计算的提示嵌入得到增强，这提升了模型捕获相互依赖关系的能力，同时无需引入大量计算开销。通过冻结LLM骨干网络，RHYTHM实现了显著的计算效率。在三个真实世界数据集上的评估结果表明，与最先进方法相比，RHYTHM在准确率方面提升了2.4%，周末准确率提升了5.0%，并且训练时间减少了24.6%。"
    },
    {
        "title": "Just Ask for Music (JAM): Multimodal and Personalized Natural Language\n  Music Recommendation",
        "url": "http://arxiv.org/abs/2507.15826v1",
        "pub_date": "2025-07-21",
        "summary": "Natural language interfaces offer a compelling approach for music recommendation, enabling users to express complex preferences conversationally. While Large Language Models (LLMs) show promise in this direction, their scalability in recommender systems is limited by high costs and latency. Retrieval-based approaches using smaller language models mitigate these issues but often rely on single-modal item representations, overlook long-term user preferences, and require full model retraining, posing challenges for real-world deployment. In this paper, we present JAM (Just Ask for Music), a lightweight and intuitive framework for natural language music recommendation. JAM models user-query-item interactions as vector translations in a shared latent space, inspired by knowledge graph embedding methods like TransE. To capture the complexity of music and user intent, JAM aggregates multimodal item features via cross-attention and sparse mixture-of-experts. We also introduce JAMSessions, a new dataset of over 100k user-query-item triples with anonymized user/item embeddings, uniquely combining conversational queries and user long-term preferences. Our results show that JAM provides accurate recommendations, produces intuitive representations suitable for practical use cases, and can be easily integrated with existing music recommendation stacks.",
        "translated": "自然语言接口为音乐推荐提供了一种引人注目的方法，使用户能够以对话式地表达复杂偏好。尽管大语言模型（LLM）在此方向上展现出潜力，但它们在推荐系统中的可扩展性受限于高成本和高延迟。使用较小语言模型的基于检索的方法缓解了这些问题，但它们通常依赖单模态物品表示，忽视用户长期偏好，并需要全模型重训练，这给实际部署带来了挑战。\n\n在本文中，我们提出了JAM（Just Ask for Music），一个用于自然语言音乐推荐的轻量级且直观的框架。JAM将用户-查询-物品交互建模为共享隐空间中的向量平移，其灵感来源于TransE等知识图谱嵌入方法。为了捕捉音乐和用户意图的复杂性，JAM通过交叉注意力（cross-attention）和稀疏专家混合（sparse mixture-of-experts）聚合多模态物品特征。我们还引入了JAMSessions，这是一个包含超过10万个用户-查询-物品三元组的新数据集，其中包含匿名化用户/物品嵌入，独特地结合了对话式查询和用户长期偏好。我们的结果表明，JAM提供了准确的推荐，生成了适用于实际用例的直观表示，并且可以轻松集成到现有音乐推荐技术栈中。"
    },
    {
        "title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme",
        "url": "http://arxiv.org/abs/2507.15742v1",
        "pub_date": "2025-07-21",
        "summary": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably the most celebrated mathematical expression in the history of information retrieval. Conceived as a simple heuristic quantifying the extent to which a given term's occurrences are concentrated in any one given document out of many, TF-IDF and its many variants are routinely used as term-weighting schemes in diverse text analysis applications. There is a growing body of scholarship dedicated to placing TF-IDF on a sound theoretical foundation. Building on that tradition, this paper justifies the use of TF-IDF to the statistics community by demonstrating how the famed expression can be understood from a significance testing perspective. We show that the common TF-IDF variant TF-ICF is, under mild regularity conditions, closely related to the negative logarithm of the $p$-value from a one-tailed version of Fisher's exact test of statistical significance. As a corollary, we establish a connection between TF-IDF and the said negative log-transformed $p$-value under certain idealized assumptions. We further demonstrate, as a limiting case, that this same quantity converges to TF-IDF in the limit of an infinitely large document collection. The Fisher's exact test justification of TF-IDF equips the working statistician with a ready explanation of the term-weighting scheme's long-established effectiveness.",
        "translated": "词频-逆文档频率（简称TF-IDF）可以说是信息检索史上最著名的数学表达式。TF-IDF 最初被构想为一种简单的启发式方法，用于量化在众多文档中，给定术语在特定文档中出现的集中程度。TF-IDF 及其众多变体常被用作词项加权方案，广泛应用于各种文本分析场景。越来越多的学术研究致力于为TF-IDF奠定坚实的理论基础。延续这一传统，本文通过展示这一著名表达式如何从显著性检验的角度来理解，从而向统计学界证明了TF-IDF的合理性。我们表明，在温和的正则性条件下，常见的TF-IDF变体TF-ICF与来自费舍尔精确显著性检验（单侧版本）的$p$值的负对数密切相关。作为推论，在某些理想化假设下，我们建立了TF-IDF与所述负对数转换$p$值之间的联系。我们进一步证明，作为一种极限情况，当文档集趋于无限大时，该相同量收敛于TF-IDF。TF-IDF基于费舍尔精确检验的论证，为在职统计学家提供了关于该词项加权方案长期以来行之有效原因的现成解释。"
    },
    {
        "title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders",
        "url": "http://arxiv.org/abs/2507.15551v1",
        "pub_date": "2025-07-21",
        "summary": "Recent progress on large language models (LLMs) has spurred interest in scaling up recommendation systems, yet two practical obstacles remain. First, training and serving cost on industrial Recommenders must respect strict latency bounds and high QPS demands. Second, most human-designed feature-crossing modules in ranking models were inherited from the CPU era and fail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and poor scalability. We introduce RankMixer, a hardware-aware model design tailored towards a unified and scalable feature-interaction architecture. RankMixer retains the transformer's high parallelism while replacing quadratic self-attention with multi-head token mixing module for higher efficiency. Besides, RankMixer maintains both the modeling for distinct feature subspaces and cross-feature-space interactions with Per-token FFNs. We further extend it to one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic routing strategy is adapted to address the inadequacy and imbalance of experts training. Experiments show RankMixer's superior scaling abilities on a trillion-scale production dataset. By replacing previously diverse handcrafted low-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and scale our ranking model parameters by 100x while maintaining roughly the same inference latency. We verify RankMixer's universality with online A/B tests across three core application scenarios (Recommendation, Advertisement and Search). Finally, we launch 1B Dense-Parameters RankMixer for full traffic serving without increasing the serving cost, which improves user active days by 0.2% and total in-app usage duration by 0.5%.",
        "translated": "大语言模型（LLMs）的近期进展激发了扩展推荐系统规模的兴趣，然而，仍存在两大实际障碍。首先，工业级推荐系统的训练和部署成本必须满足严格的延迟限制和高QPS（每秒查询量）需求。其次，排序模型中大多数人工设计的特征交叉模块继承自CPU时代，未能充分利用现代GPU，导致模型浮点运算利用率（MFU）低下且可扩展性差。\n\n我们引入了RankMixer，这是一种硬件感知的模型设计，旨在构建统一且可扩展的特征交互架构。RankMixer保留了Transformer模型的高并行性，同时用多头token混合模块替代了二次方的自注意力机制，以提高效率。此外，RankMixer通过逐token前馈网络（Per-token FFNs）同时实现了对不同特征子空间的建模以及跨特征空间交互。我们进一步通过稀疏MoE（专家混合）变体将其扩展到十亿参数，以获得更高的投资回报率（ROI）。针对专家训练中的不足和不平衡问题，我们采用了动态路由策略。\n\n实验表明，RankMixer在万亿级生产数据集上展现出卓越的扩展能力。通过用RankMixer替代先前多样化的人工设计低MFU模块，我们将模型MFU从4.5%提升至45%，并将排序模型参数规模扩大100倍，同时基本保持相同的推理延迟。我们通过在推荐、广告和搜索三个核心应用场景中进行的在线A/B测试，验证了RankMixer的普适性。最后，我们部署了10亿密集参数的RankMixer用于全流量服务，且未增加部署成本，此举使用户活跃天数提升了0.2%，应用内总使用时长提升了0.5%。"
    },
    {
        "title": "Hierarchical Graph Information Bottleneck for Multi-Behavior\n  Recommendation",
        "url": "http://arxiv.org/abs/2507.15395v1",
        "pub_date": "2025-07-21",
        "summary": "In real-world recommendation scenarios, users typically engage with platforms through multiple types of behavioral interactions. Multi-behavior recommendation algorithms aim to leverage various auxiliary user behaviors to enhance prediction for target behaviors of primary interest (e.g., buy), thereby overcoming performance limitations caused by data sparsity in target behavior records. Current state-of-the-art approaches typically employ hierarchical design following either cascading (e.g., view$\\rightarrow$cart$\\rightarrow$buy) or parallel (unified$\\rightarrow$behavior$\\rightarrow$specific components) paradigms, to capture behavioral relationships. However, these methods still face two critical challenges: (1) severe distribution disparities across behaviors, and (2) negative transfer effects caused by noise in auxiliary behaviors. In this paper, we propose a novel model-agnostic Hierarchical Graph Information Bottleneck (HGIB) framework for multi-behavior recommendation to effectively address these challenges. Following information bottleneck principles, our framework optimizes the learning of compact yet sufficient representations that preserve essential information for target behavior prediction while eliminating task-irrelevant redundancies. To further mitigate interaction noise, we introduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant edges through learnable edge dropout mechanisms. We conduct comprehensive experiments on three real-world public datasets, which demonstrate the superior effectiveness of our framework. Beyond these widely used datasets in the academic community, we further expand our evaluation on several real industrial scenarios and conduct an online A/B testing, showing again a significant improvement in multi-behavior recommendations. The source code of our proposed HGIB is available at https://github.com/zhy99426/HGIB.",
        "translated": "在实际推荐场景中，用户通常会与平台进行多种类型的行为交互。多行为推荐算法旨在利用多种辅助用户行为，以增强对主要目标行为（例如购买）的预测，从而克服由目标行为记录数据稀疏性所造成的性能限制。当前最先进的方法通常采用层次化设计，遵循级联（例如：浏览$\\rightarrow$加入购物车$\\rightarrow$购买）或并行（统一$\\rightarrow$行为$\\rightarrow$特定组件）范式，以捕捉行为间的关系。然而，这些方法仍面临两个关键挑战：(1) 行为间严重的分布差异；(2) 辅助行为中噪声引起的负迁移效应。\n\n在本文中，我们提出了一种新颖的模型无关的层次图信息瓶颈（HGIB）框架，用于多行为推荐，以有效解决这些挑战。遵循信息瓶颈原则，我们的框架优化了对紧凑但信息充足的表示学习，这些表示能够保留目标行为预测所需的基本信息，同时消除与任务无关的冗余。为了进一步减轻交互噪声，我们引入了一个图精炼编码器（GRE），它通过可学习的边丢弃机制动态修剪冗余边。我们在三个真实世界的公开数据集上进行了全面实验，结果证明了我们框架的卓越有效性。除了学术界广泛使用的这些数据集，我们还进一步将评估扩展到几个真实的工业场景，并进行了在线A/B测试，再次显示了多行为推荐的显著改进。我们提出的HGIB的源代码可在 https://github.com/zhy99426/HGIB 获取。"
    },
    {
        "title": "GREAT: Guiding Query Generation with a Trie for Recommending Related\n  Search about Video at Kuaishou",
        "url": "http://arxiv.org/abs/2507.15267v1",
        "pub_date": "2025-07-21",
        "summary": "Currently, short video platforms have become the primary place for individuals to share experiences and obtain information. To better meet users' needs for acquiring information while browsing short videos, some apps have introduced a search entry at the bottom of videos, accompanied with recommended relevant queries. This scenario is known as query recommendation in video-related search, where core task is item-to-query (I2Q) recommendation. As this scenario has only emerged in recent years, there is a notable scarcity of academic research and publicly available datasets in this domain. To address this gap, we systematically examine the challenges associated with this scenario for the first time. Subsequently, we release a large-scale dataset derived from real-world data pertaining to the query recommendation in video-\\textit{\\textbf{r}}elated \\textit{\\textbf{s}}earch on the \\textit{\\textbf{Kuai}}shou app (\\textbf{KuaiRS}). Presently, existing methods rely on embeddings to calculate similarity for matching short videos with queries, lacking deep interaction between the semantic content and the query. In this paper, we introduce a novel LLM-based framework named \\textbf{GREAT}, which \\textit{\\textbf{g}}uides que\\textit{\\textbf{r}}y g\\textit{\\textbf{e}}ner\\textit{\\textbf{a}}tion with a \\textit{\\textbf{t}}rie to address I2Q recommendation in related search. Specifically, we initially gather high-quality queries with high exposure and click-through rate to construct a query-based trie. During training, we enhance the LLM's capability to generate high-quality queries using the query-based trie. In the inference phase, the query-based trie serves as a guide for the token generation. Finally, we further refine the relevance and literal quality between items and queries via a post-processing module. Extensive offline and online experiments demonstrate the effectiveness of our proposed method.",
        "translated": "当前，短视频平台已成为个人分享经验和获取信息的主要场所。为了更好地满足用户在浏览短视频时获取信息的需求，一些应用在视频底部引入了搜索入口，并附带推荐的相关查询。这种场景被称为视频相关搜索中的查询推荐，其核心任务是物品到查询（Item-to-Query, I2Q）推荐。由于这种场景近年来才兴起，该领域内的学术研究和公开数据集都显著稀缺。\n\n为了弥补这一空白，我们首次系统性地研究了该场景的挑战。随后，我们发布了一个大规模数据集 KuaiRS，该数据集源自快手（Kuai）应用中视频相关（Related）搜索（Search）查询推荐的真实世界数据。当前，现有方法依赖于嵌入（embeddings）来计算相似性，以匹配短视频与查询，但缺乏短视频语义内容与查询之间的深度交互。\n\n在本文中，我们引入了一个新颖的基于大语言模型（LLM）的框架，名为 **GREAT**（基于Trie树指导查询生成），旨在解决相关搜索中的I2Q推荐问题。具体而言，我们首先收集了高曝光量和高点击率（CTR）的优质查询，以构建一个基于查询的Trie树。在训练阶段，我们利用所构建的查询Trie树增强LLM生成高质量查询的能力。在推理阶段，该查询Trie树作为词元生成的指导。最后，我们通过一个后处理模块进一步优化了物品与查询之间的相关性和字面质量。大量的离线和在线实验证明了我们所提出方法的有效性。"
    },
    {
        "title": "SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced\n  Academic Search",
        "url": "http://arxiv.org/abs/2507.15245v1",
        "pub_date": "2025-07-21",
        "summary": "Recent advances in large language models (LLMs) have opened new opportunities for academic literature retrieval. However, existing systems often rely on rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR, a multi-agent framework that incorporates RefChain-based query decomposition and query evolution to enable more flexible and effective search. To facilitate systematic evaluation, we also construct SPARBench, a challenging benchmark with expert-annotated relevance labels. Experimental results demonstrate that SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline. Together, SPAR and SPARBench provide a scalable, interpretable, and high-performing foundation for advancing research in scholarly retrieval. Code and data will be available at: https://github.com/xiaofengShi/SPAR",
        "translated": "大型语言模型（LLMs）的最新进展为学术文献检索开启了新的机遇。然而，现有系统普遍依赖固化的流程，并且推理能力有限。我们提出了SPAR，一个多智能体框架，它结合了基于RefChain的查询分解和查询演进，以实现更灵活有效的搜索。为了促进系统性评估，我们还构建了SPARBench，一个附带专家标注相关性标签的挑战性基准。实验结果表明，SPAR显著优于强大的基线，相较于表现最佳的基线，在AutoScholar上F1分数提升高达56%，在SPARBench上F1分数提升高达23%。SPAR和SPARBench共同为推动学术检索研究提供了一个可扩展、可解释且高性能的基础。代码和数据将在以下链接提供：https://github.com/xiaofengShi/SPAR"
    },
    {
        "title": "Click A, Buy B: Rethinking Conversion Attribution in E- Commerce\n  Recommendations",
        "url": "http://arxiv.org/abs/2507.15113v1",
        "pub_date": "2025-07-20",
        "summary": "User journeys in e-commerce routinely violate the one-to-one assumption that a clicked item on an advertising platform is the same item later purchased on the merchant's website/app. For a significant number of converting sessions on our platform, users click product A but buy product B -- the Click A, Buy B (CABB) phenomenon. Training recommendation models on raw click-conversion pairs therefore rewards items that merely correlate with purchases, leading to biased learning and sub-optimal conversion rates. We reframe conversion prediction as a multi-task problem with separate heads for Click A Buy A (CABA) and Click A Buy B (CABB). To isolate informative CABB conversions from unrelated CABB conversions, we introduce a taxonomy-aware collaborative filtering weighting scheme where each product is first mapped to a leaf node in a product taxonomy, and a category-to-category similarity matrix is learned from large-scale co-engagement logs. This weighting amplifies pairs that reflect genuine substitutable or complementary relations while down-weighting coincidental cross-category purchases. Offline evaluation on e-commerce sessions reduces normalized entropy by 13.9% versus a last-click attribution baseline. An online A/B test on live traffic shows +0.25% gains in the primary business metric.",
        "translated": "电子商务中的用户旅程常常违反一个一对一的假设，即在广告平台点击的商品与之后在商家网站/应用上购买的商品是同一个。在我们平台上的大量转化会话中，用户点击了商品A却购买了商品B——我们称之为“点击A购买B”（CABB）现象。因此，基于原始点击-转化对训练推荐模型会偏向于那些仅仅与购买行为相关的商品，从而导致有偏学习和次优的转化率。\n\n我们将转化预测重构为一个多任务问题，并为“点击A购买A”（CABA）和“点击A购买B”（CABB）分别设置了独立的预测头。为了从无关的CABB转化中分离出有信息量的CABB转化，我们引入了一种结合类目知识的协同过滤加权方案。在该方案中，每个商品首先被映射到产品类目树的叶节点，并从大规模的共同参与日志中学习得到一个类目间相似度矩阵。这种加权方式能够增强反映真正替代或互补关系的商品对，同时降低偶然发生的跨类目购买的权重。\n\n在电子商务会话上的离线评估显示，相较于末次点击归因基线，归一化熵降低了13.9%。在线上流量上进行的A/B测试表明，核心业务指标获得了0.25%的提升。"
    },
    {
        "title": "DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented\n  Generation Applications via Prompt Injection",
        "url": "http://arxiv.org/abs/2507.15042v1",
        "pub_date": "2025-07-20",
        "summary": "Adversarial prompt attacks can significantly alter the reliability of Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce incorrect outputs. In this paper, we present a novel method that applies Differential Evolution (DE) to optimize adversarial prompt suffixes for RAG-based question answering. Our approach is gradient-free, treating the RAG pipeline as a black box and evolving a population of candidate suffixes to maximize the retrieval rank of a targeted incorrect document to be closer to real world scenarios. We conducted experiments on the BEIR QA datasets to evaluate attack success at certain retrieval rank thresholds under multiple retrieving applications. Our results demonstrate that DE-based prompt optimization attains competitive (and in some cases higher) success rates compared to GGPP to dense retrievers and PRADA to sparse retrievers, while using only a small number of tokens (&lt;=5 tokens) in the adversarial suffix. Furthermore, we introduce a readability-aware suffix construction strategy, validated by a statistically significant reduction in MLM negative log-likelihood with Welch's t-test. Through evaluations with a BERT-based adversarial suffix detector, we show that DE-generated suffixes evade detection, yielding near-chance detection accuracy.",
        "translated": "对抗性提示攻击（Adversarial prompt attacks）可以通过重新排序检索结果，使其生成不正确输出，从而严重影响检索增强生成（RAG）系统的可靠性。本文提出了一种新颖的方法，该方法应用差分进化（DE）来优化用于基于RAG的问答的对抗性提示后缀。我们的方法是无梯度的，将RAG管道视为黑盒，并通过进化一组候选后缀来最大化目标错误文档的检索排名，旨在模拟更真实的攻击场景。我们在BEIR QA数据集上进行了实验，以评估在多种检索器下、特定检索排名阈值处的攻击成功率。我们的结果表明，与针对密集检索器的GGPP和针对稀疏检索器的PRADA相比，基于DE的提示优化取得了有竞争力的（在某些情况下甚至更高）成功率，同时对抗性后缀仅使用少量词元（<=5个词元）。此外，我们引入了一种可读性感知后缀构建策略，并通过Welch t检验验证了其在掩码语言模型（MLM）负对数似然方面的统计显著降低。通过使用一个基于BERT的对抗性后缀检测器进行评估，我们表明DE生成的后缀能够规避检测，产生了接近随机的检测准确率。"
    },
    {
        "title": "FullRecall: A Semantic Search-Based Ranking Approach for Maximizing\n  Recall in Patent Retrieval",
        "url": "http://arxiv.org/abs/2507.14946v1",
        "pub_date": "2025-07-20",
        "summary": "Patent examiners and inventors face significant pressure to verify the originality and non-obviousness of inventions, and the intricate nature of patent data intensifies the challenges of patent retrieval. Therefore, there is a pressing need to devise cutting-edge retrieval strategies that can reliably achieve the desired recall. This study introduces FullRecall, a novel patent retrieval approach that effectively manages the complexity of patent data while maintaining the reliability of relevance matching and maximising recall. It leverages IPC-guided knowledge to generate informative phrases, which are processed to extract key information in the form of noun phrases characterising the query patent under observation. From these, the top k keyphrases are selected to construct a query for retrieving a focused subset of the dataset. This initial retrieval step achieves complete recall, successfully capturing all relevant documents. To further refine the results, a ranking scheme is applied to the retrieved subset, reducing its size while maintaining 100% recall. This multi-phase process demonstrates an effective strategy for balancing precision and recall in patent retrieval tasks. Comprehensive experiments were conducted, and the results were compared with baseline studies, namely HRR2 [1] and ReQ-ReC [2]. The proposed approach yielded superior results, achieving 100% recall in all five test cases. However, HRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and 14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the second test case, and 0% for the third, fourth, and fifth test cases. The 100% recall ensures that no relevant prior art is overlooked, thereby strengthening the patent pre-filing and examination processes, hence reducing potential legal risks.",
        "translated": "专利审查员和发明人面临着验证发明新颖性和创造性的巨大压力，而专利数据的错综复杂性则加剧了专利检索的挑战。因此，迫切需要设计能够可靠实现所需查全率（recall）的前沿检索策略。本研究引入了FullRecall，这是一种新颖的专利检索方法，它能够有效应对专利数据的复杂性，同时保持相关性匹配的可靠性并最大化查全率。\n\n该方法利用IPC（国际专利分类）指导的知识来生成信息丰富的短语，然后对这些短语进行处理，以名词短语的形式提取描述待查询专利的关键信息。从中，选择前k个关键短语来构建查询，以检索数据集的一个聚焦子集。这一初步检索步骤实现了完全查全，成功捕获了所有相关文档。为了进一步优化结果，对检索到的子集应用了排序机制，在缩小其规模的同时保持了100%的查全率。这种多阶段过程展示了一种在专利检索任务中平衡查准率（precision）和查全率的有效策略。\n\n进行了全面的实验，并将结果与基线研究HRR2 [1]和ReQ-ReC [2]进行了比较。所提出的方法取得了优越的结果，在所有五个测试案例中均实现了100%的查全率。然而，HRR2 [1]在五个测试案例中的查全率分别为10%、25%、33.3%、0%和14.29%，而ReQ-ReC [2]在第一个测试案例中显示为50%，在第二个测试案例中为25%，在第三、第四和第五个测试案例中均为0%。100%的查全率确保了不会遗漏任何相关的现有技术，从而加强了专利提交前和审查过程，进而降低了潜在的法律风险。"
    },
    {
        "title": "User Invariant Preference Learning for Multi-Behavior Recommendation",
        "url": "http://arxiv.org/abs/2507.14925v1",
        "pub_date": "2025-07-20",
        "summary": "In multi-behavior recommendation scenarios, analyzing users' diverse behaviors, such as click, purchase, and rating, enables a more comprehensive understanding of their interests, facilitating personalized and accurate recommendations. A fundamental assumption of multi-behavior recommendation methods is the existence of shared user preferences across behaviors, representing users' intrinsic interests. Based on this assumption, existing approaches aim to integrate information from various behaviors to enrich user representations. However, they often overlook the presence of both commonalities and individualities in users' multi-behavior preferences. These individualities reflect distinct aspects of preferences captured by different behaviors, where certain auxiliary behaviors may introduce noise, hindering the prediction of the target behavior. To address this issue, we propose a user invariant preference learning for multi-behavior recommendation (UIPL for short), aiming to capture users' intrinsic interests (referred to as invariant preferences) from multi-behavior interactions to mitigate the introduction of noise. Specifically, UIPL leverages the paradigm of invariant risk minimization to learn invariant preferences. To implement this, we employ a variational autoencoder (VAE) to extract users' invariant preferences, replacing the standard reconstruction loss with an invariant risk minimization constraint. Additionally, we construct distinct environments by combining multi-behavior data to enhance robustness in learning these preferences. Finally, the learned invariant preferences are used to provide recommendations for the target behavior. Extensive experiments on four real-world datasets demonstrate that UIPL significantly outperforms current state-of-the-art methods.",
        "translated": "在多行为推荐场景中，分析用户多样化的行为，如点击、购买和评分，能够更全面地理解其兴趣，从而促进个性化和准确的推荐。多行为推荐方法的一个基本假设是，跨行为存在共享的用户偏好，这些偏好代表了用户内在的兴趣。基于这一假设，现有方法旨在整合来自不同行为的信息，以丰富用户表示。然而，它们通常忽略了用户多行为偏好中同时存在的共性和个体性。这些个体性反映了由不同行为捕获的偏好中不同的方面，其中某些辅助行为可能引入噪声，从而阻碍目标行为的预测。\n\n为解决这个问题，我们提出了一种用于多行为推荐的用户不变偏好学习方法（简称UIPL），旨在从多行为交互中捕获用户内在的兴趣（被称为不变偏好），以减轻噪声的引入。具体而言，UIPL利用不变风险最小化（IRM）范式来学习不变偏好。为实现这一点，我们采用变分自编码器（VAE）来提取用户的不变偏好，并用不变风险最小化约束替换标准的重构损失。此外，我们通过结合多行为数据构建不同的环境，以增强学习这些偏好的鲁棒性。最后，学习到的不变偏好被用于为目标行为提供推荐。在四个真实世界数据集上进行的大量实验表明，UIPL显著优于当前最先进的方法。"
    },
    {
        "title": "FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with\n  LLMs",
        "url": "http://arxiv.org/abs/2507.15839v1",
        "pub_date": "2025-07-21",
        "summary": "Synthetic data generation has emerged as an invaluable solution in scenarios where real-world data collection and usage are limited by cost and scarcity. Large language models (LLMs) have demonstrated remarkable capabilities in producing high-fidelity, domain-relevant samples across various fields. However, existing approaches that directly use LLMs to generate each record individually impose prohibitive time and cost burdens, particularly when large volumes of synthetic data are required. In this work, we propose a fast, cost-effective method for realistic tabular data synthesis that leverages LLMs to infer and encode each field's distribution into a reusable sampling script. By automatically classifying fields into numerical, categorical, or free-text types, the LLM generates distribution-based scripts that can efficiently produce diverse, realistic datasets at scale without continuous model inference. Experimental results show that our approach outperforms traditional direct methods in both diversity and data realism, substantially reducing the burden of high-volume synthetic data generation. We plan to apply this methodology to accelerate testing in production pipelines, thereby shortening development cycles and improving overall system efficiency. We believe our insights and lessons learned will aid researchers and practitioners seeking scalable, cost-effective solutions for synthetic data generation.",
        "translated": "合成数据生成已成为一种宝贵的解决方案，特别是在真实世界数据收集和使用受限于成本和稀缺性的场景中。大语言模型（LLM）已在各个领域展现出卓越的能力，能够生成高保真、领域相关的样本。然而，现有方法直接使用LLM逐条生成每条记录，这在需要大量合成数据时会带来高昂的时间和成本负担。\n\n在本研究中，我们提出了一种快速、经济高效的逼真表格数据合成方法，该方法利用LLM推断并编码每个字段的分布，将其转化为可重用的采样脚本。通过自动将字段分类为数值型、类别型或自由文本型，LLM能够生成基于分布的脚本，从而无需持续的模型推理即可高效地大规模生成多样化、逼真的数据集。实验结果表明，我们的方法在多样性和数据真实性方面均优于传统的直接方法，大幅降低了大批量合成数据生成的负担。我们计划将此方法应用于加速生产管线中的测试，从而缩短开发周期并提高整体系统效率。我们相信我们的见解和经验将有助于寻求可扩展、经济高效的合成数据生成解决方案的研究人员和从业者。"
    },
    {
        "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection\n  Framework for Document-level Sentiment Analysis",
        "url": "http://arxiv.org/abs/2507.14022v1",
        "pub_date": "2025-07-18",
        "summary": "This study proposes the Cognitive Pairwise Comparison Classification Model Selection (CPC-CMS) framework for document-level sentiment analysis. The CPC, based on expert knowledge judgment, is used to calculate the weights of evaluation criteria, including accuracy, precision, recall, F1-score, specificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and efficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random Forest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long Short-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from Transformers (ALBERT) are chosen as classification baseline models. A weighted decision matrix consisting of classification evaluation scores with respect to criteria weights, is formed to select the best classification model for a classification problem. Three open datasets of social media are used to demonstrate the feasibility of the proposed CPC-CMS. Based on our simulation, for evaluation results excluding the time factor, ALBERT is the best for the three datasets; if time consumption is included, no single model always performs better than the other models. The CPC-CMS can be applied to the other classification applications in different areas.",
        "translated": "本研究提出了一种用于文档级情感分析的认知成对比较分类模型选择（CPC-CMS）框架。该框架中的认知成对比较（CPC）基于专家知识判断，用于计算评估准则的权重，这些准则包括准确率、精确率、召回率、F1-分数、特异度、马修斯相关系数（MCC）、科恩Kappa系数（Kappa）和效率。本研究选择了朴素贝叶斯、线性支持向量分类（LSVC）、随机森林、逻辑回归、极端梯度提升（XGBoost）、长短期记忆网络（LSTM）以及ALBERT模型作为分类基线模型。通过构建一个由分类评估分数和准则权重组成的加权决策矩阵，从而为特定的分类问题选择最佳分类模型。本文利用三个社交媒体开放数据集，验证了所提出的CPC-CMS框架的可行性。根据我们的仿真结果，在排除时间因素的评估中，ALBERT在三个数据集上均表现最佳；但如果考虑时间消耗，则没有单一模型始终优于其他模型。CPC-CMS可以应用于不同领域的其他分类任务。"
    },
    {
        "title": "RAVine: Reality-Aligned Evaluation for Agentic Search",
        "url": "http://arxiv.org/abs/2507.16725v1",
        "pub_date": "2025-07-22",
        "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval augmentation, is driving the evolution of intelligent search systems. However, existing evaluation frameworks fail to align well with the goals of agentic search. First, the complex queries commonly used in current benchmarks often deviate from realistic user search scenarios. Second, prior approaches tend to introduce noise when extracting ground truth for end-to-end evaluations, leading to distorted assessments at a fine-grained level. Third, most current frameworks focus solely on the quality of final answers, neglecting the evaluation of the iterative process inherent to agentic search. To address these limitations, we propose RAVine -- a Reality-Aligned eValuation framework for agentic LLMs with search. RAVine targets multi-point queries and long-form answers that better reflect user intents, and introduces an attributable ground truth construction strategy to enhance the accuracy of fine-grained evaluation. Moreover, RAVine examines model's interaction with search tools throughout the iterative process, and accounts for factors of efficiency. We benchmark a series of models using RAVine and derive several insights, which we hope will contribute to advancing the development of agentic search systems. The code and datasets are available at https://github.com/SwordFaith/RAVine.",
        "translated": "智能体搜索（agentic search）作为一种更自主、更具适应性的检索增强范式，正在推动智能搜索系统的发展演进。然而，现有评估框架未能很好地契合智能体搜索的目标。首先，当前基准测试中常用的复杂查询通常偏离了现实的用户搜索场景。其次，以往的方法在提取端到端评估的真值（ground truth）时倾向于引入噪声，导致在细粒度层面的评估结果失真。第三，大多数现有框架仅关注最终答案的质量，忽视了对智能体搜索固有的迭代过程的评估。\n\n针对这些局限性，我们提出了RAVine——一个用于带搜索功能的智能体大型语言模型（LLM）的现实对齐评估框架（**R**eality-**A**ligned e**V**aluation framework for agentic LLMs w**i**th search）。RAVine旨在针对更能反映用户意图的多点查询和长篇答案，并引入了一种可归因的真值构建策略，以提高细粒度评估的准确性。此外，RAVine考察模型在整个迭代过程中与搜索工具的交互，并考虑了效率因素。我们使用RAVine对一系列模型进行了基准测试，并得出了一些见解，我们希望这些见解将有助于推动智能体搜索系统的发展。代码和数据集已发布在https://github.com/SwordFaith/RAVine。"
    },
    {
        "title": "Biases in LLM-Generated Musical Taste Profiles for Recommendation",
        "url": "http://arxiv.org/abs/2507.16708v1",
        "pub_date": "2025-07-22",
        "summary": "One particularly promising use case of Large Language Models (LLMs) for recommendation is the automatic generation of Natural Language (NL) user taste profiles from consumption data. These profiles offer interpretable and editable alternatives to opaque collaborative filtering representations, enabling greater transparency and user control. However, it remains unclear whether users consider these profiles to be an accurate representation of their taste, which is crucial for trust and usability. Moreover, because LLMs inherit societal and data-driven biases, profile quality may systematically vary across user and item characteristics. In this paper, we study this issue in the context of music streaming, where personalization is challenged by a large and culturally diverse catalog. We conduct a user study in which participants rate NL profiles generated from their own listening histories. We analyze whether identification with the profiles is biased by user attributes (e.g., mainstreamness, taste diversity) and item features (e.g., genre, country of origin). We also compare these patterns to those observed when using the profiles in a downstream recommendation task. Our findings highlight both the potential and limitations of scrutable, LLM-based profiling in personalized systems.",
        "translated": "大语言模型（LLMs）在推荐领域一个特别有前景的应用场景是：基于消费数据自动生成自然语言（NL）用户品味画像。这些画像为不透明的协同过滤表示提供了可解释和可编辑的替代方案，从而实现更高的透明度和用户控制。然而，目前尚不清楚用户是否认为这些画像能准确代表其品味，这对于建立信任和提升可用性至关重要。此外，由于大语言模型继承了社会和数据驱动的偏差，画像质量可能会在不同用户和物品特征之间存在系统性差异。在本文中，我们针对音乐流媒体场景研究了这一问题，在该领域，个性化面临着庞大且文化多元的目录所带来的挑战。我们进行了一项用户研究，让参与者评价根据他们自身听歌历史生成的自然语言画像。我们分析了用户对画像的认同度是否受到用户属性（例如，主流程度、品味多样性）和物品特征（例如，流派、原产国）的影响。我们还将这些模式与在下游推荐任务中使用这些画像时观察到的模式进行了比较。我们的研究结果揭示了在个性化系统中，基于大语言模型的可解释画像生成的潜力和局限性。"
    },
    {
        "title": "Generating Search Explanations using Large Language Models",
        "url": "http://arxiv.org/abs/2507.16692v1",
        "pub_date": "2025-07-22",
        "summary": "Aspect-oriented explanations in search results are typically concise text snippets placed alongside retrieved documents to serve as explanations that assist users in efficiently locating relevant information. While Large Language Models (LLMs) have demonstrated exceptional performance for a range of problems, their potential to generate explanations for search results has not been explored. This study addresses that gap by leveraging both encoder-decoder and decoder-only LLMs to generate explanations for search results. The explanations generated are consistently more accurate and plausible explanations than those produced by a range of baseline models.",
        "translated": "搜索结果中的面向方面解释通常是与检索文档一同呈现的简洁文本片段，作为解释帮助用户高效定位相关信息。尽管大型语言模型（LLM）已在诸多问题上展现出卓越性能，但其为搜索结果生成解释的潜力尚未被探索。本研究旨在填补这一空白，通过利用编码器-解码器架构和纯解码器架构的LLM为搜索结果生成解释。所生成的解释始终比一系列基线模型生成的解释更准确、更具说服力。"
    },
    {
        "title": "Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in\n  Real-World Applications",
        "url": "http://arxiv.org/abs/2507.16507v1",
        "pub_date": "2025-07-22",
        "summary": "Conventional Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) but often fall short on complex queries, delivering limited, extractive answers and struggling with multiple targeted retrievals or navigating intricate entity relationships. This is a critical gap in knowledge-intensive domains. We introduce INRAExplorer, an agentic RAG system for exploring the scientific data of INRAE (France's National Research Institute for Agriculture, Food and Environment). INRAExplorer employs an LLM-based agent with a multi-tool architecture to dynamically engage a rich knowledge base, through a comprehensive knowledge graph derived from open access INRAE publications. This design empowers INRAExplorer to conduct iterative, targeted queries, retrieve exhaustive datasets (e.g., all publications by an author), perform multi-hop reasoning, and deliver structured, comprehensive answers. INRAExplorer serves as a concrete illustration of enhancing knowledge interaction in specialized fields.",
        "translated": "传统的检索增强生成（RAG）系统虽然能增强大型语言模型（LLM），但在处理复杂查询时往往力有不逮，只能提供有限的、抽取式的答案，并且难以进行多重定向检索或处理复杂的实体关系。这在知识密集型领域是一个关键空白。\n\n我们引入了INRAExplorer，这是一个代理式RAG系统，旨在探索法国国家农业、食品与环境研究院（INRAE）的科学数据。INRAExplorer采用一个基于LLM的智能体，该智能体具备多工具架构，能够动态地与一个丰富的知识库进行交互，而该知识库则通过一个来源于INRAE开放获取出版物的全面知识图谱构建而成。\n\n这种设计使INRAExplorer能够执行迭代式、定向查询，检索详尽的数据集（例如，某位作者的所有出版物），进行多跳推理，并提供结构化、全面的答案。INRAExplorer是一个具体的范例，展示了如何在专业领域增强知识交互。"
    },
    {
        "title": "Enhancing patent retrieval using automated patent summarization",
        "url": "http://arxiv.org/abs/2507.16371v1",
        "pub_date": "2025-07-22",
        "summary": "Effective query formulation is a key challenge in long-document Information Retrieval (IR). This challenge is particularly acute in domain-specific contexts like patent retrieval, where documents are lengthy, linguistically complex, and encompass multiple interrelated technical topics. In this work, we present the application of recent extractive and abstractive summarization methods for generating concise, purpose-specific summaries of patent documents. We further assess the utility of these automatically generated summaries as surrogate queries across three benchmark patent datasets and compare their retrieval performance against conventional approaches that use entire patent sections. Experimental results show that summarization-based queries significantly improve prior-art retrieval effectiveness, highlighting their potential as an efficient alternative to traditional query formulation techniques.",
        "translated": "长文档信息检索（IR）中，有效的查询构建是一项关键挑战。这一挑战在专利检索等特定领域环境中尤为突出，因为相关文档篇幅冗长、语言表达复杂，并涵盖多个相互关联的技术主题。在本研究中，我们介绍了近期抽取式和抽象式摘要方法在生成专利文档的简洁、有针对性摘要方面的应用。我们进一步评估了这些自动生成的摘要在三个基准专利数据集上作为代理查询的效用，并将其检索性能与使用整个专利章节的传统方法进行了比较。实验结果表明，基于摘要的查询显著提升了现有技术检索的有效性，凸显了它们作为传统查询构建技术的有效替代方案的潜力。"
    },
    {
        "title": "Time to Split: Exploring Data Splitting Strategies for Offline\n  Evaluation of Sequential Recommenders",
        "url": "http://arxiv.org/abs/2507.16289v1",
        "pub_date": "2025-07-22",
        "summary": "Modern sequential recommender systems, ranging from lightweight transformer-based variants to large language models, have become increasingly prominent in academia and industry due to their strong performance in the next-item prediction task. Yet common evaluation protocols for sequential recommendations remain insufficiently developed: they often fail to reflect the corresponding recommendation task accurately, or are not aligned with real-world scenarios.   Although the widely used leave-one-out split matches next-item prediction, it permits the overlap between training and test periods, which leads to temporal leakage and unrealistically long test horizon, limiting real-world relevance. Global temporal splitting addresses these issues by evaluating on distinct future periods. However, its applications to sequential recommendations remain loosely defined, particularly in terms of selecting target interactions and constructing a validation subset that provides necessary consistency between validation and test metrics.   In this paper, we demonstrate that evaluation outcomes can vary significantly across splitting strategies, influencing model rankings and practical deployment decisions. To improve reproducibility in both academic and industrial settings, we systematically compare different splitting strategies for sequential recommendations across multiple datasets and established baselines. Our findings show that prevalent splits, such as leave-one-out, may be insufficiently aligned with more realistic evaluation strategies. Code: https://github.com/monkey0head/time-to-split",
        "translated": "现代序贯推荐系统，从轻量级基于Transformer的变体到大型语言模型，因其在下一个项目预测任务中的出色表现，在学术界和工业界日益受到关注。然而，序贯推荐的常见评估协议仍不够完善：它们往往无法准确反映相应的推荐任务，或未能与现实世界场景对齐。尽管广泛使用的留一法（leave-one-out）划分与下一个项目预测任务相吻合，但它允许训练和测试周期之间存在重叠，从而导致时间泄露和不切实际的过长测试周期，限制了其实际应用相关性。全局时间划分通过在不同的未来周期进行评估来解决这些问题。然而，它在序贯推荐中的应用仍定义模糊，特别是在目标交互的选择以及构建一个能够提供验证和测试指标之间必要一致性的验证子集方面。在本文中，我们证明评估结果会因划分策略的不同而显著变化，从而影响模型排名和实际部署决策。为了提高学术界和工业界的可复现性，我们系统地比较了在多个数据集和成熟基线模型上针对序贯推荐的不同划分策略。我们的研究结果表明，普遍采用的划分方法，如留一法，可能未能与更现实的评估策略充分吻合。\n\n代码：https://github.com/monkey0head/time-to-split"
    },
    {
        "title": "Reinforce Lifelong Interaction Value of User-Author Pairs for\n  Large-Scale Recommendation Systems",
        "url": "http://arxiv.org/abs/2507.16253v1",
        "pub_date": "2025-07-22",
        "summary": "Recommendation systems (RS) help users find interested content and connect authors with their target audience. Most research in RS tends to focus either on predicting users' immediate feedback (like click-through rate) accurately or improving users' long-term engagement. However, they ignore the influence for authors and the lifelong interaction value (LIV) of user-author pairs, which is particularly crucial for improving the prosperity of social community in short-video platforms. Currently, reinforcement learning (RL) can optimize long-term benefits and has been widely applied in RS. In this paper, we introduce RL to Reinforce Lifelong Interaction Value of User-Author pairs (RLIV-UA) based on each interaction of UA pairs. To address the long intervals between UA interactions and the large scale of the UA space, we propose a novel Sparse Cross-Request Interaction Markov Decision Process (SCRI-MDP) and introduce an Adjacent State Approximation (ASA) method to construct RL training samples. Additionally, we introduce Multi-Task Critic Learning (MTCL) to capture the progressive nature of UA interactions (click -&gt; follow -&gt; gift), where denser interaction signals are leveraged to compensate for the learning of sparse labels. Finally, an auxiliary supervised learning task is designed to enhance the convergence of the RLIV-UA model. In offline experiments and online A/B tests, the RLIV-UA model achieves both higher user satisfaction and higher platform profits than compared methods.",
        "translated": "推荐系统（RS）帮助用户发现感兴趣的内容，并将作者与其目标受众连接起来。大多数推荐系统研究倾向于侧重于准确预测用户的即时反馈（例如点击率）或提升用户的长期参与度。然而，它们忽略了对作者的影响以及用户-作者对的终身互动价值（LIV），这对于提升短视频平台社交社区的繁荣度尤为关键。\n\n当前，强化学习（RL）可以优化长期收益，并已在推荐系统中广泛应用。在本文中，我们引入强化学习来强化用户-作者对的终身互动价值（RLIV-UA），该方法基于UA对的每一次互动。针对UA互动之间较长的间隔以及UA空间规模庞大等问题，我们提出了一种新颖的稀疏跨请求互动马尔可夫决策过程（SCRI-MDP），并引入了一种邻近状态近似（ASA）方法来构建RL训练样本。此外，我们引入了多任务评论家学习（MTCL）来捕捉UA互动的渐进式性质（点击 -> 关注 -> 赠礼），其中利用更密集的互动信号来弥补稀疏标签的学习。最后，设计了一项辅助监督学习任务，以增强RLIV-UA模型的收敛性。在离线实验和在线A/B测试中，RLIV-UA模型相比于对比方法，取得了更高的用户满意度和更高的平台利润。"
    },
    {
        "title": "LLM-Enhanced Reranking for Complementary Product Recommendation",
        "url": "http://arxiv.org/abs/2507.16237v1",
        "pub_date": "2025-07-22",
        "summary": "Complementary product recommendation, which aims to suggest items that are used together to enhance customer value, is a crucial yet challenging task in e-commerce. While existing graph neural network (GNN) approaches have made significant progress in capturing complex product relationships, they often struggle with the accuracy-diversity tradeoff, particularly for long-tail items. This paper introduces a model-agnostic approach that leverages Large Language Models (LLMs) to enhance the reranking of complementary product recommendations. Unlike previous works that use LLMs primarily for data preprocessing and graph augmentation, our method applies LLM-based prompting strategies directly to rerank candidate items retrieved from existing recommendation models, eliminating the need for model retraining. Through extensive experiments on public datasets, we demonstrate that our approach effectively balances accuracy and diversity in complementary product recommendations, with at least 50% lift in accuracy metrics and 2% lift in diversity metrics on average for the top recommended items across datasets.",
        "translated": "互补商品推荐旨在推荐配套使用的商品以提升顾客价值，是电商领域一项关键但具挑战性的任务。尽管现有图神经网络（GNN）方法在捕捉复杂商品关系方面取得了显著进展，但它们常难以解决准确性-多样性权衡问题，尤其对于长尾商品。\n\n本文提出了一种模型无关的方法，该方法利用大语言模型（LLMs）来增强互补商品推荐的重排序。与以往主要将LLMs用于数据预处理和图增强的工作不同，我们的方法将基于LLM的提示策略直接应用于对从现有推荐模型中检索出的候选商品进行重排序，从而省去了模型再训练的需要。\n\n通过在公共数据集上进行的广泛实验，我们证明了我们的方法在互补商品推荐中有效地平衡了准确性和多样性，对于推荐结果中的顶部商品，在所有数据集上平均而言，准确性指标至少提升了50%，多样性指标提升了2%。"
    },
    {
        "title": "EBaReT: Expert-guided Bag Reward Transformer for Auto Bidding",
        "url": "http://arxiv.org/abs/2507.16186v1",
        "pub_date": "2025-07-22",
        "summary": "Reinforcement learning has been widely applied in automated bidding. Traditional approaches model bidding as a Markov Decision Process (MDP). Recently, some studies have explored using generative reinforcement learning methods to address long-term dependency issues in bidding environments. Although effective, these methods typically rely on supervised learning approaches, which are vulnerable to low data quality due to the amount of sub-optimal bids and low probability rewards resulting from the low click and conversion rates. Unfortunately, few studies have addressed these challenges.   In this paper, we formalize the automated bidding as a sequence decision-making problem and propose a novel Expert-guided Bag Reward Transformer (EBaReT) to address concerns related to data quality and uncertainty rewards. Specifically, to tackle data quality issues, we generate a set of expert trajectories to serve as supplementary data in the training process and employ a Positive-Unlabeled (PU) learning-based discriminator to identify expert transitions. To ensure the decision also meets the expert level, we further design a novel expert-guided inference strategy. Moreover, to mitigate the uncertainty of rewards, we consider the transitions within a certain period as a \"bag\" and carefully design a reward function that leads to a smoother acquisition of rewards. Extensive experiments demonstrate that our model achieves superior performance compared to state-of-the-art bidding methods.",
        "translated": "强化学习已广泛应用于自动化出价领域。传统方法将出价建模为马尔可夫决策过程（MDP）。近期，一些研究探索了使用生成式强化学习方法来解决出价环境中的长期依赖问题。尽管这些方法有效，但它们通常依赖于监督学习范式，这使得它们易受数据质量问题的影响，因为存在大量次优出价以及由低点击率和转化率导致的低概率奖励。遗憾的是，鲜有研究针对这些挑战提出解决方案。\n\n在本文中，我们将自动化出价问题形式化为序列决策问题，并提出一种新颖的专家引导袋奖励Transformer (EBaReT)，旨在解决数据质量和奖励不确定性等问题。具体而言，为解决数据质量问题，我们生成一组专家轨迹作为训练过程中的补充数据，并采用基于正未标记（PU）学习的判别器来识别专家转换。为确保决策能够达到专家水平，我们进一步设计了一种新颖的专家引导推理策略。此外，为缓解奖励的不确定性，我们将特定时间段内的转换视为一个“袋子”，并精心设计了一个奖励函数，以实现更平滑的奖励获取。大量实验表明，与现有最先进的出价方法相比，我们的模型取得了卓越的性能。"
    },
    {
        "title": "Scaling Recommender Transformers to One Billion Parameters",
        "url": "http://arxiv.org/abs/2507.15994v1",
        "pub_date": "2025-07-21",
        "summary": "While large transformer models have been successfully used in many real-world applications such as natural language processing, computer vision, and speech processing, scaling transformers for recommender systems remains a challenging problem. Recently, Generative Recommenders framework was proposed to scale beyond typical Deep Learning Recommendation Models (DLRMs). Reformulation of recommendation as sequential transduction task led to improvement of scaling properties in terms of compute. Nevertheless, the largest encoder configuration reported by the HSTU authors amounts only to ~176 million parameters, which is considerably smaller than the hundreds of billions or even trillions of parameters common in modern language models.   In this work, we present a recipe for training large transformer recommenders with up to a billion parameters. We show that autoregressive learning on user histories naturally decomposes into two subtasks, feedback prediction and next-item prediction, and demonstrate that such a decomposition scales effectively across a wide range of transformer sizes. Furthermore, we report a successful deployment of our proposed architecture on a large-scale music platform serving millions of users. According to our online A/B tests, this new model increases total listening time by +2.26% and raises the likelihood of user likes by +6.37%, constituting (to our knowledge) the largest improvement in recommendation quality reported for any deep learning-based system in the platform's history.",
        "translated": "尽管大型 Transformer 模型已成功应用于自然语言处理、计算机视觉和语音处理等许多现实世界应用中，但将 Transformer 扩展到推荐系统仍然是一个具有挑战性的问题。近期，生成式推荐系统框架被提出，旨在实现超越典型的深度学习推荐模型（DLRMs）的扩展。将推荐任务重新表述为序列转换任务，带来了计算扩展性方面的改进。然而，HSTU 作者报告的最大编码器配置仅有约1.76亿参数，这远小于现代语言模型中普遍存在的数千亿甚至数万亿参数。\n\n在这项工作中，我们提出了一种训练大型 Transformer 推荐系统的方法，其参数规模可达十亿级。我们表明，对用户历史进行自回归学习自然地分解为反馈预测和下一项预测两个子任务，并证明这种分解可以有效扩展，适用于各种 Transformer 规模。此外，我们报告了将我们提出的架构成功部署到一个服务数百万用户的大规模音乐平台上。根据我们的在线 A/B 测试，这个新模型使总听歌时长增加了 +2.26%，并将用户点赞的可能性提高了 +6.37%，这（据我们所知）构成了该平台历史上任何基于深度学习的系统所报告的推荐质量的最大改进。"
    },
    {
        "title": "Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer\n  Support",
        "url": "http://arxiv.org/abs/2507.16754v1",
        "pub_date": "2025-07-22",
        "summary": "Large Language Models (LLMs) have shown promise in assisting developers with code-related questions; however, LLMs carry the risk of generating unreliable answers. To address this, Retrieval-Augmented Generation (RAG) has been proposed to reduce the unreliability (i.e., hallucinations) of LLMs. However, designing effective pipelines remains challenging due to numerous design choices. In this paper, we construct a retrieval corpus of over 3 million Java and Python related Stack Overflow posts with accepted answers, and explore various RAG pipeline designs to answer developer questions, evaluating their effectiveness in generating accurate and reliable responses. More specifically, we (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants to answer questions that have historically similar matches, and (2) address new questions without any close prior matches by automatically lowering the similarity threshold during retrieval, thereby increasing the chance of finding partially relevant context and improving coverage for unseen cases. We find that implementing a RAG pipeline combining hypothetical-documentation-embedding (HyDE) with the full-answer context performs best in retrieving and answering similarcontent for Stack Overflow questions. Finally, we apply our optimal RAG pipeline to 4 open-source LLMs and compare the results to their zero-shot performance. Our findings show that RAG with our optimal RAG pipeline consistently outperforms zero-shot baselines across models, achieving higher scores for helpfulness, correctness, and detail with LLM-as-a-judge. These findings demonstrate that our optimal RAG pipelines robustly enhance answer quality for a wide range of developer queries including both previously seen and novel questions across different LLMs",
        "translated": "大型语言模型（LLMs）在辅助开发者处理代码相关问题方面已展现出潜力；然而，LLMs存在生成不可靠答案的风险。为解决此问题，检索增强生成（RAG）已被提出，旨在降低LLMs的不可靠性（即幻觉）。然而，由于设计选择众多，设计有效的RAG管道仍然具有挑战性。\n\n在本文中，我们构建了一个包含300多万条附带采纳答案的Java和Python相关Stack Overflow帖子的检索语料库，并探索了多种RAG管道设计以回答开发者问题，评估了它们在生成准确可靠回复方面的有效性。更具体地说，我们(1) 设计并评估了7种不同的RAG管道及其63种变体，以回答历史上存在相似匹配的问题；(2) 通过在检索过程中自动降低相似度阈值，来处理没有任何紧密先前匹配的新问题，从而增加找到部分相关上下文的机会，并提高对未见情况的覆盖范围。\n\n我们发现，结合了假设文档嵌入（HyDE）和完整答案上下文的RAG管道，在检索和回答Stack Overflow相似内容的问题方面表现最佳。最后，我们将我们最优的RAG管道应用于4个开源LLMs，并将其结果与它们的零样本性能进行比较。我们的研究结果表明，采用我们最优RAG管道的RAG在所有模型上都持续优于零样本基线，并在LLM作为评判者（LLM-as-a-judge）的评估中，在有用性、正确性和细节方面取得了更高的分数。这些发现表明，我们最优的RAG管道能够稳健地提升各种开发者查询的答案质量，包括跨不同LLM的先前已知问题和新颖问题。"
    },
    {
        "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
        "url": "http://arxiv.org/abs/2507.16784v1",
        "pub_date": "2025-07-22",
        "summary": "To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.",
        "translated": "为了突破限制大语言模型（LLM）推理准确性和效率的上下文限制，我们提出了线程推理模型（TIM）——一系列为递归和分解式问题解决而训练的LLM，以及TIMRUN——一个能够实现超越上下文限制的长程结构化推理的推理运行时。两者结合，在TIMRUN上运行的TIM支持在单次语言模型推理中实现几乎无限的工作内存和多跳工具调用，克服了输出限制、位置嵌入约束和GPU内存瓶颈。性能的实现得益于将自然语言建模为由长度和深度共同衡量的推理树，而非线性序列。这些推理树由包含思想、递归子任务和结论的任务组成，这基于我们在Schroeder等人2025年提出的概念。在生成过程中，我们维护一个工作内存，该内存通过基于规则的子任务剪枝机制，仅保留最相关上下文令牌的键值（KV）状态，从而使得在整个推理过程中能够复用位置嵌入和GPU内存页。实验结果表明，我们的系统保持了高推理吞吐量，即使在对GPU内存中高达90%的KV缓存进行操作时也是如此。它还在数学任务上实现了准确的推理，并处理了需要长程推理和多跳工具使用的信息检索挑战。"
    },
    {
        "title": "RAVine: Reality-Aligned Evaluation for Agentic Search",
        "url": "http://arxiv.org/abs/2507.16725v1",
        "pub_date": "2025-07-22",
        "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval augmentation, is driving the evolution of intelligent search systems. However, existing evaluation frameworks fail to align well with the goals of agentic search. First, the complex queries commonly used in current benchmarks often deviate from realistic user search scenarios. Second, prior approaches tend to introduce noise when extracting ground truth for end-to-end evaluations, leading to distorted assessments at a fine-grained level. Third, most current frameworks focus solely on the quality of final answers, neglecting the evaluation of the iterative process inherent to agentic search. To address these limitations, we propose RAVine -- a Reality-Aligned eValuation framework for agentic LLMs with search. RAVine targets multi-point queries and long-form answers that better reflect user intents, and introduces an attributable ground truth construction strategy to enhance the accuracy of fine-grained evaluation. Moreover, RAVine examines model's interaction with search tools throughout the iterative process, and accounts for factors of efficiency. We benchmark a series of models using RAVine and derive several insights, which we hope will contribute to advancing the development of agentic search systems. The code and datasets are available at https://github.com/SwordFaith/RAVine.",
        "translated": "智能体搜索作为一种更自主、自适应的检索增强范式，正在推动智能搜索系统的演进。然而，现有评估框架未能很好地契合智能体搜索的目标。首先，当前基准测试中常用的复杂查询往往偏离了真实用户搜索场景。其次，先前方法在为端到端评估提取真值时倾向于引入噪声，导致细粒度层面的评估失真。第三，大多数当前框架仅仅关注最终答案的质量，而忽略了对智能体搜索所固有的迭代过程的评估。\n\n为解决这些局限性，我们提出了RAVine——一个面向基于搜索的智能体大语言模型的真实对齐评估框架。RAVine 针对更好地反映用户意图的多点查询和长篇回答，并引入了一种可归因的真值构建策略，以提高细粒度评估的准确性。此外，RAVine 在整个迭代过程中检查模型与搜索工具的交互，并兼顾了效率因素。我们使用RAVine对一系列模型进行了基准测试，并得出了若干见解，我们希望这些见解能有助于推动智能体搜索系统的发展。代码和数据集可在 https://github.com/SwordFaith/RAVine 获取。"
    },
    {
        "title": "Leave No One Behind: Fairness-Aware Cross-Domain Recommender Systems for\n  Non-Overlapping Users",
        "url": "http://arxiv.org/abs/2507.17749v1",
        "pub_date": "2025-07-23",
        "summary": "Cross-domain recommendation (CDR) methods predominantly leverage overlapping users to transfer knowledge from a source domain to a target domain. However, through empirical studies, we uncover a critical bias inherent in these approaches: while overlapping users experience significant enhancements in recommendation quality, non-overlapping users benefit minimally and even face performance degradation. This unfairness may erode user trust, and, consequently, negatively impact business engagement and revenue. To address this issue, we propose a novel solution that generates virtual source-domain users for non-overlapping target-domain users. Our method utilizes a dual attention mechanism to discern similarities between overlapping and non-overlapping users, thereby synthesizing realistic virtual user embeddings. We further introduce a limiter component that ensures the generated virtual users align with real-data distributions while preserving each user's unique characteristics. Notably, our method is model-agnostic and can be seamlessly integrated into any CDR model. Comprehensive experiments conducted on three public datasets with five CDR baselines demonstrate that our method effectively mitigates the CDR non-overlapping user bias, without loss of overall accuracy. Our code is publicly available at https://github.com/WeixinChen98/VUG.",
        "translated": "**摘要翻译：**\n\n跨域推荐（CDR）方法主要利用重叠用户将知识从源领域迁移到目标领域。然而，通过实证研究，我们揭示了这些方法中固有的一个关键偏差：重叠用户在推荐质量上获得了显著提升，而非重叠用户受益甚微，甚至面临性能下降。这种不公平性可能侵蚀用户信任，从而对业务参与度和收入产生负面影响。\n\n为解决这个问题，我们提出了一种新颖的解决方案，即为非重叠目标领域用户生成虚拟源领域用户。我们的方法利用双重注意力机制来识别重叠用户和非重叠用户之间的相似性，从而合成真实的虚拟用户嵌入。我们进一步引入了一个限制器组件，以确保生成的虚拟用户与真实数据分布对齐，同时保留每个用户独有的特征。\n\n值得注意的是，我们的方法是模型无关的，可以无缝集成到任何CDR模型中。在三个公共数据集上，通过五个CDR基线模型进行的综合实验表明，我们的方法在不损失整体准确性的情况下，有效缓解了CDR非重叠用户偏差。我们的代码已公开发布在 https://github.com/WeixinChen98/VUG。"
    },
    {
        "title": "Citation Recommendation using Deep Canonical Correlation Analysis",
        "url": "http://arxiv.org/abs/2507.17603v1",
        "pub_date": "2025-07-23",
        "summary": "Recent advances in citation recommendation have improved accuracy by leveraging multi-view representation learning to integrate the various modalities present in scholarly documents. However, effectively combining multiple data views requires fusion techniques that can capture complementary information while preserving the unique characteristics of each modality. We propose a novel citation recommendation algorithm that improves upon linear Canonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a neural network extension capable of capturing complex, non-linear relationships between distributed textual and graph-based representations of scientific articles. Experiments on the large-scale DBLP (Digital Bibliography &amp; Library Project) citation network dataset demonstrate that our approach outperforms state-of-the-art CCA-based methods, achieving relative improvements of over 11% in Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These gains reflect more relevant citation recommendations and enhanced ranking quality, suggesting that DCCA's non-linear transformations yield more expressive latent representations than CCA's linear projections.",
        "translated": "引用推荐领域的近期进展，通过利用多视图表示学习（multi-view representation learning）整合学术文档中存在的多种模态（modality），显著提升了推荐准确性。然而，要有效结合多个数据视图，需要能够捕获互补信息，同时又能保留每种模态独特特征的融合技术。\n\n本文提出一种新颖的引用推荐算法，该算法通过应用深度典型相关分析（Deep CCA, DCCA）对传统的线性典型相关分析（Canonical Correlation Analysis, CCA）方法进行了改进。DCCA作为一种神经网络扩展，能够捕捉科学文章的分布式文本表示（distributed textual representations）与图表示（graph-based representations）之间复杂的非线性关系。\n\n在 DBLP（数字文献目录与图书馆项目）大规模引用网络数据集上进行的实验表明，我们提出的方法优于目前最先进的基于 CCA 的方法，在平均精度均值@10（Mean Average Precision@10, MAP@10）上取得了超过11%的相对提升，在准确率@10（Precision@10）上提升了5%，在召回率@10（Recall@10）上提升了7%。这些性能提升反映了更相关的引用推荐和更高的排序质量，这表明 DCCA 的非线性变换能够生成比 CCA 的线性投影更具表达力的潜在表示（latent representations）。"
    },
    {
        "title": "Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for\n  Tumor Flagging and Staging",
        "url": "http://arxiv.org/abs/2507.17412v1",
        "pub_date": "2025-07-23",
        "summary": "The increasing volume of medical images poses challenges for radiologists in retrieving relevant cases. Content-based image retrieval (CBIR) systems offer potential for efficient access to similar cases, yet lack standardized evaluation and comprehensive studies. Building on prior studies for tumor characterization via CBIR, this study advances CBIR research for volumetric medical images through three key contributions: (1) a framework eliminating reliance on pre-segmented data and organ-specific datasets, aligning with large and unstructured image archiving systems, i.e. PACS in clinical practice; (2) introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's contextualized late interaction mechanism for 3D medical imaging; (3) comprehensive evaluation across four tumor sites using three feature extractors and three database configurations. Our evaluations highlight the significant advantages of C-MIR. We demonstrate the successful adaptation of the late interaction principle to volumetric medical images, enabling effective context-aware re-ranking. A key finding is C-MIR's ability to effectively localize the region of interest, eliminating the need for pre-segmentation of datasets and offering a computationally efficient alternative to systems relying on expensive data enrichment steps. C-MIR demonstrates promising improvements in tumor flagging, achieving improved performance, particularly for colon and lung tumors (p&lt;0.05). C-MIR also shows potential for improving tumor staging, warranting further exploration of its capabilities. Ultimately, our work seeks to bridge the gap between advanced retrieval techniques and their practical applications in healthcare, paving the way for improved diagnostic processes.",
        "translated": "医学影像数量的不断增长给放射科医生检索相关病例带来了挑战。基于内容的图像检索（CBIR）系统为高效获取相似病例提供了潜力，但目前缺乏标准化评估和全面研究。本研究基于先前通过CBIR进行肿瘤特征分析的研究，通过三项关键贡献推进了体积医学影像的CBIR研究：(1) 提出了一个框架，消除了对预分割数据和器官特异性数据集的依赖，与临床实践中大型非结构化图像归档系统（如PACS）保持一致；(2) 引入了C-MIR，这是一种新颖的体积重排序方法，它借鉴了ColBERT的上下文后期交互机制，并将其应用于三维医学影像；(3) 在四个肿瘤部位上使用三种特征提取器和三种数据库配置进行了全面评估。我们的评估结果突出显示了C-MIR的显著优势。我们证明了后期交互原理在体积医学影像上的成功应用，从而实现了有效的上下文感知重排序。一个关键发现是C-MIR能够有效定位感兴趣区域（ROI），从而无需对数据集进行预分割，并为依赖昂贵数据增强步骤的系统提供了一种计算效率更高的替代方案。C-MIR在肿瘤检出方面展现出可喜的改进，尤其在结肠癌和肺癌方面取得了显著的性能提升（p<0.05）。C-MIR还在改善肿瘤分期方面显示出潜力，有待对其功能进行进一步探索。最终，我们的工作旨在弥合先进检索技术与医疗保健领域实际应用之间的鸿沟，为改进诊断流程铺平道路。"
    },
    {
        "title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic\n  Learning",
        "url": "http://arxiv.org/abs/2507.17402v1",
        "pub_date": "2025-07-23",
        "summary": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of matching untrimmed videos with text queries describing only partial content. Existing methods suffer from geometric distortion in Euclidean space that sometimes misrepresents the intrinsic hierarchical structure of videos and overlooks certain hierarchical semantics, ultimately leading to suboptimal temporal modeling. To address this issue, we propose the first hyperbolic modeling framework for PRVR, namely HLFormer, which leverages hyperbolic space learning to compensate for the suboptimal hierarchical modeling capabilities of Euclidean space. Specifically, HLFormer integrates the Lorentz Attention Block and Euclidean Attention Block to encode video embeddings in hybrid spaces, using the Mean-Guided Adaptive Interaction Module to dynamically fuse features. Additionally, we introduce a Partial Order Preservation Loss to enforce \"text &lt; video\" hierarchy through Lorentzian cone constraints. This approach further enhances cross-modal matching by reinforcing partial relevance between video content and text queries. Extensive experiments show that HLFormer outperforms state-of-the-art methods. Code is released at https://github.com/lijun2005/ICCV25-HLFormer.",
        "translated": "偏相关视频检索（PRVR）旨在解决将未剪辑视频与仅描述部分内容的文本查询进行匹配这一关键挑战。现有方法存在欧几里得空间中的几何失真问题，有时无法准确表示视频固有的层级结构，并忽略某些层级语义，最终导致次优的时序建模。为解决此问题，我们提出了首个针对PRVR的双曲建模框架——HLFormer，它利用双曲空间学习来弥补欧几里得空间在层级建模能力上的不足。具体而言，HLFormer集成了洛伦兹注意力块和欧几里得注意力块，用于在混合空间中编码视频嵌入，并使用均值引导自适应交互模块动态融合特征。此外，我们引入了偏序保持损失，通过洛伦兹锥约束强制执行“文本 < 视频”的层级关系。这种方法通过强化视频内容与文本查询之间的偏相关性，进一步增强了跨模态匹配。大量实验表明，HLFormer优于现有最先进方法。代码已发布于 https://github.com/lijun2005/ICCV25-HLFormer。"
    },
    {
        "title": "Millions of $\\text{GeAR}$-s: Extending GraphRAG to Millions of Documents",
        "url": "http://arxiv.org/abs/2507.17399v1",
        "pub_date": "2025-07-23",
        "summary": "Recent studies have explored graph-based approaches to retrieval-augmented generation, leveraging structured or semi-structured information -- such as entities and their relations extracted from documents -- to enhance retrieval. However, these methods are typically designed to address specific tasks, such as multi-hop question answering and query-focused summarisation, and therefore, there is limited evidence of their general applicability across broader datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG solution: $\\text{GeAR}$ and explore its performance and limitations on the SIGIR 2025 LiveRAG Challenge.",
        "translated": "近期研究探索了基于图的检索增强生成（RAG）方法，利用从文档中提取的实体及其关系等结构化或半结构化信息以增强检索。然而，这些方法通常旨在解决特定任务，例如多跳问答和面向查询的摘要，因此，在更广泛的数据集上，它们的通用适用性证据有限。在本文中，我们旨在调整一个最先进的基于图的RAG解决方案：$\\text{GeAR}$，并探究其在 SIGIR 2025 LiveRAG 挑战赛上的性能和局限性。"
    },
    {
        "title": "\"Beyond the past\": Leveraging Audio and Human Memory for Sequential\n  Music Recommendation",
        "url": "http://arxiv.org/abs/2507.17356v1",
        "pub_date": "2025-07-23",
        "summary": "On music streaming services, listening sessions are often composed of a balance of familiar and new tracks. Recently, sequential recommender systems have adopted cognitive-informed approaches, such as Adaptive Control of Thought-Rational (ACT-R), to successfully improve the prediction of the most relevant tracks for the next user session. However, one limitation of using a model inspired by human memory (or the past), is that it struggles to recommend new tracks that users have not previously listened to. To bridge this gap, here we propose a model that leverages audio information to predict in advance the ACT-R-like activation of new tracks and incorporates them into the recommendation scoring process. We demonstrate the empirical effectiveness of the proposed model using proprietary data, which we publicly release along with the model's source code to foster future research in this field.",
        "translated": "在音乐流媒体服务中，用户的听歌会话通常兼顾熟悉曲目与新曲目。近期，序列推荐系统已采纳认知驱动的方法（例如ACT-R），成功提升了对用户下一个会话中最相关曲目的预测能力。然而，此类受人类记忆（或用户历史行为）启发的模型存在一个局限性，即难以推荐用户此前从未听过的新曲目。为弥补这一不足，本文提出一种新模型，该模型利用音频信息提前预测新曲目的类ACT-R激活度，并将其整合到推荐评分流程中。我们使用专有数据集验证了所提出模型的实证有效性，并连同模型的源代码一并公开发布，以促进该领域的未来研究。"
    },
    {
        "title": "EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp\n  Diagnosis Leveraging Latent Scene Representations",
        "url": "http://arxiv.org/abs/2507.17323v1",
        "pub_date": "2025-07-23",
        "summary": "Colorectal cancer (CRC) remains a leading cause of cancer-related mortality, underscoring the importance of timely polyp detection and diagnosis. While deep learning models have improved optical-assisted diagnostics, they often demand extensive labeled datasets and yield \"black-box\" outputs with limited interpretability. In this paper, we propose EndoFinder, an online polyp retrieval framework that leverages multi-view scene representations for explainable and scalable CRC diagnosis. First, we develop a Polyp-aware Image Encoder by combining contrastive learning and a reconstruction task, guided by polyp segmentation masks. This self-supervised approach captures robust features without relying on large-scale annotated data. Next, we treat each polyp as a three-dimensional \"scene\" and introduce a Scene Representation Transformer, which fuses multiple views of the polyp into a single latent representation. By discretizing this representation through a hashing layer, EndoFinder enables real-time retrieval from a compiled database of historical polyp cases, where diagnostic information serves as interpretable references for new queries. We evaluate EndoFinder on both public and newly collected polyp datasets for re-identification and pathology classification. Results show that EndoFinder outperforms existing methods in accuracy while providing transparent, retrieval-based insights for clinical decision-making. By contributing a novel dataset and a scalable, explainable framework, our work addresses key challenges in polyp diagnosis and offers a promising direction for more efficient AI-driven colonoscopy workflows. The source code is available at https://github.com/ku262/EndoFinder-Scene.",
        "translated": "结直肠癌（CRC）仍然是癌症相关死亡的主要原因之一，这突显了及时息肉检测和诊断的重要性。尽管深度学习模型改进了光学辅助诊断，但它们通常需要大规模标注数据集，并产生可解释性有限的“黑箱”输出。在本文中，我们提出了 EndoFinder，一个在线息肉检索框架，它利用多视角场景表示来实现可解释且可扩展的结直肠癌诊断。\n\n首先，我们通过结合对比学习和重建任务，并在息肉分割掩模的引导下，开发了一个息肉感知图像编码器。这种自监督方法无需依赖大规模标注数据即可捕获鲁棒特征。其次，我们将每个息肉视为一个三维“场景”，并引入了一个场景表示Transformer，它将息肉的多个视角融合成单一潜在表示。通过哈希层对这种表示进行离散化，EndoFinder 能够从一个包含历史息肉病例的编译数据库中进行实时检索，其中诊断信息可作为新查询的可解释参考。\n\n我们在公开和新收集的息肉数据集上对 EndoFinder 进行了评估，用于息肉重识别和病理分类。结果表明，EndoFinder 在准确性方面优于现有方法，同时为临床决策提供了透明的、基于检索的洞察。通过贡献一个新颖数据集和一个可扩展、可解释的框架，我们的工作解决了息肉诊断中的关键挑战，并为更高效的AI驱动结肠镜检查工作流程提供了有前景的方向。源代码可在 https://github.com/ku262/EndoFinder-Scene 获取。"
    },
    {
        "title": "Exploring the Potential of LLMs for Serendipity Evaluation in\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2507.17290v1",
        "pub_date": "2025-07-23",
        "summary": "Serendipity plays a pivotal role in enhancing user satisfaction within recommender systems, yet its evaluation poses significant challenges due to its inherently subjective nature and conceptual ambiguity. Current algorithmic approaches predominantly rely on proxy metrics for indirect assessment, often failing to align with real user perceptions, thus creating a gap. With large language models (LLMs) increasingly revolutionizing evaluation methodologies across various human annotation tasks, we are inspired to explore a core research proposition: Can LLMs effectively simulate human users for serendipity evaluation? To address this question, we conduct a meta-evaluation on two datasets derived from real user studies in the e-commerce and movie domains, focusing on three key aspects: the accuracy of LLMs compared to conventional proxy metrics, the influence of auxiliary data on LLM comprehension, and the efficacy of recently popular multi-LLM techniques. Our findings indicate that even the simplest zero-shot LLMs achieve parity with, or surpass, the performance of conventional metrics. Furthermore, multi-LLM techniques and the incorporation of auxiliary data further enhance alignment with human perspectives. Based on our findings, the optimal evaluation by LLMs yields a Pearson correlation coefficient of 21.5\\% when compared to the results of the user study. This research implies that LLMs may serve as potentially accurate and cost-effective evaluators, introducing a new paradigm for serendipity evaluation in recommender systems.",
        "translated": "惊喜发现（Serendipity）在提升推荐系统用户满意度方面发挥着关键作用，然而，由于其固有的主观性和概念模糊性，对其进行评估面临巨大挑战。现有算法方法主要依赖代理指标进行间接评估，但常常无法与真实用户感知保持一致，从而产生了鸿沟。随着大语言模型（LLMs）在各种人工标注任务中日益革新评估方法论，我们受到启发去探索一个核心研究命题：大语言模型能否有效模拟人类用户进行惊喜发现评估？\n\n为解答这一问题，我们对来自电子商务和电影领域真实用户研究的两个数据集进行了一项元评估，重点关注三个关键方面：LLM与传统代理指标相比的准确性、辅助数据对LLM理解能力的影响，以及近期流行的多LLM技术的有效性。我们的研究结果表明，即使是最简单的零样本LLM，其性能也能与传统指标持平或超越。此外，多LLM技术和引入辅助数据进一步增强了与人类视角的契合度。根据我们的研究结果，LLM的最佳评估结果与用户研究结果相比，取得了21.5%的皮尔逊相关系数。这项研究表明，LLM或可作为潜在的准确且经济高效的评估者，为推荐系统中的惊喜发现评估引入了新的范式。"
    },
    {
        "title": "Large Learning Rates Simultaneously Achieve Robustness to Spurious\n  Correlations and Compressibility",
        "url": "http://arxiv.org/abs/2507.17748v1",
        "pub_date": "2025-07-23",
        "summary": "Robustness and resource-efficiency are two highly desirable properties for modern machine learning models. However, achieving them jointly remains a challenge. In this paper, we position high learning rates as a facilitator for simultaneously achieving robustness to spurious correlations and network compressibility. We demonstrate that large learning rates also produce desirable representation properties such as invariant feature utilization, class separation, and activation sparsity. Importantly, our findings indicate that large learning rates compare favorably to other hyperparameters and regularization methods, in consistently satisfying these properties in tandem. In addition to demonstrating the positive effect of large learning rates across diverse spurious correlation datasets, models, and optimizers, we also present strong evidence that the previously documented success of large learning rates in standard classification tasks is likely due to its effect on addressing hidden/rare spurious correlations in the training dataset.",
        "translated": "鲁棒性（robustness）和资源效率（resource-efficiency）是现代机器学习模型极具吸引力的两大特性。然而，如何同时实现这两者仍具挑战。在本文中，我们提出将大学习率（high learning rates）作为一种促进因素，能够同时提高模型对虚假关联的鲁棒性以及实现网络的可压缩性。我们进一步证明，大学习率还能带来理想的表征特性，例如不变特征利用、类别间可分性以及激活稀疏性。重要的是，我们的研究结果表明，在持续协同地满足这些特性方面，大学习率表现优于其他超参数和正则化方法。除了在多种虚假关联数据集、模型和优化器上展示大学习率的积极作用外，我们还提供了有力证据，表明大学习率在标准分类任务中此前已记录的成功，很可能归因于其在解决训练数据集中隐藏/稀有虚假关联方面的作用。"
    },
    {
        "title": "TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in\n  Languages of West Asia and North Africa",
        "url": "http://arxiv.org/abs/2507.17709v1",
        "pub_date": "2025-07-23",
        "summary": "We present TyDi QA-WANA, a question-answering dataset consisting of 28K examples divided among 10 language varieties of western Asia and northern Africa. The data collection process was designed to elicit information-seeking questions, where the asker is genuinely curious to know the answer. Each question in paired with an entire article that may or may not contain the answer; the relatively large size of the articles results in a task suitable for evaluating models' abilities to utilize large text contexts in answering questions. Furthermore, the data was collected directly in each language variety, without the use of translation, in order to avoid issues of cultural relevance. We present performance of two baseline models, and release our code and data to facilitate further improvement by the research community.",
        "translated": "我们介绍了 TyDi QA-WANA，这是一个问答数据集，包含 2.8 万个示例，涵盖来自西亚和北非的 10 种语言变体。数据收集过程旨在引出信息寻求型问题，即提问者真心希望了解答案。数据集中每个问题都与一篇完整的文章配对，文章中可能包含也可能不包含答案；文章相对较大的篇幅使得这项任务适合评估模型在回答问题时利用大文本上下文的能力。此外，数据是直接在每种语言变体中收集的，未经翻译，以避免文化相关性问题。我们展示了两个基线模型的性能，并发布了我们的代码和数据，以促进研究社区的进一步改进。"
    },
    {
        "title": "DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge\n  Injection and Synthetic Data",
        "url": "http://arxiv.org/abs/2507.18583v1",
        "pub_date": "2025-07-24",
        "summary": "Electronic Health Records (EHRs) are pivotal in clinical practices, yet their retrieval remains a challenge mainly due to semantic gap issues. Recent advancements in dense retrieval offer promising solutions but existing models, both general-domain and biomedical-domain, fall short due to insufficient medical knowledge or mismatched training corpora. This paper introduces \\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV discharge summaries to address the need for extensive medical knowledge and large-scale training data. The first stage involves medical entity extraction and knowledge injection from a biomedical knowledge graph, while the second stage employs large language models to generate diverse training data. We train two variants of \\texttt{DR.EHR}, with 110M and 7B parameters, respectively. Evaluated on the CliniQ benchmark, our models significantly outperforms all existing dense retrievers, achieving state-of-the-art results. Detailed analyses confirm our models' superiority across various match and query types, particularly in challenging semantic matches like implication and abbreviation. Ablation studies validate the effectiveness of each pipeline component, and supplementary experiments on EHR QA datasets demonstrate the models' generalizability on natural language questions, including complex ones with multiple entities. This work significantly advances EHR retrieval, offering a robust solution for clinical applications.",
        "translated": "电子健康记录（EHR）在临床实践中至关重要，然而，其检索仍然面临挑战，主要原因在于语义鸿沟问题。稠密检索的最新进展提供了有前景的解决方案，但现有的模型，无论是通用领域还是生物医学领域，都因医学知识不足或训练语料不匹配而表现不佳。\n\n本文提出了 \\texttt{DR.EHR}，一系列专门为EHR检索量身定制的稠密检索模型。我们提出了一种两阶段训练流水线，利用MIMIC-IV出院总结来满足对广泛医学知识和大规模训练数据的需求。第一阶段涉及从生物医学知识图谱中进行医学实体抽取和知识注入，而第二阶段则利用大型语言模型生成多样化的训练数据。我们训练了 \\texttt{DR.EHR} 的两个变体，参数量分别为1.1亿和70亿。\n\n在CliniQ基准上进行评估，我们的模型显著优于所有现有稠密检索器，并取得了最先进的成果。详细分析证实了我们的模型在各种匹配和查询类型上的优越性，尤其是在蕴含和缩写等挑战性语义匹配方面。消融研究验证了每个流水线组件的有效性，而对EHR问答数据集的补充实验则证明了模型在自然语言问题上的泛化能力，包括涉及多个实体的复杂问题。这项工作显著推进了EHR检索领域，为临床应用提供了鲁棒的解决方案。"
    },
    {
        "title": "Transform Before You Query: A Privacy-Preserving Approach for Vector\n  Retrieval with Embedding Space Alignment",
        "url": "http://arxiv.org/abs/2507.18518v1",
        "pub_date": "2025-07-24",
        "summary": "Vector Database (VDB) can efficiently index and search high-dimensional vector embeddings from unstructured data, crucially enabling fast semantic similarity search essential for modern AI applications like generative AI and recommendation systems. Since current VDB service providers predominantly use proprietary black-box models, users are forced to expose raw query text to them via API in exchange for the vector retrieval services. Consequently, if query text involves confidential records from finance or healthcare domains, this mechanism inevitably leads to critical leakage of user's sensitive information. To address this issue, we introduce STEER (\\textbf{S}ecure \\textbf{T}ransformed \\textbf{E}mbedding v\\textbf{E}ctor\\textbf{ R}etrieval), a private vector retrieval framework that leverages the alignment relationship between the semantic spaces of different embedding models to derive approximate embeddings for the query text. STEER performs the retrieval using the approximate embeddings within the original VDB and requires no modifications to the server side. Our theoretical and experimental analyses demonstrate that STEER effectively safeguards query text privacy while maintaining the retrieval accuracy. Even though approximate embeddings are approximations of the embeddings from proprietary models, they still prevent the providers from recovering the query text through Embedding Inversion Attacks (EIAs). Extensive experimental results show that Recall@100 of STEER can basically achieve a decrease of less than 5\\%. Furthermore, even when searching within a text corpus of millions of entries, STEER achieves a Recall@20 accuracy 20\\% higher than current baselines.",
        "translated": "向量数据库（VDB）能够高效索引和搜索来自非结构化数据的高维向量嵌入，这对于支持现代AI应用（如生成式AI和推荐系统）中的快速语义相似性搜索至关重要。鉴于当前的VDB服务提供商主要使用专有黑盒模型，用户被迫通过API向其暴露原始查询文本，以换取向量检索服务。因此，如果查询文本涉及金融或医疗领域的机密记录，这种机制不可避免地导致用户敏感信息的严重泄露。\n\n为解决这一问题，我们引入了STEER（**S**ecure **T**ransformed **E**mbedding v**E**ctor **R**etrieval，安全转换嵌入向量检索），这是一个私有向量检索框架。它利用不同嵌入模型语义空间之间的对齐关系，为查询文本推导出近似嵌入。STEER在原始VDB中利用这些近似嵌入执行检索，并且无需对服务器端进行任何修改。我们的理论和实验分析表明，STEER能够有效保护查询文本隐私，同时保持检索准确性。尽管近似嵌入是专有模型生成嵌入的近似值，但它们仍然能够阻止服务提供商通过嵌入反演攻击（EIA）恢复查询文本。大量实验结果表明，STEER的Recall@100基本可以实现不到5%的性能下降。此外，即使在包含数百万条目的文本语料库中进行搜索，STEER的Recall@20准确率也比当前基线高20%。"
    },
    {
        "title": "The Best is Yet to Come: Graph Convolution in the Testing Phase for\n  Multimodal Recommendation",
        "url": "http://arxiv.org/abs/2507.18489v1",
        "pub_date": "2025-07-24",
        "summary": "The efficiency and scalability of graph convolution networks (GCNs) in training recommender systems remain critical challenges, hindering their practical deployment in real-world scenarios. In the multimodal recommendation (MMRec) field, training GCNs requires more expensive time and space costs and exacerbates the gap between different modalities, resulting in sub-optimal recommendation accuracy. This paper critically points out the inherent challenges associated with adopting GCNs during the training phase in MMRec, revealing that GCNs inevitably create unhelpful and even harmful pairs during model optimization and isolate different modalities. To this end, we propose FastMMRec, a highly efficient multimodal recommendation framework that deploys graph convolutions exclusively during the testing phase, bypassing their use in training. We demonstrate that adopting GCNs solely in the testing phase significantly improves the model's efficiency and scalability while alleviating the modality isolation problem often caused by using GCNs during the training phase. We conduct extensive experiments on three public datasets, consistently demonstrating the performance superiority of FastMMRec over competitive baselines while achieving efficiency and scalability.",
        "translated": "图卷积网络（GCNs）在训练推荐系统时的效率和可扩展性仍然是关键挑战，阻碍了它们在实际场景中的实际部署。在多模态推荐（MMRec）领域，训练GCNs需要耗费更高昂的时空成本，并加剧了不同模态之间的差距，导致次优的推荐准确性。本文深刻指出在MMRec训练阶段采用GCNs所固有的挑战，揭示了GCNs在模型优化过程中不可避免地会创建无用甚至有害的配对，并隔离了不同模态。为此，我们提出了FastMMRec，一个高效的多模态推荐框架，该框架仅在测试阶段部署图卷积，从而避免了其在训练中的使用。我们表明，仅在测试阶段采用GCNs显著提升了模型的效率和可扩展性，同时缓解了通常由训练阶段GCNs的使用引起的模态隔离问题。我们在三个公共数据集上进行了大量实验，一致地表明FastMMRec在实现效率和可扩展性的同时，其性能优于竞争基线。"
    },
    {
        "title": "How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to\n  Expert-Defined Concepts",
        "url": "http://arxiv.org/abs/2507.18479v1",
        "pub_date": "2025-07-24",
        "summary": "Prerequisite skills - foundational competencies required before mastering more advanced concepts - are important for supporting effective learning, assessment, and skill-gap analysis. Traditionally curated by domain experts, these relationships are costly to maintain and difficult to scale. This paper investigates whether large language models (LLMs) can predict prerequisite skills in a zero-shot setting, using only natural language descriptions and without task-specific fine-tuning. We introduce ESCO-PrereqSkill, a benchmark dataset constructed from the ESCO taxonomy, comprising 3,196 skills and their expert-defined prerequisite links. Using a standardized prompting strategy, we evaluate 13 state-of-the-art LLMs, including GPT-4, Claude 3, Gemini, LLaMA 4, Qwen2, and DeepSeek, across semantic similarity, BERTScore, and inference latency. Our results show that models such as LLaMA4-Maverick, Claude-3-7-Sonnet, and Qwen2-72B generate predictions that closely align with expert ground truth, demonstrating strong semantic reasoning without supervision. These findings highlight the potential of LLMs to support scalable prerequisite skill modeling for applications in personalized learning, intelligent tutoring, and skill-based recommender systems.",
        "translated": "前置技能——掌握更高级概念之前所需的基础能力——对于支持有效的学习、评估和技能差距分析至关重要。这些关系传统上由领域专家编撰，但维护成本高昂且难以扩展。本文研究了大型语言模型（LLMs）能否在零样本设置下，仅使用自然语言描述且无需进行任务特定的微调来预测前置技能。我们引入了ESCO-PrereqSkill，这是一个基于ESCO分类法构建的基准数据集，包含3,196项技能及其专家定义的前置链接。采用标准化的提示策略，我们从语义相似度、BERTScore和推理延迟三个方面评估了13个最先进的LLMs，包括GPT-4、Claude 3、Gemini、LLaMA 4、Qwen2和DeepSeek。我们的结果表明，LLaMA4-Maverick、Claude-3-7-Sonnet和Qwen2-72B等模型生成的预测与专家定义的真值高度吻合，在无监督情况下展现出强大的语义推理能力。这些发现突出了LLMs在支持可扩展的前置技能建模方面的潜力，可应用于个性化学习、智能辅导和基于技能的推荐系统等领域。"
    },
    {
        "title": "LLM-based Embedders for Prior Case Retrieval",
        "url": "http://arxiv.org/abs/2507.18455v1",
        "pub_date": "2025-07-24",
        "summary": "In common law systems, legal professionals such as lawyers and judges rely on precedents to build their arguments. As the volume of cases has grown massively over time, effectively retrieving prior cases has become essential. Prior case retrieval (PCR) is an information retrieval (IR) task that aims to automatically identify the most relevant court cases for a specific query from a large pool of potential candidates. While IR methods have seen several paradigm shifts over the last few years, the vast majority of PCR methods continue to rely on traditional IR methods, such as BM25. The state-of-the-art deep learning IR methods have not been successful in PCR due to two key challenges: i. Lengthy legal text limitation; when using the powerful BERT-based transformer models, there is a limit of input text lengths, which inevitably requires to shorten the input via truncation or division with a loss of legal context information. ii. Lack of legal training data; due to data privacy concerns, available PCR datasets are often limited in size, making it difficult to train deep learning-based models effectively. In this research, we address these challenges by leveraging LLM-based text embedders in PCR. LLM-based embedders support longer input lengths, and since we use them in an unsupervised manner, they do not require training data, addressing both challenges simultaneously. In this paper, we evaluate state-of-the-art LLM-based text embedders in four PCR benchmark datasets and show that they outperform BM25 and supervised transformer-based models.",
        "translated": "在普通法系中，律师和法官等法律专业人士依赖判例来构建其论点。随着时间推移，案件量大幅增长，有效检索既往案例变得至关重要。既往案例检索（Prior Case Retrieval, PCR）是一项信息检索（Information Retrieval, IR）任务，旨在从庞大的潜在候选案例库中，为特定查询自动识别出最相关的法院案例。\n\n尽管信息检索方法在过去几年中经历了多次范式转变，但绝大多数PCR方法仍沿用BM25等传统信息检索方法。最先进的深度学习信息检索方法在PCR中未能取得成功，原因在于两个关键挑战：\n\ni. **冗长法律文本限制**：在使用强大的基于BERT的Transformer模型时，输入文本长度存在限制，这不可避免地需要通过截断或分割来缩短输入，从而导致法律上下文信息的丢失。\nii. **法律训练数据匮乏**：由于数据隐私问题，可用的PCR数据集规模通常有限，这使得有效训练基于深度学习的模型变得困难。\n\n在本研究中，我们通过在PCR中利用基于大型语言模型（LLM）的文本嵌入器来应对这些挑战。基于LLM的嵌入器支持更长的输入长度，并且由于我们以无监督方式使用它们，它们不需要训练数据，从而同时解决了这两个挑战。在本文中，我们在四个PCR基准数据集上评估了最先进的基于LLM的文本嵌入器，并表明它们优于BM25和监督式Transformer模型。"
    },
    {
        "title": "RecPS: Privacy Risk Scoring for Recommender Systems",
        "url": "http://arxiv.org/abs/2507.18365v1",
        "pub_date": "2025-07-24",
        "summary": "Recommender systems (RecSys) have become an essential component of many web applications. The core of the system is a recommendation model trained on highly sensitive user-item interaction data. While privacy-enhancing techniques are actively studied in the research community, the real-world model development still depends on minimal privacy protection, e.g., via controlled access. Users of such systems should have the right to choose \\emph{not} to share highly sensitive interactions. However, there is no method allowing the user to know which interactions are more sensitive than others. Thus, quantifying the privacy risk of RecSys training data is a critical step to enabling privacy-aware RecSys model development and deployment. We propose a membership-inference attack (MIA)- based privacy scoring method, RecPS, to measure privacy risks at both the interaction and user levels. The RecPS interaction-level score definition is motivated and derived from differential privacy, which is then extended to the user-level scoring method. A critical component is the interaction-level MIA method RecLiRA, which gives high-quality membership estimation. We have conducted extensive experiments on well-known benchmark datasets and RecSys models to show the unique features and benefits of RecPS scoring in risk assessment and RecSys model unlearning. Our code is available at https://anonymous.4open.science/r/RsLiRA-4BD3/readme.md.",
        "translated": "推荐系统（RecSys）已成为许多网络应用中不可或缺的组成部分。系统的核心是基于高度敏感的用户-物品交互数据训练的推荐模型。尽管隐私增强技术在研究界得到了积极研究，但实际模型开发仍依赖于最低限度的隐私保护，例如通过受控访问。此类系统的用户应有权选择不共享高度敏感的交互数据。然而，目前还没有方法能让用户知道哪些交互比其他交互更敏感。因此，量化推荐系统训练数据的隐私风险，是促成隐私感知推荐模型开发与部署的关键一步。\n\n我们提出了一种基于成员推断攻击（MIA）的隐私评分方法RecPS，用于衡量交互层面和用户层面的隐私风险。RecPS的交互层面评分定义受差分隐私启发并从中推导而来，随后被扩展到用户层面的评分方法。其关键组成部分是交互层面的MIA方法RecLiRA，它能提供高质量的成员关系估计。我们对知名基准数据集和推荐模型进行了大量实验，展示了RecPS评分在风险评估和推荐模型遗忘方面的独特特性和优势。我们的代码可在 https://anonymous.4open.science/r/RsLiRA-4BD3/readme.md 获取。"
    },
    {
        "title": "Fashion-AlterEval: A Dataset for Improved Evaluation of Conversational\n  Recommendation Systems with Alternative Relevant Items",
        "url": "http://arxiv.org/abs/2507.18017v1",
        "pub_date": "2025-07-24",
        "summary": "In Conversational Recommendation Systems (CRS), a user provides feedback on recommended items at each turn, leading the CRS towards improved recommendations. Due to the need for a large amount of data, a user simulator is employed for both training and evaluation. Such user simulators critique the current retrieved item based on knowledge of a single target item. However, system evaluation in offline settings with simulators is limited by the focus on a single target item and their unlimited patience over a large number of turns. To overcome these limitations of existing simulators, we propose Fashion-AlterEval, a new dataset that contains human judgments for a selection of alternative items by adding new annotations in common fashion CRS datasets. Consequently, we propose two novel meta-user simulators that use the collected judgments and allow simulated users not only to express their preferences about alternative items to their original target, but also to change their mind and level of patience. In our experiments using the Shoes and Fashion IQ as the original datasets and three CRS models, we find that using the knowledge of alternatives by the simulator can have a considerable impact on the evaluation of existing CRS models, specifically that the existing single-target evaluation underestimates their effectiveness, and when simulatedusers are allowed to instead consider alternative relevant items, the system can rapidly respond to more quickly satisfy the user.",
        "translated": "在对话式推荐系统（CRS）中，用户在每一轮对话中都会对推荐的商品提供反馈，从而引导CRS提供更优的推荐。由于需要大量数据，用户模拟器被用于训练和评估。此类用户模拟器依据对单一目标商品的了解，对当前检索到的商品进行评价。然而，在离线环境下使用模拟器进行系统评估，受到了其专注于单一目标商品以及在大量轮次中表现出的无限耐心的限制。为了克服现有模拟器的这些局限性，我们提出了Fashion-AlterEval，这是一个通过在常见时尚CRS数据集中添加新标注而构建的新数据集，其中包含对一系列备选商品的人工判断。因此，我们提出了两种新颖的元用户模拟器，它们利用所收集的判断，允许模拟用户不仅可以表达他们对不同于原始目标的备选商品的偏好，而且还可以改变主意和耐心程度。在我们使用Shoes和Fashion IQ作为原始数据集以及三个CRS模型进行的实验中，我们发现模拟器利用备选商品的知识，能对现有CRS模型的评估产生显著影响，具体而言，现有的单一目标评估低估了它们的有效性。当允许模拟用户转而考虑替代相关商品时，系统可以快速响应，从而更快地满足用户需求。"
    },
    {
        "title": "Failure Prediction in Conversational Recommendation Systems",
        "url": "http://arxiv.org/abs/2507.17976v1",
        "pub_date": "2025-07-23",
        "summary": "In a Conversational Image Recommendation task, users can provide natural language feedback on a recommended image item, which leads to an improved recommendation in the next turn. While typical instantiations of this task assume that the user's target item will (eventually) be returned, this might often not be true, for example, the item the user seeks is not within the item catalogue. Failing to return a user's desired item can lead to user frustration, as the user needs to interact with the system for an increased number of turns. To mitigate this issue, in this paper, we introduce the task of Supervised Conversational Performance Prediction, inspired by Query Performance Prediction (QPP) for predicting effectiveness in response to a search engine query. In this regard, we propose predictors for conversational performance that detect conversation failures using multi-turn semantic information contained in the embedded representations of retrieved image items. Specifically, our AutoEncoder-based predictor learns a compressed representation of top-retrieved items of the train turns and uses the classification labels to predict the evaluation turn. Our evaluation scenario addressed two recommendation scenarios, by differentiating between system failure, where the system is unable to find the target, and catalogue failure, where the target does not exist in the item catalogue. In our experiments using the Shoes and FashionIQ Dresses datasets, we measure the accuracy of predictors for both system and catalogue failures. Our results demonstrate the promise of our proposed predictors for predicting system failures (existing evaluation scenario), while we detect a considerable decrease in predictive performance in the case of catalogue failure prediction (when inducing a missing item scenario) compared to system failures.",
        "translated": "在对话式图像推荐任务中，用户可以对推荐的图像项提供自然语言反馈，从而促使系统在下一轮中提供更佳推荐。然而，尽管该任务的典型实现假设用户的目标项（最终）会被返回，但这种假设通常不成立，例如用户寻找的商品可能不在商品目录中。未能返回用户期望的商品会导致用户沮丧，因为用户需要与系统进行更多轮次的交互。为了缓解这一问题，本文引入了监督式对话表现预测任务，其灵感来源于用于预测搜索引擎查询有效性的查询表现预测（QPP）。为此，我们提出了对话表现预测器，它们利用检索到的图像项的嵌入表示中包含的多轮语义信息来检测对话失败。具体而言，我们基于自编码器的预测器学习训练轮次中靠前检索到的项的压缩表示，并使用分类标签来预测评估轮次。我们的评估场景涉及两种推荐场景，即区分系统故障（系统无法找到目标）和目录故障（目标项不存在于商品目录中）。在我们使用Shoes和FashionIQ Dresses数据集进行的实验中，我们衡量了预测器对于系统故障和目录故障的准确性。我们的结果表明，我们提出的预测器在预测系统故障（现有评估场景）方面具有潜力，但与预测系统故障相比，在目录故障预测（当引入缺失商品场景时）的情况下，预测性能出现显著下降。"
    },
    {
        "title": "DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge\n  Injection and Synthetic Data",
        "url": "http://arxiv.org/abs/2507.18583v1",
        "pub_date": "2025-07-24",
        "summary": "Electronic Health Records (EHRs) are pivotal in clinical practices, yet their retrieval remains a challenge mainly due to semantic gap issues. Recent advancements in dense retrieval offer promising solutions but existing models, both general-domain and biomedical-domain, fall short due to insufficient medical knowledge or mismatched training corpora. This paper introduces \\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV discharge summaries to address the need for extensive medical knowledge and large-scale training data. The first stage involves medical entity extraction and knowledge injection from a biomedical knowledge graph, while the second stage employs large language models to generate diverse training data. We train two variants of \\texttt{DR.EHR}, with 110M and 7B parameters, respectively. Evaluated on the CliniQ benchmark, our models significantly outperforms all existing dense retrievers, achieving state-of-the-art results. Detailed analyses confirm our models' superiority across various match and query types, particularly in challenging semantic matches like implication and abbreviation. Ablation studies validate the effectiveness of each pipeline component, and supplementary experiments on EHR QA datasets demonstrate the models' generalizability on natural language questions, including complex ones with multiple entities. This work significantly advances EHR retrieval, offering a robust solution for clinical applications.",
        "translated": "电子健康记录（EHRs）在临床实践中至关重要，但其检索仍然面临挑战，主要原因在于语义鸿沟问题。稠密检索的最新进展提供了有前景的解决方案，然而，现有的模型，无论是通用领域还是生物医学领域，都因医学知识不足或训练语料不匹配而表现不佳。\n\n本文介绍了 \\texttt{DR.EHR}，这是一系列专门为 EHR 检索量身定制的稠密检索模型。为了满足对丰富医学知识和大规模训练数据的需求，我们提出了一个利用 MIMIC-IV 出院小结的两阶段训练流水线。第一阶段涉及从生物医学知识图谱中进行医学实体抽取和知识注入，而第二阶段则利用大语言模型生成多样化的训练数据。我们训练了 \\texttt{DR.EHR} 的两个变体，其参数量分别为1.1亿和70亿。\n\n在 CliniQ 基准测试集上进行评估，我们的模型显著优于所有现有稠密检索器，取得了当前最先进的（SOTA）结果。详细分析证实了我们的模型在各种匹配和查询类型上的优越性，特别是在隐含关系和缩写等挑战性的语义匹配中表现突出。消融实验验证了每个流水线组件的有效性，而针对 EHR 问答数据集的补充实验则证明了模型在自然语言问题上的泛化能力，包括那些包含多个实体的复杂问题。这项工作显著推动了 EHR 检索领域的发展，为临床应用提供了稳健的解决方案。"
    },
    {
        "title": "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains",
        "url": "http://arxiv.org/abs/2507.17746v1",
        "pub_date": "2025-07-23",
        "summary": "Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world tasks often requires balancing objective and subjective evaluation criteria. However, many such tasks lack a single, unambiguous ground truth-making it difficult to define reliable reward signals for post-training language models. While traditional preference-based methods offer a workaround, they rely on opaque reward functions that are difficult to interpret and prone to spurious correlations. We introduce $\\textbf{Rubrics as Rewards}$ (RaR), a framework that uses structured, checklist-style rubrics as interpretable reward signals for on-policy training with GRPO. Our best RaR method yields up to a $28\\%$ relative improvement on HealthBench-1k compared to simple Likert-based approaches, while matching or surpassing the performance of reward signals derived from expert-written references. By treating rubrics as structured reward signals, we show that RaR enables smaller-scale judge models to better align with human preferences and sustain robust performance across model scales.",
        "translated": "将可验证奖励的强化学习 (RLVR) 扩展到现实世界任务中，通常需要平衡客观和主观评估标准。然而，许多此类任务缺乏单一、明确的基准真相，这使得难以定义用于训练后语言模型的可靠奖励信号。尽管传统的基于偏好的方法提供了一种变通方案，但它们依赖于不透明的奖励函数，这些函数难以解释且容易产生虚假关联。我们引入了$\\textbf{量规即奖励}$ (RaR)，这是一个利用结构化的、清单式量规作为可解释奖励信号的框架，用于结合 GRPO 进行在策略训练。我们最好的 RaR 方法在 HealthBench-1k 上取得了高达 28% 的相对提升，超越了简单的基于李克特量表的方法，同时达到或超越了源自专家编写参考资料的奖励信号的性能。通过将量规视为结构化的奖励信号，我们表明 RaR 能够使规模较小的判断模型更好地与人类偏好对齐，并在不同模型规模下保持稳健的性能。"
    },
    {
        "title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many\n  Mapping for Zero-shot Image Captioning",
        "url": "http://arxiv.org/abs/2507.18616v1",
        "pub_date": "2025-07-24",
        "summary": "Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets generated by text-to-image (T2I) models to mitigate the need for costly manual annotation. However, these T2I models often produce images that exhibit semantic misalignments with their corresponding input captions (e.g., missing objects, incorrect attributes), resulting in noisy synthetic image-caption pairs that can hinder model training. Existing dataset pruning techniques are largely designed for removing noisy text in web-crawled data. However, these methods are ill-suited for the distinct challenges of synthetic data, where captions are typically well-formed, but images may be inaccurate representations. To address this gap, we introduce SynC, a novel framework specifically designed to refine synthetic image-caption datasets for ZIC. Instead of conventional filtering or regeneration, SynC focuses on reassigning captions to the most semantically aligned images already present within the synthetic image pool. Our approach employs a one-to-many mapping strategy by initially retrieving multiple relevant candidate images for each caption. We then apply a cycle-consistency-inspired alignment scorer that selects the best image by verifying its ability to retrieve the original caption via image-to-text retrieval. Extensive evaluations demonstrate that SynC consistently and significantly improves performance across various ZIC models on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art results in several scenarios. SynC offers an effective strategy for curating refined synthetic data to enhance ZIC.",
        "translated": "**零样本图像描述（ZIC）**任务越来越多地利用文本到图像（T2I）模型生成的合成数据集，以减轻对昂贵人工标注的需求。然而，这些T2I模型生成的图像常与其对应的输入描述（例如缺失目标、属性错误）存在语义错位，从而产生嘈杂的合成图像-描述对，这可能阻碍模型训练。现有数据集修剪技术主要设计用于移除网络抓取数据中的嘈杂文本。然而，这些方法不适合处理合成数据的独特挑战——合成数据中的描述通常是格式良好的，但图像可能是不准确的表示。\n\n为弥补这一不足，我们引入了**SynC**，一个专门为ZIC任务精炼合成图像-描述数据集的新颖框架。SynC没有采用传统的过滤或重新生成方法，而是专注于将描述重新分配给已存在于合成图像池中、语义上最对齐的图像。我们的方法采用一对多映射策略，首先为每个描述检索多个相关的候选图像。随后，我们应用一种受循环一致性启发的对齐评分器，通过验证图像到文本检索中其检索原始描述的能力来选择最佳图像。广泛的评估表明，SynC在标准基准数据集（MS-COCO、Flickr30k、NoCaps）上持续显著地提升了各种ZIC模型的性能，并在多个场景中达到了最先进（SOTA）的结果。SynC为整理精炼的合成数据以增强ZIC任务提供了一种有效的策略。"
    },
    {
        "title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many\n  Mapping for Zero-shot Image Captioning",
        "url": "http://arxiv.org/abs/2507.18616v1",
        "pub_date": "2025-07-24",
        "summary": "Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets generated by text-to-image (T2I) models to mitigate the need for costly manual annotation. However, these T2I models often produce images that exhibit semantic misalignments with their corresponding input captions (e.g., missing objects, incorrect attributes), resulting in noisy synthetic image-caption pairs that can hinder model training. Existing dataset pruning techniques are largely designed for removing noisy text in web-crawled data. However, these methods are ill-suited for the distinct challenges of synthetic data, where captions are typically well-formed, but images may be inaccurate representations. To address this gap, we introduce SynC, a novel framework specifically designed to refine synthetic image-caption datasets for ZIC. Instead of conventional filtering or regeneration, SynC focuses on reassigning captions to the most semantically aligned images already present within the synthetic image pool. Our approach employs a one-to-many mapping strategy by initially retrieving multiple relevant candidate images for each caption. We then apply a cycle-consistency-inspired alignment scorer that selects the best image by verifying its ability to retrieve the original caption via image-to-text retrieval. Extensive evaluations demonstrate that SynC consistently and significantly improves performance across various ZIC models on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art results in several scenarios. SynC offers an effective strategy for curating refined synthetic data to enhance ZIC.",
        "translated": "零样本图像描述（Zero-shot Image Captioning, ZIC）越来越多地利用文本到图像（Text-to-Image, T2I）模型生成的合成数据集，以减少对昂贵手动标注的需求。然而，这些T2I模型生成的图像常与其对应的输入字幕存在语义不匹配（例如，对象缺失、属性错误），从而导致含有噪声的合成图像-字幕对，这会阻碍模型训练。现有数据集剪枝技术主要设计用于去除网络爬取数据中的噪声文本。然而，这些方法不适用于合成数据的独特挑战，即字幕通常是结构良好的，但图像可能是不准确的表示。\n\n为了解决这一问题，我们引入了SynC，一个专门为ZIC设计，用于精炼合成图像-字幕数据集的新颖框架。与传统的过滤或重新生成不同，SynC侧重于将字幕重新分配给合成图像池中已有的、语义对齐度最高的图像。我们的方法采用一对多映射策略，首先为每个字幕检索多个相关的候选图像。随后，我们应用一个受循环一致性启发的对齐评分器，通过验证图像能否通过图像到文本检索来检索到原始字幕，从而选择最佳图像。广泛的评估表明，SynC在标准基准（MS-COCO、Flickr30k、NoCaps）上，持续且显著地提升了各种ZIC模型的性能，并在多个场景中取得了最先进的成果。SynC为精选优化后的合成数据提供了一种有效的策略，以增强ZIC的性能。"
    },
    {
        "title": "PosterMate: Audience-driven Collaborative Persona Agents for Poster\n  Design",
        "url": "http://arxiv.org/abs/2507.18572v1",
        "pub_date": "2025-07-24",
        "summary": "Poster designing can benefit from synchronous feedback from target audiences. However, gathering audiences with diverse perspectives and reconciling them on design edits can be challenging. Recent generative AI models present opportunities to simulate human-like interactions, but it is unclear how they may be used for feedback processes in design. We introduce PosterMate, a poster design assistant that facilitates collaboration by creating audience-driven persona agents constructed from marketing documents. PosterMate gathers feedback from each persona agent regarding poster components, and stimulates discussion with the help of a moderator to reach a conclusion. These agreed-upon edits can then be directly integrated into the poster design. Through our user study (N=12), we identified the potential of PosterMate to capture overlooked viewpoints, while serving as an effective prototyping tool. Additionally, our controlled online evaluation (N=100) revealed that the feedback from an individual persona agent is appropriate given its persona identity, and the discussion effectively synthesizes the different persona agents' perspectives.",
        "translated": "海报设计可以从目标受众的同步反馈中获益。然而，汇集具有不同视角的受众并在设计修改上达成一致可能充满挑战。近期生成式人工智能模型带来了模拟人类般交互的机遇，但尚不清楚它们如何应用于设计中的反馈流程。我们介绍 PosterMate，这是一款海报设计助手，它通过根据营销文档构建受众驱动的“人格代理”来促进协作。PosterMate 从每个人格代理处收集关于海报组成部分的反馈，并借助主持人促进讨论以达成结论。这些达成一致的修改随后可以直接整合到海报设计中。通过我们的用户研究（N=12），我们发现 PosterMate 具有捕捉被忽视的观点的潜力，同时可作为一种有效的原型设计工具。此外，我们的对照在线评估（N=100）显示，来自单独人格代理的反馈鉴于其人格身份是恰当的，并且讨论有效地综合了不同人格代理的观点。"
    },
    {
        "title": "Checklists Are Better Than Reward Models For Aligning Language Models",
        "url": "http://arxiv.org/abs/2507.18624v1",
        "pub_date": "2025-07-24",
        "summary": "Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this -- typically using fixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead propose using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item - using both AI judges and specialized verifier programs - then combine these scores to compute rewards for RL. We compare RLCF with other alignment methods applied to a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only method to improve performance on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. These results establish checklist feedback as a key tool for improving language models' support of queries that express a multitude of needs.",
        "translated": "语言模型必须进行适应性调整，以理解并遵循用户指令。强化学习被广泛应用于实现这一目标——通常采用“有用性”和“无害性”等固定标准。在我们的工作中，我们提出改用灵活的、指令特定的标准，以此拓宽强化学习在引导模型遵循指令方面的影响力。我们提出了“检查清单反馈强化学习”（Reinforcement Learning from Checklist Feedback, RLCF）。我们从指令中提取检查清单，并结合人工智能评判员和专用验证程序，评估模型响应满足每个项目的程度——然后将这些分数结合起来，计算出强化学习的奖励。我们将RLCF与应用于一个强大的指令遵循模型（Qwen2.5-7B-Instruct）的其他对齐方法在五个被广泛研究的基准测试上进行了比较。结果显示，RLCF是唯一在所有基准测试上都提升了性能的方法，包括在FollowBench上硬性满足率提升4个百分点，在InFoBench上提升6个百分点，以及在Arena-Hard上胜率提高3个百分点。这些结果表明，检查清单反馈是改进语言模型支持表达多种需求的查询的关键工具。"
    }
]